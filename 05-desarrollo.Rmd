---
bibliography: references.bib
---

# Desarrollo de la Solución: {#desarrollo}

En este Capítulo en \@ref(desarollodescripcion) se presenta la **Descripción General de la Solución**. Posteriormente se muestra la \@ref(desarrolloarquitectura) **Arquitectura de la Solución** con el "Modelo-Vista-Controlador". En \@ref(desarrollociclos) se exponen los cuatro **Ciclos de Desarrollo** que se efectuaron, mientras que en \@ref(pruebas) **Pruebas** se encuentran las distintas pruebas que fueron ejecutadas para medir el comportamiento y redindimiento del software desarrollado.

## Descripción General de la Solución: {#desarollodescripcion}

Se implementa un Sistema de Recuperación de Información sobre un corpus de documentos de tesis de grado y trabajos de grado que originalmente se encuentran alojados en el repositorio digital Saber UCV . Utilizando técnicas de extracción de datos de archivos HTML, desde la ficha de cada investigación se obtiene: el título, el nombre del autor, palabras clave, fecha de publicación, el resumen y el url de descarga del documento que la sustenta.

Posteriormente el Sistema descarga el documento referenciado en cada ficha, el cual contiene el texto completo de la investigación, da lectura y clasifica la información sobre el nombre de la facultad, la escuela o postgrado donde fue realizado el trabajo e igualmente extrae el nombre del tutor.

Todos los datos obtenidos son sometidos a técnicas del estado del arte en el Procesamiento del Lenguaje Natural y la Minería de Texto para conformar un corpus anotado, un índice invertido y una tabla con los vectores de *embeddings* (vector database), esenciales para un eficiente manejo de la base de datos.

El Sistema incluye una aplicación web que permite a los usuarios desde un navegador explorar extensivamente el corpus anotado, realizando consultas de texto y aplicando varios filtros como la selección de la jerarquía, el área académica y el rango de fechas.

La relevancia de los resultados recuperados se determina mediante una función de ponderación y los documentos se presentan de manera priorizada para mejorar la experiencia del usuario.

Adicionalmente, el Sistema ofrece recomendaciones de documentos que presentan similitud con aquellos que fueron recuperados en el proceso anterior y muestra "Mapas de Conocimiento" mediante una herramienta gráfica interactiva de visualización.

La solución implementada cuenta con procesos automatizados de actualización para incorporar las nuevas investigaciones que sean añadidas al repositorio Saber UCV.

El Sistema se soporta en un sistema distribuido conformado por contenedores que son gestionados por un orquestador con la arquitectura "modelo-vista-controlador".

## Arquitectura de la Solución: {#desarrolloarquitectura}

La arquitectura "Modelo-Vista-Controlador" se muestra en la figura \@ref(fig:arquitecturamvc) y posteriormente se describe el comportamiento y las interacciones de los componentes.

```{r, arquitecturamvc, echo=FALSE, out.width='90%',fig.cap='Modelo de Arquitectura MVC',fig.align='center'}
knitr::include_graphics("images/05-desarrollo/MVC9.png")

```

### **Modelo:**

El *Modelo,* en el contexto de esta propuesta, es la parte del Sistema que se ocupa de la obtención, manipulación y gestión de los datos. Esto incluye la descarga de la información, la clasificación, el Procesamiento del Lenguaje Natural, la Minería de Texto y la creación del índice invertido. Esta parte del Sistema también incluye la lógica para generar los *embeddings* y manejar el corpus anotado. En la implementación el manejador de base de datos que usa es PostgreSQL.

Igualmente le corresponde realizar las tareas de actualizar el corpus periódicamente y las recomendaciones de documentos a medida que se agreguen nuevos textos en Saber UCV. En \@ref(desarrollociclos4), el tercer ciclo de desarrollo, se expondrán con detalle los componentes del *Modelo* y a continuación se enumeran los principales procesos que él sustenta:

1.  **Creación del Índice Invertido:**

    -   Organizar los términos y sus ubicaciones en los documentos para permitir búsquedas eficientes.

    -   Asociar cada término con la lista de documentos en los que aparece.

2.  **Procesamiento de Texto:**

    -   Tokenización: Dividir el texto en palabras o frases significativas.

    -   Lematización: Reducir las palabras a su forma base para un análisis más preciso.

    -   POS: etiquetado de partes del discurso.

3.  **Minería de Texto:**

    -   Análisis de Frecuencia: Determinar la frecuencia de ocurrencia de palabras o frases.

4.  **Generación de Embeddings:**

    -   Utilización de un modelo preentrenado para convertir palabras o frases en vectores numéricos.

    -   Convertir en vectores las palabras que componen el *query* para realizar comparaciones semánticas y determinar similitudes entre palabras o documentos.

5.  **Gestión de la Base de Datos:**

    -   Almacenar y recuperar datos estructurados para su posterior consulta.

    -   Actualizar el corpus con nuevos datos.

6.  **Cálculo de Relevancia:**

    -   Aplicar algoritmos para calcular la relevancia de los documentos en función de las consultas del usuario.

    -   Ordenar los resultados en función de su relevancia para presentar los documentos más relevantes primero.

7.  **Actualización de datos:**

    -   Los procesos mencionados previamente son ejecutados y/o actualizados periódicamente.

    -   Se realiza la validación de la integridad de datos para asegurar que los nuevos datos se integren correctamente sin errores o inconsistencias, eliminando posibles duplicados o valores incorrectos.

### **Vista:**

La *Vista* se implementa mediante el *framework* "Shiny" [@shiny] [^05-desarrollo-1] que permite crear aplicaciones web interactivas y tiene un componente de "User Inferface (UI)" donde el usuario interactúa al introducir el texto con el que se hará la búsqueda y aplica filtros como: jerarquía, área académica y rango de fechas. Posterior a la definición de los atributos del *query*, se desencadenan acciones que son enviadas al *Controlador* y al recibir la respuesta la *Vista* se actualiza y muestra las tablas con los resultados de la búsqueda, las representaciones visuales como el "Mapa del Conocimiento" y las recomendaciones de documentos similares.

[^05-desarrollo-1]: El framework Shiny incluye dos componentes principales. El primero es la UI (interfaz de Usuario), que corresponde a la "Vista". El otro componente es el "Server" que en la representación actual es el Controlador.

### **Controlador:**

El *Controlador* se implementa mediante el *framework* "Shiny" que tiene un componente denominado "Server" que es el responsable de manejar las interacciones del usuario, gestionar las consultas de texto y aplicar los filtros seleccionados. También se encarga de orquestar las operaciones entre el *Modelo* y la *Vista*, asegurando que los datos se presenten correctamente y que las consultas se procesen de manera eficiente. Desde el *Controlador* se hace el llamado al componente del *Modelo* donde se encuentra la API para generar el *embedding* del *query* y determinar la relevancia de los documentos recuperados. El *Controlador* aplica el re ordenamiento para mostrar en orden los resultados más relevantes. En él se conforma la estructura para representar el "Mapa del Conocimiento" con datos obtenidos del *Modelo*.

## Ciclos de Desarrollo: {#desarrollociclos}

Los Ciclos de Desarrollo \@ref(mm) constituyen fases críticas del proceso, donde se conciben, diseñan y perfeccionan las funcionalidades del Sistema. Cada uno de estos ciclos está estructurado en tres etapas fundamentales: la etapa de especulación, donde se plantean las ideas y se exploran posibles soluciones; la etapa de colaboración, donde se trabaja en equipo para implementar estas ideas y se evalúan los resultados; y la etapa de aprendizaje, donde se analizan las experiencias pasadas y se ajustan las estrategias para futuras iteraciones.

Para el desarrollo del SCSU se hizo un proceso iterativo donde en cada ciclo se abordó cada una de las fases descritas y así incrementalmente se fueron añadiendo funcionalidades al Sistema.

La literatura en este tema siempre especifica a un cliente del que hay que obtener retroalimentación temprana, para así adaptar el producto a medida que evoluciona. Esto fue lo que se hizo en reuniones continuas en la materia *Tópicos Especiales en Sistemas de Información y Gerencia* que representó a la unidad requirente (cliente) y así se fueron evaluando los requisitos y se formularon las correspondientes hipótesis, se observó y se midió el desempeño, por ejemplo, en los modelos de aprendizaje automático preentrenados, usados para los procesamiento de los textos.

Los Ciclos que se van a exponer son los siguientes: en \@ref(desarrollociclos1) se muestran las tres iteraciones realizadas para la **Conformación del Conjunto de Datos**. En \@ref(desarrollociclos3) se revisan las iteraciones para el desarrollo del **Prototipo del SCSU**, mientras que en \@ref(desarrollociclos4) se hace la **Integración de los Componentes del Software**. En \@ref(desarrollociclos6) se hace la "**Incorporación de Otras Investigaciones"** al incluir Revistas de investigación dentro del Sistema. Finalmente en \@ref(desasarrollociclos5) se hace una versión del Sistema donde se incluye un **Buscador Semántico**.

\newpage

<br> <br>

### Ciclo - Conformación del Conjunto de Datos: {#desarrollociclos1}

En este ciclo es donde se ejecutaron las tareas que permitieron conformar el conjunto de datos acorde a lo planteado en el \@ref(objeespe) **Objetivo Específico** **1,** siendo el insumo con el que se desarrollará el **Sistema Complementario Saber UCV**.

Se realizaron tres iteraciones para lograr el objetivo. La primera \@ref(scrapeo) fue la **Extracción de Datos web**, también conocidas como "web scraping", la segunda iteración \@ref(labels) correspondió al **Levantamiento de las Categoría**s, que son los nombres de las carreras de pregrado y de los postgrados que se imparten en la Universidad Central de Venezuela, mientras que en la tercera \@ref(asignacion) iteración se hizo la **Clasificación de los Trabajos** asociando a cada investigación el nombre de la carrera o del postgrado e igualmente se hizo la extracción del nombre del tutor, acorde a lo planteado en el \@ref(objeespe) **Objetivo Específico 2**.

#### Iteración- "Extracción de Datos web Saber UCV": {#scrapeo}

##### Especulación:

El repositorio Saber UCV en la sección "Comunidades/Tesis" aloja las cantidades de trabajos por nivel académico que se muestran en el cuadro \@ref(tab:cantidadesteg) :

```{r cantidadesteg, echo=FALSE, fig.cap='Cantidades de Trabajos por Categoría', warning=FALSE }

totales_tegs <- readRDS('data/totales_tegs.rds') |>
  select(-fecha)

totales_tegs1 <- prettyNum(sum(totales_tegs[1,]), big.mark='.')

flextable(totales_tegs) |>
  set_caption(caption='Cantidades de Trabajos por Categoría')|>
  add_footer_lines(paste("cifras de Saber.UCV a la fecha", 
                         "15/10/2023"
                         ))|>  #format(Sys.time(), "%d/%m/%Y"
  theme_vanilla()|>
  # set_table_properties(width = 1, layout = "autofit")
  autofit(add_w = 0.2, add_h = 0)



```

En la minería de datos con la extracción de datos web es posible "recolectar, procesar, analizar y extraer útiles conocimientos a partir de los datos disponibles" [@aggarwal2018]. Por esto se planteó replicar en un conjunto de datos alojado localmente, la información contenida en el repositorio Saber UCV que asciende a una cantidad de `r totales_tegs1` investigaciones, incluyendo: categoría (pregrado, otros, maestría, doctorado), el título, autor, fecha de publicación, palabras clave, *url* de descarga [^05-desarrollo-2] y el texto del resumen. Al obtener esta información se puede dar inicio a la conformación del corpus.

[^05-desarrollo-2]: este *url* correponde a el documento escrito del trabajo de grado o tesis que se encuentra alojado en word o pdf.

##### Colaboración:

En esta etapa se realizaron dos procesos de extracción de datos web usando el lenguaje de programación `r version$version.string[1]` [@R].

1.  El primero fue encontrar los *url*´s de cada trabajo alojado en el repositorio Saber UCV. Usando usando la extensión *SelectorGadget* (<https://selectorgadget.com/>) del navegador *Google Chrome* se puede obtener mediante un clic en un elemento de la página la etiqueta *css* asociada al nodo que se desea extraer. En este caso al visitar la página "[http://saber.ucv.ve/handle/10872/1957/browse?type=dateissued&sort_by=2&order=DESC&rpp=1000](#0)", se identificó la etiqueta ´*evenRowOddCol´*, ver figura \@ref(fig:nodosurl), que identifica a los nodos dentro de la página que tienen los enlaces *href* a las fichas de cada investigación.

    Posteriormente con el paquete *rvest* [@rvest] que permite la descarga de páginas web y la manipulación de nodos XML se pudieron extraer los `r prettyNum(rowSums(totales_tegs),big.mark=".")` *urls* a visitar.

```{r nodosurl, echo=FALSE, fig.align='center', fig.cap=c('Etiquetas nodos url´s'), fig.show='hold', out.width="50%"}

knitr::include_graphics(c("images/05-desarrollo/1_ciclo/Picture3.png",
                          "images/05-desarrollo/1_ciclo/Picture2.png"))
#knitr::include_graphics("images/05-desarrollo/1_ciclo/Picture2.png")
#knitr::include_graphics("images/05-desarrollo/1_ciclo/Picture3.png")

```

2.  En una segunda fase, mediante la misma técnica indicada en el punto anterior, se localizó en la ficha de un trabajo la etiqueta *css*, en este caso la *'.metadataFieldValue',* que está asociada al nodo que contiene los valores del: título, autor, fecha de publicación, palabras clave, url de descarga del documento y el texto del resumen. En la figura \@ref(fig:nodosurl)-b se aprecia una imagen de una ficha. Al contar con el listado de url´s y la identificación de los datos a extraer, se hizo un bucle para visitar cada enlace, se descargó el archivo html, se accedió al nodo, se extrajeron los valores y se fueron almacenando en una estructura de datos.

```{r lecturadupli, echo=FALSE}

duplicados_tegs <- readRDS('data/duplicados_tabla.rds') 
```

##### Aprender:

Se enfrentaron las siguientes dificultades y se adoptaron en algunos casos las correspondientes soluciones:

1.  Se realizaron varios intentos para la descarga y extracción de los valores. Para la obtención de cada campo, en principio se tomó de referencia la posición fija en que aparecía dentro de la ficha, porque se había asumido que estas tenían la misma estructura para todos los trabajos, sin embargo en algunas aparecían otros valores, p. ej. el de "colección", alterando la posición en que se encuentra el dato a extraer. La solución adoptada fue que primero se localizarán, dentro de cada ficha, los títulos de los campos, con esto se generó el listado de los valores posicionales y relativo a estos se extrajeron los valores propuestos.

2.  Algunos valores de las fechas contenían información parcial faltando el mes y/o el día. Se adoptó un método de imputar el valor "1" tanto al mes como al día faltante.

3.  Adoptar previsiones para caídas del servidor de Saber UCV y resguardar en cada vuelta del bucle la información extraída, para no perder el trabajo de extracción acumulado en caso de una falla remota o local en el acceso.

4.  La revisión del conjunto de datos obtenido mostró que existen `r prettyNum(sum(duplicados_tegs$Duplicados),big.mark=".")` documentos duplicados bien sea en el "título" o en el texto del "resumen". Esto ocurre por la introducción de algún carácter adicional o mínimas alteraciones en el texto. Para descartar estos registros, se aplicó una función de limpieza al texto (convertir a minúscula, remover signos puntuación, etc.). Posteriormente se obtuvieron sendos valores *hash* sobre el título y el resumen y luego se descartaron los *hashes* duplicados. Adicionalmente se decidió usar el *hash* obtenido del "título procesado" como el identificador único de cada documento. La remoción de investigaciones que puedan estar duplicadas es importante efectuarla, ya que al ejecutar los procesos de recuperación de información o la representación de los resultados en Mapas de Conocimiento, al incluir textos repetidos, creará representaciones distorsionadas. En el cuadro \@ref(tab:cantidadesduplicados) se muestra la cantidad de los valores detectados por Jerarquía.

```{r cantidadesduplicados, echo=FALSE, fig.cap='Cantidades de Trabajos por Categoría', message=FALSE, warning=FALSE}


#duplicados_tegs <- readRDS('data/duplicados_tabla.rds') 

flextable(duplicados_tegs) |>
  set_caption(caption='Cantidades de Trabajos Duplicados')|>
  add_footer_lines(paste("cifras de Saber.UCV a la fecha", 
                         "15/10/2023")
                         )|> #format(Sys.time(), "%d/%m/%Y"
  theme_vanilla()|>
  # set_table_properties(width = 1, layout = "autofit")
  autofit(add_w = 0.2, add_h = 0)|>
  style(j = 4, 
        pr_t = fp_text_default(
          # italic = TRUE, 
          color = "red"))
```

#### Iteración- Levantamiento de Categorías: {#labels}

##### Especulación:

Para poder clasificar cada investigación es necesario contar con las categorías que serán asignadas. Se entiende por "categoría" el nombre de la carrera de pregrado o el postgrado, junto con la facultad, que constituyen la oferta de la Universidad Central de Venezuela en educación universitaria.

Al no encontrarse el listado de categorías disponible en el propio repositorio Saber UCV fue necesario realizar una búsqueda web de esta información, extraerla y estructurarla, para así contar con el conjunto de datos de categorías que permita ejecutar la siguiente iteración, que es la de **Extracción y Clasificación de las Investigaciones** \@ref(asignacion).

##### Colaboración:

Se visitó al sitio web oficial de la Universidad Central de Venezuela para revisar la oferta de pregrados y postgrados. Para los postgrados se encontró para cada categoría (especialización, maestría y doctorado) una página con el listado, p. ej. [http://www.ucv.ve/organizacion/maestria.html](http://www.ucv.ve/organizacion/vrac/gerencia-de-investigacion-cientifica-y-humanistica/gerencia-de-estudios-de-postgrado/programas-de-postgrado-ucv/maestria.html) [^05-desarrollo-3]. En la figura \@ref(fig:maestrias) se aprecian las potenciales etiquetas para las maestrías.

[^05-desarrollo-3]: previendo posibles modificacions en las páginas que contienen los listado de postgrados, se procedió a respaldarlas y forman parte del contenido del repositorio asociado a esta Investigación para garantizar la reproducibilidad de los resultados obtenidos. Para la fecha de redacción de este documento el contenido de los *urls* indicados fue modificado

```{r maestrias, echo=FALSE, fig.align='center', fig.cap=c('Listado de Maestrías'), fig.show='hold', out.width="40%"}

knitr::include_graphics(c("images/05-desarrollo/1_ciclo/maestrias.png"
                          ))
```

Mediante la técnica de recuperación de datos web descrita en \@ref(scrapeo), se procedió a extraer el nombre de cada postgrado, añadir el nivel académico y asociar la facultad a la cual está adscrito.

La cantidad de postgrados por nivel académico: Doctorado, Maestría y Especializaciones se muestran en el cuadro \@ref(tab:resultpostgrado).

```{r resultpostgrado, echo=FALSE, fig.cap='Cantidades de Postgrado por Nivel Académico', message=FALSE, warning=FALSE, out.width='50%'}
df_cdad_postgrados <- readRDS('data/df_cdad_postgrados.rds')
#names(df_cdad_postgrados) <- c('Otros', 'Maestría', 'Doctorado')
df_cdad_postgrados <- df_cdad_postgrados[,c(3,2,1)]

flextable(df_cdad_postgrados) |>
  set_caption(caption='Cantidades de Postgrados por Categoría')|>
  theme_vanilla()|>
  # set_table_properties(width = 1, layout = "autofit")
  autofit(add_w = 0.2, add_h = 0)


```

En cuanto a los pregrados no se encontró en el sitio de la Universidad en una página centralizada la información y se procedió a obtenerla de la página [wikipedia](https://es.wikipedia.org/wiki/Anexo:Facultades_de_la_Universidad_Central_de_Venezuela) asociada a la U.C.V., recuperando un total del 50 nombres de escuelas de pregrado junto con la facultad de adscripción.

En la figura \@ref(fig:jerarquias) se muestran los totales apilados de pregrados y postgrados que se encontraron por Facultad-Centro de Investigación durante el levantamiento de información, cifra que al totalizar los postgrados y pregrados, alcanza la cantidad de `r sum(50+ rowSums(df_cdad_postgrados))` áreas de conocimiento.

```{r jerarquias,echo=FALSE, fig.cap='Total pregrados y postgrados por Facultad-Centro', message=FALSE, out.width='70%'}
totales_facultad <- readRDS('data/totales_facultad.rds')

ggbarplot(totales_facultad,
          x='facultad',
          y='cdad',
          label.rectangle=FALSE,
          # label = distribucion$nn,
          fill = 'jerarquía',
          # color = "#4785FF",
          color = 'jerarquía',
          palette = "Paired",
          # label=TRUE,
          label=format(totales_facultad$cdad,),
          lab.size = 3,
          lab.col = 'black',
          # lab.pos = c("in"),
          # xticks.by=2,
          # ylab = TRUE,
          # yticks.by=100
)+
  theme(axis.text.x = element_text(color = "grey20",
                                   size = 6, 
                                   angle=30,
                                   hjust = 1,
                                   vjust = 1, 
                                   face = "plain"))+ 
  grids(axis='y')


```

##### Aprender:

1.  En el caso de los postgrados inicialmente se esperaba que sólo estuvieran asociados a Facultades pero también se encontró que el Centro de Estudios del Desarrollo y el Centro de Estudios Integrales del Ambiente imparten este tipo de estudios.

2.  Se evaluó que existen nombres postgrados duplicados con la misma categoría, lo que puede generar problemas en la clasificación de las investigaciones, teniendo como ejemplo la "Maestría en Estadística" que se dicta en la Facultad de Agronomía y en la Facultad de Ciencias Económicas y Sociales.

3.  Se detectó que en pregrado existen escuelas que otorgan distintos títulos, p. ej. de la "Escuela de Administración y Contaduría" se pueden obtener los títulos de "contador" o de "licenciado en administración". Esto es algo a tener presente al momento de hacer la categorización, ya que el nombre de la escuela no sirve en estos casos para realizarla, siendo necesario agregar al conjunto de datos el atributo del nombre de los títulos emitidos.

4.  Sobre los textos se tuvieron que realizar modificaciones, ya que en los listados se encontró que algunos nombres les faltaban palabras. La revisión final de nombres, dada la cantidad total de `r rowSums(df_cdad_postgrados) + 50` dependencias, se hizo manualmente para evitar posteriores categorizaciones erróneas .

#### Iteración- Extracción y Clasificación de las Investigaciones: {#asignacion}

En esta iteración se mencionan los principales obstáculos y las estrategias implementadas para alcanzar el objetivo de realizar la clasificación de cada trabajo alojado en Saber UCV, no obstante se omite especificar algunos de los problemas que se encontraron, ya que extenderse en esto abultaría considerablemente el contenido expuesto.

##### Especulación:

Para las investigaciones que reposan en Saber UCV que cuentan con un archivo anexo, correspondiente al documento de la misma, es posible realizar la descarga, extraer una porción de texto y adoptando métodos basados en reglas de coincidencia de patrones, con las etiquetas obtenidas en la iteración \@ref(labels) **Levantamiento de Categorías**, hacer la categorización por área de estudio, asignando el nombre del pre o postgrado, la escuela-postgrado y la facultad-centro de adscripción. Igualmente de esta porción de texto se estima viable extraer el nombre del tutor.

##### Colaboración:

Motivado a que en la primera iteración \@ref(scrapeo) para conformar el conjunto de datos se había obtenido el *url* asociado al documento soporte de la investigación, se procedió mediante un bucle a realizar la descarga de cada documento y extraer una cantidad de dos mil caracteres, partiendo del principio de que los trabajos de grado o tesis en sus primeras páginas tienen el nombre de la carrera o el postgrado, el nombre de la facultad-centro donde se cursó el estudio, el nombre del título al que optan y el nombre del tutor.

En esta iteración fue necesario realizar distintas adaptaciones para lograr la coincidencia de patrones. Teniendo en cuenta que son `r rowSums(df_cdad_postgrados) + 50` etiquetas las que se usarán para realizar la clasificación, llegando a tener 14 palabras algunas categorías, es elevada la probabilidad de que no se pueda hacer el "*pattern matching*" entre el texto y la etiqueta.

Lo anterior motivo a realizar un proceso de limpieza, modificación y disminución de la cantidad de palabras, tanto en las etiquetas como en el texto extraído. Se evaluó en cada adaptación cuáles razones impedían clasificar los documentos aún pendientes, se tomaron los correctivos y así se fue incrementando, de forma iterativa, la exactitud en este proceso.

También se tuvo que tomar en cuenta el orden en que se iba a ejecutar la secuencia de encontrar las coincidencias. Ejemplo es que varias facultades contienen las mismas tres palabras en la parte inicial de su nombre: *Facultad de Ciencias*, *Facultad de Ciencias* Jurídicas y Políticas, *Facultad de Ciencias* Económicas y Sociales y la *Facultad de Ciencias* Veterinarias. La secuencia para hacer la detección de la coincidencia fue buscar hacer la comparación de cada elemento ordenado en forma decreciente, según el total de caracteres que tenga el nombre de la facultad, para evitar clasificaciones erróneas.

Adicionalmente en el proceso de hacer coincidir las frases, se encontraron 17 postgrados que no estaban en el conjunto de datos de las categorías, los cuales se tuvo que proceder a agregar.

Para aquellos casos donde no se podía hacer *match,* se aplicó el algoritmo "Smith Waterman" [@smith1981], expuesto en el Capítulo del Marco Teórico \@ref(alghist), el cual permite alinear dos cadenas de texto cuando una de ellas no tiene coincidencia absoluta con la otra, como puede pasar en este caso por la introducción de caracteres adicionales.

Un elemento que introdujo ruido en el texto leído de los documentos, fue la aparición de diversos *encodings* que no resultó viable codificarlos a "UTF-08", haciendo que aparecieran caracteres no reconocidos dentro del texto, dificultando la tarea de lograr realizar el proceso de "pattern matching". El algorimo SW resultó eficaz para solucionar este problema, aunque igualmente se hicieron pruebas con otros métodos como el algoritmo de "Distancia de Levenshtein" o similares.

```{r lecturaclasificacion, echo=FALSE}
df_draw_seleccion <- readRDS('data/df_draw_seleccion.rds')%>%
  rename(Fecha= fecha_procesada)

sin_lectura <- as.numeric(table(nchar(df_draw_seleccion$texto_intro)==0)[2])+
  as.numeric(table(is.na(df_draw_seleccion$texto_intro))[2])
# 
df_clasificados <- df_draw_seleccion%>%
  filter(facultad != 'sin clasificación')

clasificados= nrow(df_clasificados)
# 
totales_clasificacion <- df_draw_seleccion%>%
  group_by(facultad)%>%
  count(facultad)%>%
  mutate(facultad= str_to_title(facultad) ) %>%
  rename(Total=n,Facultad= facultad)%>%
  filter(!is.na(Facultad))%>%
  filter(Facultad!='Sin Clasificación')
# 
totales_porarea <- table(df_draw_seleccion$nombre)%>%
  as.data.frame()
df_ascenso <- readRDS('data/df_ascenso.rds')
#   
# tutores <- as.numeric(table(df_draw_seleccion$texto_tutor=="sin información")[1])
# 
# tutores_distintos <-length( unique(df_draw_seleccion$texto_tutor))
```

Sobre un total de `r prettyNum(nrow(df_draw_seleccion),big.mark=".")` potenciales documentos se lograron clasificar `r prettyNum(clasificados,big.mark=".")` investigaciones, mientras que `r sin_lectura` no disponían información en el texto del documento y resultaba inviable hacer la categorización. En algunos casos esta falta de información estuvo motivada en que el archivo contenía imágenes por estar escaneado el contenido o también motivado en que los documentos anexos no eran trabajos de grado o tesis, sino informes de algún otro estilo.

La cantidad de categorías distintas, con al menos una investigación asignada a una facultad o centro, ascendió a `r nrow(totales_porarea)-1` que se distribuyen por Facultad según lo que se observa en la figura \@ref(fig:categorias)

```{r categorias,echo=FALSE, fig.cap='Cantidades de categorías por facultad y nivel académico', message=FALSE, warning=FALSE,fig.show='hold',out.width="70%"}
library(lubridate)


categorias_asignadas <- readRDS('data/categorias_asignadas.rds')

ggbarplot(categorias_asignadas,
          x='facultad',
          y='cdad',
          label.rectangle=FALSE,
          # label = distribucion$nn,
          fill = 'Jerarquía',
          # color = "#4785FF",
          color = 'Jerarquía',
          palette = "Paired",
          # label=TRUE,
          label=format(categorias_asignadas$cdad,),
          lab.size = 3,
          lab.col = 'black',
          # lab.pos = c("in"),
          # xticks.by=2,
          # ylab = TRUE,
          # yticks.by=100
)+
  theme(axis.text.x = element_text(color = "grey20",
                                   size = 6, 
                                   angle=30,
                                   hjust = 1,
                                   vjust = 1, 
                                   face = "plain"))+ 
  grids(axis='y')

fecha_min <- min(df_draw_seleccion$Fecha)%>%
  as.Date()%>%
  format('%m/%d/%Y')

fecha_max <- max(df_draw_seleccion$Fecha) %>%
  as.Date()%>%
  format('%m/%d/%Y')
```

En en la figura \@ref(fig:totalesporfacultad) se pueden ver primero la cantidad total de investigaciones que pudieron ser clasificadas por cada Facultad - Centro de estudios y en la segunda posición, la cantidad de documentos disponibles por fecha de publicación que abarcan el período `r fecha_min` al `r fecha_max`.

```{r totalesporfacultad,echo=FALSE, fig.cap='Cantidades de investigaciones clasificadas por Facultad y por año de publicación', message=FALSE, warning=FALSE,fig.show='hold',out.width="45%"}
#el cuadro @ref(tab:totalesporfacultad) y
library(ggpubr)
library(scales)
articulos_rm2 <- paste('en ',
                       'tutorado',
                       'señor ',
                       'm\\.sc\\.|m\\.sc |sc\\.|msc\\.|msc |ph\\.',
                       ' [a-z]\\: | [a-z]\\.| i\\: ',
                       'señora ',
                       'academico e industrial',
                       'industrial',
                       'academic[ao]s\\:|academic[oa]s\\.|academic[ao]s |academic[ao]\\:|academic[oa]\\.|academic[ao] |acad\\.\\:|acad\\.|acad ',
                       'autor[ea]s',
                       'autor\\.|autor ',
                       'cedula',
                       ' la ',
                       'alumnas',
                       'alumnos',
                       'licenciad[ao]\\.|licenciad[ao]',
                       'bachilleres',
                       'bachiler\\.|bachiller',
                       'br\\.',
                       'br ',
                       ' al ',
                       ' el ',
                       ' de ',
                       ' los ',
                       ' las ',
                       ' del ',
                       ' por ',
                       'profesora\\.|profesora |profª\\.',
                       'profesor[ae]s\\.|profesor[ae]s |profe\\.|profesor',
                       'titular|dedicacion exclusiva',
                       'profesor\\.|profesor ',
                       'profa\\.|prof\\. a ',
                       'prof\\.',
                       'prof ',
                       'profa ',
                       'abg\\.|abg ',
                       'arq\\.|arq |arqte |arqta ',
                       'propuesto',
                       'dra\\.|dra ',
                       't\\.s\\.u\\.|tsu\\.|tsu ',
                       'dr\\.|dr ',
                       'soc\\.',
                       'bra\\.|bra ',
                       'od\\.|oda\\.|odontolog[ao]',
                       'dsc\\.',
                       'dra ',
                       'esp\\.|esp |especialista',
                       ' m\\.',
                       ' eng\\.',
                       '\\(es\\)',
                       'dr |doc\\.|doc |drs\\.',
                       ' c\\.i\\. ',
                       'doctora\\.|doctora ',
                       'jefes',
                       'asesora',
                       'coordinadora',
                       'asesor',
                       'coordinador',
                       'doctor\\.|doctor ',
                       'lic\\.|lic |licd[ao]\\.|licd[ao] ',
                       'ingenier[oa]|ing\\º,ing\\.|ingº|ing ',
                       'lcd[ao]\\.',
                       'geolog[oa]',
                       'lcd[ao] ',
                       'pasante',
                       'tesistas|tesista',
                       'guia\\:,guia\\.|guia ',
                       'ftco\\.|ftico\\.',
                       'farmaceutic[ao]|mercadeo|toxicologia|farmacologo',
                       '\\:|\\-',
                       'econ\\.|econ ',
                       'zoot\\.|zoot ',
                       'ing\\.|ing ',
                       'agr\\.|agr ',
                       'mgs\\.|mgs |ms\\.|msg\\.| ms |mag\\.|mc\\.|mci\\.|mcs\\.|md\\.|mg\\.|mgr\\.|mgrs\\.|mgsc\\.|mgse\\.|mgster\\.|mv\\.',
                       'medic[ao]',
                       'phd\\.|phd ',
                       'magister\\.|magister ',
                       'scientiarum\\.|scientiarum |scientiarun ',
                       'caracas\\,|caracas\\.|caracas',
                       'venezuela\\,|venezuela\\.|venezuela',
                       'maracay\\,|maracay\\.|maracay',
                       sep = '|')

limpieza_texto4 <- function(texto){
  texto%>%
    enc2utf8()%>%
    str_replace_all('[Pp][Hh]\\.[Dd]\\. ',' ')%>%
    str_replace_all(' [A-Z]\\.[A-Z]\\. ',' ')%>%
    str_replace_all(' [A-Z]\\. ',' ')%>%
    str_replace_all('ª',' ')%>%
    str_replace_all('\\|',' ')%>%
    tolower()%>%
    gsub('á','a',.)%>%
    gsub('è','e',.)%>%
    gsub('é','e',.)%>%
    gsub('é','e',.)%>%
    gsub('í','i',.)%>%
    gsub('ó','o',.)%>%
    gsub('ú','u',.)%>%
    gsub('í','i',.) %>%
    gsub('ı́́','i' ,.)%>%
    gsub('ı́', 'i',.)%>%
    gsub('á' ,'a' ,.)%>%
    gsub('ó','o',.)%>%
    gsub('ó','o' ,.)%>%
    gsub('ú','u',.) %>%
    gsub('ñ','ñ',.)%>%
    str_replace_all('\\(a\\)\\.|\\(a\\)','')%>%
    chartr("áéíóú", "aeiou", .) %>%
    gsub('[[:digit:]]+',' ',.)%>%
    str_replace_all('[0-9]',' ')%>%
    # str_replace_all('[[:punct:]]',' ') %>%
    # str_replace_all('industrial',' ')%>%
    str_replace_all('\\(es\\)',' ')%>%
    # gsub('\\:|\\-|\\,|\\;|\\(|\\)',' ',.)%>%
    gsub('\\(|\\)','',.)%>%
    gsub(' i\\:','',.)%>%
    gsub('\\:|\\-|\\_|\\;|\\(|\\)',' ',.)%>%
    str_replace_all(articulos_rm2,' ')#%>%
    #str_squish()
}


# flextable(totales_clasificacion) |>
#   set_caption(caption='Cantidades de investigaciones clasificadas por Facultad')|>
#   theme_vanilla()|>
#   # set_table_properties(width = 1, layout = "autofit")
#   autofit(add_w = 0.2, add_h = 0)

cdad_tutores <- df_draw_seleccion%>%
  mutate(texto_tutor= limpieza_texto4(texto_tutor),
         texto_tutor= str_to_title(texto_tutor))%>%
  group_by(texto_tutor)%>%
  count()%>%
  arrange(desc(n))%>%
  filter(!str_detect(texto_tutor,'Sin Informacion|^Autora|^Autor'))%>%
  filter(nchar(texto_tutor)>4)

distribucion <- cdad_tutores%>%
  group_by(n)%>%
  count()%>%
  mutate(cdad=as.integer( nn*n))%>%
  rename(`Cdad. investigaciones tuteladas por mismo tutor`=n,
         'Frecuencia'=nn)%>%
  ungroup()

masdeunaportutor <- distribucion%>%
  slice(-1)%>%
  pull(cdad)%>%
  sum()

ggdotchart(totales_clasificacion,
           x = "Facultad", 
           y = "Total",
           color = "#A1CAE3",                                # Color by groups
           # palette = c("#00AFBB"), # Custom color palette
           sorting = "descending",                       # Sort value in descending order
           add = "segments",                             # Add segments from y = 0 to dots
           rotate = TRUE,                                # Rotate vertically
           # group = "cyl",                                # Order by groups
           dot.size = 6,                                 # Large dot size
           label = format(totales_clasificacion$Total, big.mark = ".", decimal.mark= ","),                        # Add mpg values as dot labels
           font.label = list(color = "black", size = 12,
                             vjust = 0.5,hjust=.6),               # Adjust label parameters
           ggtheme = theme_pubr(),
           xlab=FALSE,
           ylab = FALSE,
           title ='Cantidad Investigaciones por Facultad',
           font.title=list(size=12, color='#3D3D3D'))%>%
  ggpar(subtitle='')+
  scale_y_continuous(labels = comma_format(big.mark = ".",
                                           decimal.mark = ","),limits=c(0,3100))+
  theme(axis.text.x = element_text(face="bold", color='#3D3D3D', 
                                   size=11 ))+
  theme(axis.text.y = element_text( color='#3D3D3D', 
                                   size=8))




gghistogram(df_draw_seleccion,
            x = "Fecha",
            bins = 25, 
                 fill = "#A1CAE3", color = "#4785FF", rug = TRUE)%>%
    ggpar(subtitle='Cantidad de documentos disponibles por año de publicación',
          ylab='cantidad')+
  scale_y_continuous(labels = comma_format(big.mark = ".",
                                           decimal.mark = ","))+ 
  grids(axis='y')

```

En cuanto a la obtención de los nombres de los tutores, el procedimiento adoptado fue nuevamente realizar algunas limpiezas sobre el texto como remover digitos, abreviaturas de títulos académicos (PhD, MgS, etc), y luego extraer el texto que se encontraba delimitado entre la propia palabra "tutor" y el brinco de línea "\\n" más próximo a la aparición de dicha palabra. Con el procedimiento descrito se pudo extraer un total de `r prettyNum(sum(cdad_tutores$n),big.mark=".")` nombres, equivalente al `r prettyNum(round(sum(cdad_tutores$n)/nrow(df_draw_seleccion)*100,1),decimal.mark = ",")`% de las investigaciones, así como `r prettyNum(nrow(cdad_tutores),big.mark="." )` nombres únicos. En la figura \@ref(fig:tutores) se aprecia la frecuencia para la cantidad de investigaciones que corresponden a un mismo tutor.

```{r tutores,echo=FALSE, fig.cap='Histógrama Nombres Tutores Extraídos ', message=FALSE, warning=FALSE,fig.show='hold',out.width="65%"}

textos_smith <- readRDS('data/textos_smith.rds')
per_smith <- round(nrow(textos_smith)/nrow(df_draw_seleccion)*100,1)
ggbarplot(distribucion,
          x='Cdad. investigaciones tuteladas por mismo tutor',
          y='Frecuencia',
          label.rectangle=FALSE,
          # label = distribucion$nn,
          fill = "#A1CAE3", 
          color = "#4785FF",
          # label=TRUE,
          label=format(distribucion$Frecuencia, big.mark = ".", decimal.mark= ","),
          lab.size = 2,
          lab.col = 'black',
          lab.pos = c("out"),
          xticks.by=2,
          # ylab = TRUE,
          # yticks.by=100
          )+
  theme(axis.text.x = element_text(color = "grey20",
                                   size = 6, 
                                   # hjust = .5, 
                                   vjust = .5, 
                                   face = "plain"))+
  scale_y_continuous(labels = comma_format(big.mark = ".",
                                           decimal.mark = ","))

```

La cantidad de trabajos donde fue detectado que un tutor tuvo dos o más trabajos tutelados es `r prettyNum(masdeunaportutor, big.mark = ".",decimal.mark = ",")`, equivalente a un `r prettyNum(round(masdeunaportutor/nrow(df_draw_seleccion)*100,1),decimal.mark = ",")`%. La cifra anterior tiene la significancia de mostrar que para más de la mitad de los trabajos existe la certeza de que se encontró un nombre idéntico, en al menos dos casos, brindando mayor confianza sobre el proceso ejecutado.

En los otros `r prettyNum(max(distribucion$Frecuencia), big.mark='.')` casos, en que sólo se encontró un tutor por investigación, al realizar un análisis exploratorio, se pudo apreciar que en una variedad de casos el nombre no fue escrito en forma idéntica entre un trabajo y otro, p. ej. dejando unicamente la inicial del segundo apellido u omitiendo el segundo nombre. Escapa al alcance de este trabajo realizar la depuración que permita consolidar más trabajos de grado al correspondiente tutor.

##### Aprender:

A continuación se agrupan y enuncian los principales inconvenientes que se encontraron y se indica el aprendizaje obtenido para fases sucesivas de desarrollo:

1.  Aparición de errores en la redacción y ortográfia por parte de los autores, como por ejemplo, escribir incorrectamente el nombre del título al que optan o la facultad donde realizaron los estudios. Esto implicó crear reglas para realizar reemplazos de palabras en los textos y limpiezas para disminuir el ruido y facilitar el proceso de obtener la coincidencia. Un problema que se presentaría al intentar extender el SCSU a otros repositorios, cuando los trabajos de grado no estén categorizados, es que el procedimiento anterior no es completamente generalizable si los nombres de los postgrados-escuelas y el de las facultades son distintos, ya que las reglas de modificación de texto son específicas para el corpus de la Universidad Central de Venezuela.

2.  Cambios en el estilo y formalidades con que se deben presentar los documentos de grado en las distintas facultades o niveles académicos, cuestión que dificultó la detección de las reglas para hacer la comparación. Ante esto se buscó encontrar las formas más genéricas en el procesamiento, así como la adopción de un procedimiento que progresivamente intentaba obtener la coindicidencia: primero con el nombre del título, en caso de fallo se sigue con el nombre del pregrado o postgrado, si nuevamente fallaba se procedía a hacer la búsqueda del nombre de la facultad y finalmente si ninguna de las estrategias anteriores tenía éxito, se aplicaba el algoritmo "Smith Waterman".

    Con este último método se pudieron clasificar `r prettyNum(nrow(textos_smith), big.mark='.')` trabajos, que equivalen a un `r prettyNum(per_smith,decimal.mark = ',')`% del total. Es importante señalar que al aplicar este recurso se pueden generar "falsos positivos" y por esto en la Sección \@ref(pruebas) se hace una estimación estadística del error que puede representar acudir a este método.

3.  Dentro de los archivos descargados se encontraron algunos en formato de presentaciones *power point* los cuales fueron desechados sólo siendo procesados los que estuviesen en formato *word* o *pdf* . Esto llevo a condicionar que para ejecutar la descarga y hacer el procesamiento de extracción de datos, la extensión debía ser alguna de las mencionadas como válidas*.*

4.  También se encontraron trabajos que contaban con más de un archivo disponible para descargar. En la fase inicial se descargaron 12.765 de documentos aunque la cantidad de trabajos disponibles en el repositorio (incluyendo duplicados) era `r prettyNum(sum(duplicados_tegs$Disponibles),big.mark=".")`. Al evaluar las razones que motivaban que existiera una cantidad superior de archivos vs. documentos, se distinguieron casos en que por investigación existía un archivo por capítulo y no se encontraba algún elemento que indicará la secuencia en que se debía cargar cada archivo para armar el documento unificado. Para estos casos sólo se tomó el primer archivo en la lista de *url*´s disponibles para tratar de hacer el proceso de clasificación.

5.  Se hicieron algunas simplificaciones sobre postgrados que dependen de dos facultades imputándolo sólo a una que fuese la primera en aparecer en el texto. Como queda fuera del alcance de esta Investigación determinar los casos en que existen este tipo de adscripciones compartidas se hizo esta simplificación [^05-desarrollo-4].

6.  Algunos trabajos en su primera página incluyen el nombre de dos facultades o escuelas creando errores en la clasificación. P. ej., investigaciones que indican en la portada el siguiente texto "Facultad de Ciencias, Escuela de Computación, título: se realiza la propuesta de un sistema de gestión académica para la Escuela de Economía de la Facultad de Ciencias Económicas....", lo cual genera mútilples clasificaciones. En estos casos se optó por realizar la imputación con base en la primera coincidencia detectada.

7.  En la obtención del nombre del tutor es importante destacar que en varios casos el texto extraído no se corresponde propiamente al nombre del mismo, motivado a que la escritura de la portada puede responder a la representación visual. Se encontraron trabajos que tienen tutor académico, tutor industrial, cotutor y otras variantes, donde se hace una disposición en la escritura de colocar el tipo de cada tutor alíneado, cada uno en los extremos de una línea y los nombres también se disponen en los extremos de la línea superior, quebrando la regla de extracción que se había diseñando. Esto pareciera un problema a enfrentar con técnicas de segmentación de archivos que tienen en consideración la disposición visual. En el caso de esta investigación no se adoptaron métodos para abordar este problema. En las Sección \@ref(pruebas) se hace una evaluación estadística de la precisión alcanzada en esta extracción.

8.  Se simplificaron algunos nombres de especializaciones por la cantidad de palabras que tienen estableciendo un límite, o *prunning*, de 5 palabras para el nombre del postgrado, lo que implica que algunos trabajos habrán quedado agrupados en la misma categoría. Estos casos mayormente están asociados a las especializaciones en el área de medicina, teniendo de ejemplo la figura \@ref(fig:especial).

    ```{r especial, echo=FALSE, fig.align='center', fig.cap=c('Ejemplo de nombres simplificados'), fig.show='hold', out.width="50%"}


    knitr::include_graphics(c("images/05-desarrollo/1_ciclo/especializaciones2.png"))

    ```

9.  En este proceso de clasificación no resultaba conveniente usar técnicas de aprendizaje automático dada la cantidad de categorías y el gran desbalanceo de clases.

10. Se detectó que en Saber UCV en la categoría que se denomina "otros" en Saber UCV se encuentran documentos que corresponden a especializaciones y también se alojan `r nrow(df_ascenso)` investigaciones que son "trabajos de ascenso" de profesores.

[^05-desarrollo-4]: Un ejemplo de esto es la Maestría en Física Médica que es impartida de manera conjunta por la Facultad de Ciencias y por la Facultad de Medicina. Al no disponer de información oficial sobre otros casos de postrgrados que presenten esta característica, se adoptó el método mencionado de imputar sólo a una dependencia que sea la primera en aparecer en el texto.

#### Objetivos alcanzados:

1.  Obtener las fichas de `r prettyNum(nrow(df_draw_seleccion),big.mark=".")` investigaciones, equivalente al 100% de los trabajos de pre y postgrado alojados en Saber UCV (descartando los duplicados), conforme a lo propuesto en el Objetivo Específico 1.

2.  Obtener `r rowSums(df_cdad_postgrados)` nombres de postgrado y 50 de carreras de pregrado, generando 454 etiquetas en total que sirvieron de insumo para alcanzar lo propuesto en el Objetivo Específico 2.

3.  Categorizar `r prettyNum(clasificados,big.mark=".")` investigaciones por área académica, equivalente al `r  prettyNum(round( clasificados/nrow(df_draw_seleccion)*100,1), decimal.mark=",")`% de toda la información a clasificar, de acuerdo al Objetivo Específico 2.

4.  Extraer `r prettyNum(sum(cdad_tutores$n),big.mark=".")` nombres de tutores, equivalente al `r prettyNum(round(sum(cdad_tutores$n)/nrow(df_draw_seleccion)*100,1),decimal.mark = ",")`% del total de investigaciones, que también se estableció como parte del Objetivo Específico 2.

\newpage

<br> <br>

### Ciclo-Prototipo del SCSU: {#desarrollociclos3}

En este ciclo se desarrolló el **Prototipo del Sistema Complementario Saber UCV** con tres iteraciones. En la primera iteración de este ciclo \@ref(iternlp) se realizó la **Preparación del Corpus**. En la segunda iteración se hicieron las pruebas para realizar las **Recomendaciones** de cada documento, basado en la similitud que presente con el resto de las investigaciones. En la tercera \@ref(imrecomendacion) \@ref(iterbol) se creó una aplicación web interactiva en la que se implementó el **Prototipo** donde ante un *query* se presentan los resultados, ordenados según criterios de relevancia y se representan los **"Mapa de Conocimiento"** (ver \@ref(mapacon)) .

Todo las rutinas y codificaciones que se mencionan a continuación fueron realizadas en el lenguaje `r version$version.string[1]` [@R].

#### Iteración- Preparación del Corpus: {#iternlp}

En esta iteración el Corpus que se conformó en \@ref(desarrollociclos1), fue sometido a distintos procesamientos conocidos como "Preparación del Corpus" mediante la aplicación de técnicas de PLN, ver \@ref(nlproc).

##### Especulación:

Creando un corpus anotado con métodos del Procesamiento del Lenguaje Natural se facilita el acceso a información de relevancia para los investigadores mediante el análisis lingüístico de los documentos recolectados [@article].

##### Colaboración:

Para realizar el etiquetado de la "Parte del Discurso (POS)" \@ref(pos), se hizo una revisión exhaustiva de distintos *frameworks* para realizar el anotado del Corpus, como *Freeling* [@padro12], CoreNLP [@manning-etal-2014-stanford], UDPIPE [@udpipe-2] y se decidió acudir a la librería "spacyr" [@spacyr] que es un *wrapper* de la librería Spacy [@spacy2020], que se ejecuta en el lenguaje python. Esta librería, en lo relativo al etiquetado de las palabras por su función gramátical, lo hace acorde al marco de trabajo propuesto en la investigación "Universal Dependencies" [@demarneffe2021]. La implementación de *Spacy* dispone de un encadenamiento en el flujo de trabajo, resultando conveniente aplicarlo dentro del desarrollo del SCSU. Otra de las características que dispone es contar con varios modelos preentrenados para realizar el etiquetado, así como disponer de métricas de desempeño que se encuentran dentro del estado del arte en el campo del Procesamiento del Lenguaje Natural.

La librería carga el modelo preentrenado de aprendizaje automático para el idioma español denominado "es_core_news_lg". Mediante un *pipeline* que se observa en la figura \@ref(fig:spacypi) se ejecutan procesos que permiten conformar el Corpus Anotado.

```{r spacypi,echo=FALSE, fig.align='center', fig.cap='Arquitectura General pipeline Spacy',  out.width="80%"}
knitr::include_graphics("images/05-desarrollo/2_ciclo/nlp/spacy_pipeline1.png")
```

Ilustrativamente se muestra el texto "*... y algunos no-metales. Contrariamente a lo...*", que al aplicar los métodos que dispone "spacyr", permite generar el Corpus Anotado que se puede ver en la figura \@ref(fig:corpusano), donde se aprecia:

-   id del documento

-   id de la oración dentro del documento

-   id del token dentro del documento

-   token

-   lema

-   etiquetado de parte del discurso (POS) para cada token.

```{r corpusano, echo=FALSE, fig.align='center', fig.cap='Detalle anotación del Corpus',  out.width="90%" }
pos_total <- readRDS('data/pos_total.rds')
total_adj <- pos_total%>%
  filter(POS=='ADJ')%>%
  pull(Cantidad)

total_noum <- pos_total%>%
  filter(POS=='NOUN')%>%
  pull(Cantidad)

cdad_vocabulario <- readRDS('data/cdad_vocabulario.rds')
cdad_lemmas <- readRDS('data/cdad_lemmas.rds')
total_tokens <- sum(pos_total$Cantidad)
knitr::include_graphics("images/05-desarrollo/2_ciclo/nlp/corpusanotado2.png")
```

El Corpus Anotado cuenta con `r prettyNum(nrow(df_draw_seleccion), big.mark='.')` documentos que generan `r prettyNum(total_tokens, big.mark = '.')` tokens, un vocabulario de `r prettyNum(cdad_vocabulario, big.mark = '.')` palabras distintas y `r prettyNum(cdad_lemmas, big.mark = '.')` lemas únicos. Los tokens, agrupados por función gramatical se presentan [^05-desarrollo-5] en el gráfico \@ref(fig:posgr) :

[^05-desarrollo-5]: El proyecto Universal Dependencies disponible en el enlace <https://universaldependencies.org> contiene la documentación sobre las distintas funciones gramaticales que fueron etiquetadas.

```{r posgr,echo=FALSE, fig.align='center', fig.cap='Cantidad de palabras por función gramatical',  out.width="70%"}
library(ggpubr)


ggdotchart(pos_total,
           x = "POS", 
           y = "Cantidad",
           color = "#4785FF",                                # Color by groups
           # palette = c("#00AFBB"), # Custom color palette
           sorting = "descending",                       # Sort value in descending order
           add = "segments",                             # Add segments from y = 0 to dots
           rotate = TRUE,                                # Rotate vertically
           # group = "cyl",                                # Order by groups
           dot.size = 8,                                 # Large dot size
           label = format(pos_total$Cantidad, big.mark = ".", decimal.mark= ","),                        # Add mpg values as dot labels
           font.label = list(color = "black", size = 10,
                             vjust = 0.5),               # Adjust label parameters
           ggtheme = theme_pubr(),
           xlab=FALSE,
           ylab = FALSE,
           
           title ='Corpus Anotado',
           font.title=list(size=14, color='#3D3D3D'))%>%
  ggpar(subtitle='cantidad de palabras por función gramatical')+
  scale_y_continuous(labels = comma_format(big.mark = ".",
                                           decimal.mark = ","),limits=c(0,800000))+
  theme(axis.text.x = element_text(face="bold", color='#3D3D3D', 
                                   size=11 ))+
  theme(axis.text.y = element_text( color='#3D3D3D', 
                                   size=8))
```

```{r posfun, echo=FALSE, eval=FALSE, fig.cap='cantidad de palabras por función gramatical', message=FALSE, warning=FALSE}

flextable(pos_total) |>
  set_caption(caption='Totales Etiquetado Gramatical')|>
  theme_vanilla()|>
  # set_table_properties(width = 1, layout = "autofit")
  autofit(add_w = 0.2, add_h = 0)
```

Realizando el análisis exploratorio del Corpus, es interesante ver la cantidad de palabras únicas por función gramatical y el ratio que presentan con respecto al total de palabras por "POS" que se vio en la figura \@ref(fig:posgr). En la tabla \@ref(tab:lemmasg) se observa que las categorías del "etiquetado de la parte del discurso" que contienen mayor cantidad de información son los nombres propios (PROPN), los adjetivos (ADJ), números (NUM) y los sustantivos (NOUN), pero los nombres propios al ser únicos en cada trabajo y los números al representar cantidades, realmente la mayor cantidad de información del Corpus se encuentra en los adjetivos y sustantivos, lo que refuerza la construcción y representación de los "Mapas de Conocimiento" mediante la selección de las palabras que tienen esas funciones gramatical.

```{r lemmasg, echo=FALSE, fig.cap='Cantidad de palabras por función gramatical', message=FALSE, warning=FALSE}

lemmas_agrupados <- readRDS('data/lemmas_agrupados.rds')

flextable(lemmas_agrupados) |>
  set_caption(caption='Ratio de Cantidad Única de lemas Vs. Cantidad total de lemas')|>
  theme_vanilla()|>
  # set_table_properties(width = 1, layout = "autofit")
  autofit(add_w = 0.2, add_h = 0)|>
  style(j = 4, 
        pr_t = fp_text_default(
          # italic = TRUE, 
          color = "#4785FF"))

```

Contar con un Corpus Anotado permite ir inspeccionando los patrones que se presentan en él. Un ejemplo se tiene en la figura \@ref(fig:compususadj) donde mediante el método *Rake*, que basado en la determinación de la frecuencia de palabras y patrones co-ocurrentes, logra extraer términos clave. En la figura se muestran las palabras claves asociadas al subconjunto de trabajos que fueron realizados en la Escuela de Computación.

```{r compususadj, echo=FALSE, fig.align='center', fig.cap=c('Palabras Claves en investigaciones de la Escuela de Computación'), fig.show='hold', out.width="60%"}

knitr::include_graphics("images/05-desarrollo/2_ciclo/nlp/computacion_noun_adj.png")
```

La tabla que conforma el Corpus Anotado para esta fase de la Investigación se alojó en memoria RAM en una estructura de datos tabular denominada *dataframe* del lenguaje R, sin ser registrada en un gestor de base de datos.

##### Aprender:

Al analizar el Corpus anotado se detectaron algunos puntos a considerar para análisis posteriores.

-   El proceso de separación por tokens y de lematización, como ya se mencionó, se hizo con la librería "spacyr", que hace estos procesos mediante el uso de un modelo preentrenado de aprendizaje automático. Hubo ciertas palabras, o términos que son muy específicos de un área de conocimiento, como en la química, donde los *compuestos, moléculas, nomenclaturas* u otros, contienen denominaciones conformadas por cadenas de letras mayúsculas seguidas de puntos, que no presentan estructuras gramaticales propias del lenguaje natural, sino de un dominio específico, ver figura \@ref(fig:quimica). Al tener un modelo intentando hacer la separación de los tokens o el etiquetado, bajo unos textos con los cuales no fue entrenado, el proceso falla en la clasificación. Este problema, para ese tipo de términos, no fue resuelto al no disponer con un modelo entrenado para este dominio. En investigaciones posteriores pudiera realizarse un sobreentrenamiento que incluyera el etiquetado de estos *tokens* de dominios muy especializados, como generalmente se presenta en carreras científicas y así poder mejorar la fase del etiquetado de las funciones gramaticales (POS).

    ```{r quimica, echo=FALSE, fig.align='center', fig.cap=c('Texto "Resumen" de Trabajo de Grado de Maestría en Química. Autora: Margarita González'), fig.show='hold', out.width="85%"}

    knitr::include_graphics("images/05-desarrollo/2_ciclo/quimica.png")
    ```

-   La lematización permitió reducir en un `r prettyNum(100-round((cdad_lemmas/cdad_vocabulario)*100,1), decimal.mark = ',')` % el vocabulario al pasar de `r prettyNum(cdad_vocabulario, big.mark = '.')` palabras a `r prettyNum(cdad_lemmas, big.mark = '.')` lemas. Específicamente los lemas que se van a usar en fases posteriores del análisis, son los que están categorizados en el etiquetado del discurso como "ADJ" (adjetivos) que totalizan `r prettyNum(total_adj, big.mark='.')` 289.283, equivalentes a un `r prettyNum(round((total_adj/total_tokens)*100,1), decimal.mark = ',')`% del total de palabras y los "NOUMS" (sustantivos) son `r prettyNum(total_noum, big.mark='.')` equivalentes al `r prettyNum(round((total_noum/total_tokens)*100,1), decimal.mark = ',')`% de las palabras presentes en el corpus. En el registro en base de datos se conservará completo el corpus anotado independientemente de la función gramatical con se etiquetasen.

-   Los textos del "Resumen" que se encuentran en la ficha de cada trabajo alojado en Saber UCV, en `r sum(str_detect(df_draw_seleccion$resumen,'(?i)abstract:|(?i)abstract '))` casos, equivalentes al `r round(sum(str_detect(df_draw_seleccion$resumen,'(?i)abstract:|(?i)abstract'))/nrow(df_draw_seleccion)*100,1)`% del total, contienen una porción de texto en idioma inglés referente al "Abstract", siendo óptimo aislar únicamente las partes que se encuentran en idioma español para que funcione correctamente el "etiquetado de la parte del discurso".

#### Iteración - Recomendación Documentos: {#imrecomendacion}

En esta iteración se revisó la creación de recomendaciones de investigaciones, basándose en la similitud que presente un documento con el resto de los documentos presentes en el Corpus.

##### Especulación:

Se ha determinado que los investigadores no necesariamente realizan la búsqueda de documentos que le puedan ser de interés mediante un *query* que contenga términos claves sino a partir de un documento que les resulta de interés, quieren localizar otros que puedan compartir ciertos aspectos [@zhou2018].

La similitud de un documento con otro se puede entender, en esta implementación, como aquellos que tienden a compartir palabras y dentro de una "term document matrix" generan vectores similares [@jurafsky2009]. Es por esto que se considera que los documentos que presenten mayor similitud, pueden resultar de interés para los investigadores, expandiendo así las posibilidades de inspección del Corpus.

##### Colaboración:

Mediante la creación de una "term document matrix", vista en \@ref(tdm), usando el framework para análisis cuantitativo de textos "quanteda" [@quanteda], la matriz obtenida refleja características esenciales de los documentos, en lo relativo a las palabras que lo conforman y su frecuencia.

En cada fila de la matriz se representa un documento, también equivalente a un vector. Midiendo la similitud *coseno*, revisada en \@ref(similitud), con la función `textstat_simil`, entre un determinado documento y el resto de los que conforman el corpus, es viable determinar cuáles son los más "parecidos".

Se realizó un proceso iterativo para calcular la similitud coseno entre documentos y se pudo apreciar que los resultados de similitud pueden resultar de interés en diversos casos, como el que se muestra en la figura \@ref(fig:similitudreco).

```{r similitudreco, echo=FALSE, fig.align='center', fig.cap='Resultado de recomendaciones al documento: "Diseño de un sistema para la desinfección de aguas de consumo humano y de uso industrial empleando un material inorgánico antibacterial"', fig.show='hold', out.width="90%"}

knitr::include_graphics("images/05-desarrollo/2_ciclo/similitud_reco2.png")
```

##### Aprendizaje:

Es necesario señalar que el método adaptado no es se considera que esté dentro del "Estado del Arte" para realizar recomendaciones, no obstante, se hace la evaluación de este recurso dentro del SCSU, al ser de sencilla implementación, cálculos rápidos, robustos que no consumen mayores recursos computacionales, los cuales quedan disponibles para otras tareas que ejecuta el SCSU.

#### Iteración- Implementación Prototipo: {#iterbol}

En esta iteración se hace el desarrollo del Prototipo de un "Sistema de Recuperación de Información" que servirá de base para probar distintas funcionalidades con las cuales contará el Sistema Complementario Saber UCV. En particular se hace el Prototipo de la aplicación web, mediante el uso del framework Shiny y se establecen los casos de uso, para que el usuario pueda realizar procesos de búsqueda, sobre el corpus anotado que se conformó en la iteración anterior \@ref(iternlp).

En este Prototipo solo se va a trabajar con el subconjunto de las investigaciones que han sido realizadas en la Facultad de Ciencias y ante un *query*, son extraídos los documentos que contengan tales palabras y con ellos se representan los "Mapas de Conocimiento".

Esta implementación se apoya en el uso del sistema gestor de base de datos PostgreSQL, ya que este software nativamente cuenta con la funcionalidad de construir un índice invertido, sobre un conjunto de documentos y así se da sustento a las "búsquedas de texto completo".

##### Especulación:

Contar con un prototipo permitirá evaluar las posibles interacciones entre el usuario y la versión que entre a producción del Sistema Complementario Saber UCV, así como analizar los tiempos de respuesta y los resultados de las visualizaciones.

Para el desarrollo de este prototipo se seleccionan los textos resúmenes que fueron categorizados como pertenecientes a la Facultad de Ciencias.

Durante la fase de especulación de este ciclo se realizaron los Diagramas de Caso de Uso que se ven en las figuras \@ref(fig:protoUC1) y \@ref(fig:protoUC11):

\newpage

```{r protoUC1, echo=FALSE, fig.align='center', fig.cap='Caso de Uso 1 - Prototipo SCSU - Nivel 1', out.width=c("45%"), fig.show='hold'}

knitr::include_graphics("images/05-desarrollo/2_ciclo/UC/prototipo_nivel1.png")

```

```{r protoUC11, echo=FALSE, fig.align='center', fig.cap='Caso de Uso 1.1 - Prototipo SCSU - Nivel 2', out.width=c("45%"), fig.show='hold'}

knitr::include_graphics("images/05-desarrollo/2_ciclo/UC/prototipo_nivel2.png")
```

En el cuadro \@ref(tab:prototipoUC1) se muestra el caso de uso UC.1 del Prototipo del SCSU.

\newpage

```{r prototipoUC1, echo=FALSE}

titulo_tabla <- 'Prototipo SCSU UC.1'

nombre_uc <- 'UC.1: Realizar proceso de recuperación de información (query)'

descripcion_uc <- 'El usuario realiza búsquedas sobre los textos que conforman el Corpus del Prototipo'

actor_uc <- 'usuario'

flujo_basico <- 'El caso de uso inicia cuando el usuario ingresa a la aplicación y el prototipo muesta dos campos: \n 1) En el primero debe introducir el texto a buscar. \n 2) En el segundo, en el que aparece por defecto el valor 60, se corresponde con las cantidades de coocurrencias que serán presentadas en los "Mapas del Conocimiento". \n 3) El usuario hace "clic" en el "buscar palabras" \n 4) El Prototipo presenta los resultados obtenidos \n 5) El caso de uso termina'

#######flujo alternativo
flu_alt_titulo <- c('N/A')
flu_alt_descrip <- c('N/A')

#######Precondiciones
precond_titulo <- c('N/A')
precond_descri <- c('N/A')

#######Postcondiciones
postcond_titulo <- c('Éxito','Fracaso')
postcond_descri <- c('El prototipo presenta: \n 1) Total de investigaciones por jeraraquías de las investigaciones que fueron recuperadas \n 2) Gráfico con "Mapas de Conocimiento". \n 3) Tabla con datos de los texto Resumen recuperados de acuerdo al query','N/A')



#####
df_encabezado=data.frame(`a`=c('Nombre',
                     'Descripción',
                     'Actor',
                     'Flujo de Eventos',
                     'Flujo Básico',
                     flujo_basico,
                     'Flujo Alternativo',
                     'Título'),
               `b`=c(nombre_uc,
                    descripcion_uc,
                    actor_uc,
                    '',
                    '',
                    '',
                    '',
                    'Descripción'))


df_flujo_alt <- data.frame(`a`=flu_alt_titulo,
                  `b`=flu_alt_descrip)

df_precondiciones <- data.frame(`a`=c('Precondiciones',
                                      'Título',
                                      precond_titulo),
                                `b`=c('',
                                      'Descripción',
                                      precond_descri))

df_postcondiciones <- data.frame(`a`=c('Postcondiciones',
                                       'Título',
                                       postcond_titulo),
                                 `b`=c('',
                                       'Descripción',
                                       postcond_descri))


indice_1=nrow(df_encabezado)+nrow(df_flujo_alt)+1
indice_2=nrow(df_encabezado)+nrow(df_flujo_alt)+nrow(df_precondiciones)+1
######

df <- bind_rows(df_encabezado,
                df_flujo_alt,
                df_precondiciones,
                df_postcondiciones)


flextable(df)%>%
  delete_part(part = "header")%>%
  theme_box()%>%
  fontsize(size = 10)%>%
  bg(i=1,j=1, bg = "#C2C2C2", part = "body")%>%
  bg(i=2,j=1, bg = "#C2C2C2", part = "body")%>%
  bg(i=3,j=1, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=4)%>%
  bg(i=4, bg = "#8F8F8F", part = "body")%>%
  merge_at(i=5)%>%
  bg(i=5, bg = "#A3A3A3", part = "body")%>%
  merge_at(i=6)%>%
  merge_at(i=7)%>%
  bg(i=7, bg = "#A3A3A3", part = "body")%>%
  bg(i=8, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=indice_1)%>%
  bg(i=indice_1, bg = "#A3A3A3", part = "body")%>%
  bg(i=indice_1+1, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=indice_2)%>%
  bg(i=indice_2, bg = "#A3A3A3", part = "body")%>%
  bg(i=indice_2+1, bg = "#C2C2C2", part = "body")%>%
  set_caption(caption=titulo_tabla)%>%
  align(i=4, align = 'center')%>%
  # align(i=5, align = 'center')%>%
  align(i=indice_1, align = 'center')%>%
  align(i=indice_2, align = 'center')%>%
  width(j = 1, width = 1)%>%
  width(j = 2, width = 6)%>%
  height(height = .1)
  
  

```

En la figura \@ref(fig:sidebar) se muestra el mock-up de la Interfaz para el prototipo del SCSU.

```{r sidebar, echo=FALSE, fig.align='center', fig.cap='Mock-Up de campos de entrada en la Interfaz del Prototipo', out.width="25%"}

knitr::include_graphics("images/05-desarrollo/2_ciclo/UI/prototipo_sidebar.png")
```

En cuanto a los "Mapas de Conocimiento (MC)", motivado a que visualizar gráficamente los resultados es una forma conveniente de ayudar a los usuarios a descubrir relaciones y patrones presentes en los textos que pueden estar ocultos [@li2018], en la figura \@ref(fig:mapacon) se muestra la propuesta de representación de los "MC" en el Prototipo.

```{r mapacon, echo=FALSE, fig.align='center', fig.cap='Representación Mapas de Conocimiento', out.width="70%"}

knitr::include_graphics("images/05-desarrollo/2_ciclo/UI/mapcon.png")

```

Para poder implementar la búsqueda de texto se usará *postgreSQL V16*, por lo cual en la fase de "especulación" se realizó el diseño del modelo "entidad-relación" que se ve en detalle en la figura \@ref(fig:entrel).

```{r entrel, echo=FALSE, fig.align='center', fig.cap='Modelo Entidad-Relación', out.width=c("60%")}

knitr::include_graphics("images/05-desarrollo/2_ciclo/esquemas/diagrama_entidadrel.png")
```

En el modelo, la entidad "Corpus" se apoya en la tabla "POS" que contiene el detalle del etiquetado de la parte del discurso para cada palabra (token) presente en cada documento, mientras que la tabla "Indice Invertido" se destinará a almacenar la estructura que permite general la búsqueda de texto y básicamente almacena cada palabra que compone el vocabulario y el *id* de los documentos donde aparece cada palabra.

Un factor determinante para la selección de PostgreSQL es el soporte que tiene para ejecutar procesos de "Busqueda de Texto Completo (*Full Text Search*)" en el idioma español.

El esquema de la arquitectura general de la aplicación, el cuál se implementará en el *framework* shiny, se representa en la figura \@ref(fig:esqshinyproto).

```{r esqshinyproto, echo=FALSE, fig.align='center', fig.cap='Esquema General del Prototipo de la Aplicación', out.width="50%"}
knitr::include_graphics("images/05-desarrollo/2_ciclo/esquemas/shinyappproto.png")
```

##### Colaboración:

La aplicación web que formará parte del Prototipo, una vez que se tiene conformado el corpus anotado, como se vio en \@ref(iternlp), se debe iniciar el "Poblado de la Base de Dato" con las tres tablas que se mostraron en el diagrama de entidad relación propuesto en \@ref(fig:entrel) . A continuación se van a describir los procesos con los que se realizó el "Poblado**"** que sirve al Prototipo.

1.  **Tabla Corpus**: con los datos del Corpus Anotado, se conforma la tabla con los siguientes datos: jerarquía, título, nombre autor, fecha publicación, nombre de tutor, texto resumen, palabras claves, nombre de la facultad y nombre de la escuela-postgrado y el código único identificador de cada documento.

2.  **Tabla "indiceinvertido":** mediante una extensión de *PostrgreSQL V16.1* denominada `"TS_Vector`", se crea una columna de nombre `"document_tokens"`, donde se almacena una estructura de datos de tipo "`tsvector`", que facilita al gestor de BD la búsqueda de texto completo. El *script* siguiente hace el trabajo de crear el índice invertido mediante la función `"to_tsvector"`, para los datos del "título", "palabras claves", "autor" y del "resumen". En el mismo proceso se añaden pesos distintos (importancia para el criterio de reordenamiento de los resultados, según lo revisado en \@ref(ranking) ) a cada atributo mediante la función `"setweight"`.

    \newpage

    ```{SQL, echo=TRUE, eval=FALSE}
    ALTER TABLE Corpus ADD COLUMN document_tokens tsvector
    GENERATED ALWAYS AS ((setweight(to_tsvector('spanish',
                            coalesce(autor,'')),'A') ||  
                          setweight(to_tsvector('spanish',
                            coalesce(titulo,'')),'B')||  
                          setweight(to_tsvector('spanish',
                            coalesce(kw,'')),'C')    ||  
                            setweight(to_tsvector('spanish',
                            coalesce(resumen,'')),'D'))) STORED;

    ```

    En la figura \@ref(fig:doctok) se puede ver parcialmente la estructura de datos de tipo "`tsvector`" que se genera para el texto "Las planicies de inundación son sistemas asociados al margen de un río que están sujetas a pulsos estacionales de inundación y sequía. La desembocadura del Río Mapire constituye un sistema complejo de planicie de inundación, como resultado del aumento del nivel de agua en el río por un efecto de represamiento por el Río Orinoco durante la época lluviosa. El gradiente de inundación que se forma genera un estrés ambiental en el sistema suelo que depende de la duración y profundidad...." .

    ```{r doctok, echo=FALSE, fig.align='center', fig.cap='Estructura de datos Índice Invertido', out.width="70%"}

    knitr::include_graphics("images/05-desarrollo/2_ciclo/esquemas/doc_tokens.png")

    ```

    La función `"to_tsvector"` hace el parseo de un documento a tokens y estos son modificados realizando el *stemming* para extraer la raíz de la palabra, proceso que se revisó en \@ref(steaming), lo cual ayuda a disminuir el espacio de búsqueda y a realizar las recuperación de textos de manera más eficiente. Para lograr ejecutar este proceso, es necesario dar el parámetro "spanish" a la función, para que sea sobre la base del idioma español, que sea extraída la raíz.

    La posición de la palabra dentro del texto, también queda registrada para así poder determinar en un *query* de varias palabras, la cercanía que tienen dentro del texto las palabras que son buscadas, dando una mayor relevancia a las que estén más cercanas.

    Otro aspecto a destacar es que ciertas palabras de uso común y frecuente, denominadas *stopwords*, como: "el", "la", "y", "a", "en", "con", "para"; no son registradas, es decir que son omitidas así como los signos de puntuación, con lo cual un query que contenga una stopword o un signo de puntuación, no será buscada la coincidencia al no formar parte de la estructura de datos "`tsvector`".

    Para culminar el proceso de creación del "Índice Invertido" se debe ejecutar el comando `"CREATE INDEX document_weights_idx ON Corpus USING GIN (document_tokens);"` que cierra el ciclo de la generación del índice y es lo que permite realizar los procesos de búsqueda en una forma más eficiente, disminuyendo los tiempos de respuesta, debiendo tener presente que la creación del índice puede incrementar entre un cincuenta y doscientos por ciento el espacio de almacenamiento al ser generado. Una de las bondades que adicionalmente brinda la función `"TS_VECTOR"`,es que al añadir nuevos documentos a la base de datos, el índice será actualizado automáticamente.

3.  **Tabla "POS":** con los procesos que fueron revisados en el etiquetado de la parte del discurso en \@ref(iternlp), se obtuvieron los datos que formarán parte de esta tabla, donde cada palabra que conforma el corpus, pasa a ser una fila identificando el documento de origen, la oración dentro del documento donde se encuentra la palabra, el número de orden de aparición de la palabra, la palabra, su lema y la función gramatical asignada, idéntico a lo que se observó en la figura \@ref(fig:spacypi).

En cuanto a la aplicación web, mediante el *framework* Shiny se implementó el Prototipo, con la codificación de dos componentes principales que son: la interfaz de usuario (UI) y el Servidor (Server).

Shiny se basa en la "programación reactiva", lo que significa que ante cambios en los "campos de entrada" en la interfaz de usario (UI), los elementos asociados se actualizan dinámicamente desde el componente "Server" sin necesidad de recargar la página, permitiendo crear una experiencia al usuario fluida. Shiny utiliza conceptos como "observadores" y "reactividad" para mantener sincronizados los elementos de la UI con los datos subyacentes, que en el caso del Prototipo son el texto del *query*, el valor para establecer la cantidad de coocurrencias a representar en el "Mapa del Conocimiento" y el botón para ejecutar el *query*.

\
A nivel de la aplicación web, en la interfaz de usuario están estos tres componentes:

1.  **textInput**: que corresponde a la casilla del *query*.

2.  **numericInput**: la cantidad de palabras que se van a mostrar en la representación de los Mapas de Conocimiento.

3.  **actionBttn**: botón de acción para ejecutar el query.

En el servidor de la aplicación, a nivel de generación de estructuras a ser representadas como resultados, dispone de estos componentes:

1.  **dataTableOutput**: mediante la librería "datatable" [@DT] se genera una tabla con los resultados obtenidos donde se incluyen los atributos: "Fecha","Títutlo", "Autor", "Resumen", "Facultad", "Escuela", "Nombre Tutor".

2.  **renderPlot**: con el uso de la librería "ggraph" [@ggraph] se crea un grafo que reproduce los "Mapas de Conocimiento", representando las palabras mediante nodos y la coocurrencia implica la unión mediante arcos, y a mayor grosor en el arco, quiere decir que la cantidad de veces que se presenta la coocurrencia es mayor.

En la figura \@ref(fig:prototipoapp) se muestra la versión implementada del prototipo donde ante la búsqueda de la palabra "física", se recuperan los documentos que incluyen tal palabra y se generan los "Mapas de Conocimiento".

```{r prototipoapp, echo=FALSE, fig.align='center', fig.cap='Estructura de datos Índice Invertido', out.width="80%"}

knitr::include_graphics("images/05-desarrollo/2_ciclo/UI/prototipo_app.png")
```

**Lógica de la aplicación:**

En el detalle que se expone en las siguientes líneas es necesario mencionar que Shiny, funciona a nivel de computo como monohilo ("single-threaded"), por lo cual es estrictamente necesario que los pasos sean ejecutados secuencialmente según lo descrito.

1.  **Definición "texto búsqueda":** el usuario define el texto del query y la cantidad de coocurrencias.

2.  **Procesamiento Query texto:** Cuando el servidor recibe el texto para generar el *query*, este es procesado con la sintaxis del siguiente código:

    ```{SQL, echo=TRUE, eval=FALSE}
    "select titulo, id_doc, facultad, jerarquia ,fecha,
      autor, kw, resumen, nombre, tutor from Corpus where
      document_tokens @@ websearch_to_tsquery('spanish',
        'texto a buscar') 
      order by ts_rank_cd (document_tokens,
        websearch_to_tsquery('spanish','texto a buscar')) desc
    ```

    Lo relevante es apreciar que el texto del *query*, mediante la función `"websearch_to_tsquery"`, es sometido a un parseo que lo convierte a la estructura de datos "`tsvector"`, revisada anteriormente, y así la hace compatible con el contenido de la columna "`document_tokens"` . El operador de coincidencia "`@@`" es el que determina si el texto del `tsquery` coincide con los distintos textos registrados en el "`tsvector"` . Si la frase del query, p. ej. "física química", que incluye dos palabras, el operador lógico que se intercala entre cada palabra es el "y (and)", no obstante, la propia función `websearch_to_tsquery` permite que se defina que pueda ser el operador "o (or)", por ejemplo si se escribe "física OR química". [^05-desarrollo-6]

    También incluye la función `order by ts_rank_cd` la cual es una implementación del método "*cover density* *ranking*" que fue introducido en la investigación de [@clarke2000], donde la relevancia se determina mediante la proximidad y coocurrencia de las palabras que conforman el query dentro de cada documento del Corpus ejecutando el reordenamiento, según lo visto en \@ref(ranking), teniendo como base los criterios de peso que habían sido definidos al crear el `"tsvector"` y también toma en cuenta la proximidad que puedan tener las distintas palabras que componen el query. Es conveniente citar la documentación de PostgreSQL relativa a esta función "...*es decir, consideran la frecuencia con la que los términos de la consulta aparecen en el documento, la proximidad de los términos en el documento y la importancia de la parte del documento en la que aparecen. Sin embargo, el concepto de relevancia es vago y muy específico de cada aplicación. Diferentes aplicaciones pueden requerir información adicional para la clasificación, por ejemplo, la hora de modificación del documento*".\

3.  **Query datos MC:** Con la lista de "id_docs", que fue obtenida en el paso anterior, se ejecuta un segundo *query* sobre la tabla "POS", se seleccionan las filas que tienen la lista de "id_docs" y en las cuales las palabras (tokens) cumplan con la condición de ser sustantivos y adjetivos, obteniendo la tabla filtrada con "doc_id", "token_id", "sentence_id", "pos", "lemma".

    Contar en esta tabla con el "doc_id", "token_id" y el "sentence_id", sirve para determinar el nivel de representación de las coocurrencias, a modo de granularidad, permitiendo que posteriormente el resultado pueda ser representado con palabras que coocurren: a) una seguida de otra, b) dentro de la misma oración, o, c) dentro del mismo texto resumen.

4.  **Generación de estructura Mapas de Conocimiento-coocurrencia:** a los datos obtenidos en el paso (3) se les aplica la función `coocurrence` de la librería [@udpipe] que convierte los datos en una tabla de coocurrencias, donde se puede ajustar la granularidad revisada en (3).

5.  **Render resultados:** El servidor hace el render del **dataTableOutput** y del **renderPlot** mencionados como los componentes del server.

[^05-desarrollo-6]: Otros operadores que se pueden usar en esta función son: "!" para excluir un término, "\<-\>" para indicar que las palabras deben aparecer una seguida de otra. En la documentación oficial de PostgreSQL disponible en el enlace <https://www.postgresql.org/docs/current/textsearch-controls.html> se encuentra más información sobre los operadores que se pueden aplicar en los procesos de búsqueda.

##### Aprender:

En la fase de aprendizaje se pudo detectar que es conveniente hacer distintas representaciones de los "Mapas de Conocimiento" con distintas granularidades y que incorporar interactividad a la representación puede constituir una herramienta de filtrado adicional para inspeccionar el Corpus. En el Ciclo de Integración de Componentes del Software \@ref(desarrollociclos4) será abordado en detalle la propuesta final que se implementó en este particular.

Igualmente se constató la necesidad de mejorar los niveles de reproducibilidad para realizar la implementación del Sistema, mediante configuraciones que puedan ser independientes del sistema operativo y demás dependencias que estén preinstaladas en el Host, lo que llevó a realizar el diseño de la aplicación y sus componentes, mediante el uso de contenedores orquestados, lo cual se expondrá en \@ref(desarrollociclos4).

Al crear el GIN (General Inverted Index) se pudo apreciar que el peso de la tabla con los datos del Corpus se incrementó en aproximadamente un 40%, no obstante, por ser un conjunto de datos relativamente pequeño, esto no representa un mayor problema, pero sí se debe tener en cuenta en caso de que el Sistema diese soporte a un Corpus mucho mayor.

Otro proceso de aprendizaje de este ciclo fue hacer los ajustes a la función de reordenamiento, según la relevancia, donde se estableció la siguiente jerarquía:

1.  Autor

2.  Título

3.  Palabras Claves

4.  Texto Resumen

Lo que quiere decir que si ante un texto de una búsqueda, en los documentos que conforman el Corpus, las palabras de *query* presentan coincidencia en el título, la función de relevancia le otorgará mayor peso a ese documento por sobre otro que pueda tener la misma coincidencia de aparición, pero en el texto resumen. Este ejemplo aplica para el resto de las combinaciones posibles.

Realizar el etiquetado de las partes del discurso, previamente a realizar la selección de los textos que serán representados mediante Mapas de Conocimiento, representa una mejora en los tiempos que tarda el Prototipo en realizar el render, lo cual refuerza la necesidad de contar con un sistema gestor de base de datos que pueda tener indexadas las distintas tablas que conforman el SCSU.

#### Objetivos Alcanzados:

1.  Implementar un buscador de texto sobre un subconjunto del Corpus.

2.  Hacer las pruebas de integración entre la base de datos con la aplicación web.

3.  Contar con almacenamiento persistente para los datos.

4.  Con los textos del Corpus generar recomendaciones de documentos que sean similares a una determinada investigación.

\newpage

<br> <br>

### Ciclo Integración de Componentes del Software: {#desarrollociclos4}

En este Ciclo se realiza la integración de los elementos constituidos y evaluados en procesos anteriores, tomando en cuenta los "aprendizajes" para realizar las modificaciones que permitan implementar la primera versión del **Sistema Complementario Saber UCV (SCSU)**. En la sección \@ref(implemenesp) se presenta el proceso de Especulación, en \@ref(implemencolab) se expone la Colaboración y finalmente en \@ref(implemenapre) el Aprendizaje alcanzado a lo largo del Ciclo.

##### Especulación: {#implemenesp}

Usando las técnicas y métodos aplicados en ciclos anteriores e integrando los aprendizajes obtenidos, se puede implementar el SCSU. Concretamente se replicarán en este ciclo:

1.  Usar la arquitectura "Modelo-Vista-Contralador" revisada en \@ref(desarrolloarquitectura).

2.  "Conformación del Corpus" revisado en \@ref(desarrollociclos1), \@ref(scrapeo) y \@ref(iternlp).

3.  "Clasificación de Documentos" revisado en \@ref(asignacion).

4.  "Búsqueda de textos" revisado en \@ref(desarrollociclos3) y \@ref(iterbol).

5.  Representación de "Mapas de Conocimiento (MC)" revisado en \@ref(fig:mapacon).

6.  Generación de "Recomendaciones" revisada en \@ref(imrecomendacion).

En el proceso de especulación de este ciclo se aplicaron los métodos de la "Ingeniería de Software" necesarios para implementar el SCSU estableciendo formalmente los requerimientos funcionales y no funcionales, los diagramas y tablas de casos de uso, el modelo entidad-relación y demás aspectos necesarios, para posteriormente en la fase de Colaboración realizar la implementación del Sistema.

###### **Requerimientos Funcionales del SCSU:**

1.  **Interactividad:**

-   **Descripción:**

    La aplicación web debe ser altamente interactiva permitiendo al usuario de una manera fluida y receptiva interactuar con la interfaz para realizar búsquedas, explorar resultados y utilizar funcionalidades como filtros e inspección de "mapas del conocimiento".

-   **Criterios de Aceptación:**

    -   La interfaz de usuario debe responder de manera rápida y efeciciente a las interacciones del usuario.

2.  **Búsqueda de Contenido**

-   **Descripción:**

    El sistema debe permitir a los usuarios realizar búsquedas de contenido utilizando palabras clave o frases. La búsqueda debe ser insensible a mayúsculas y minúsculas, y devolver resultados en función a las palabras clave ingresadas.

    **Criterios de Aceptación:**

    -   El sistema debe proporcionar una interfaz de usuario intuitiva para la entrada de términos de búsqueda.
    -   Los resultados de la búsqueda deben mostrar: títulos, autores, fecha elaboración, palabras clave, facultad, nombre tutor, resúmenes.
    -   La búsqueda debe ser eficiente, respondiendo en un tiempo razonable.
    -   Se debe permitir a los usuarios hacer clic en un resultado para obtener más detalles sobre una investigación.

3.  **Filtrado de Búsquedas**

-   **Descripción:**

    El sistema debe permitir a los usuarios aplicar filtros avanzados a los resultados de búsqueda para refinar y limitar la información recuperada. Los filtros pueden incluir fechas, jerarquía (nivel académico), facultad, escuela-postgrado.

-   **Criterios de Aceptación:**

    -   Los filtros deben ser fácilmente accesibles y configurables.

    -   Las opciones de criterios de búsqueda deben actualizarse dinámicamente al aplicar los filtros.

    -   Debe ser posible combinar múltiples filtros para refinar aún más la búsqueda.

4.  **Generar Mapas del Conocimiento**

-   **Descripción:**

    El sistema debe tener la capacidad de generar "mapas del conocimiento" con la información recuperada. Estos mapas deben representar de manera clara las relaciones y conexiones entre los conceptos dentro de la información mediante una estructura de grafos.

-   **Criterios de Aceptación:**

    -   La generación de mapas del conocimiento debe ser automática y basarse en la estructura y contenido de los documentos recuperados.

    -   Los mapas deben ser interactivos, permitiendo a los usuarios explorar y comprender las relaciones entre los conceptos.

    -   Deben existir opciones para ajustar la complejidad y el nivel de detalle de los mapas.

5.  **"Rankiar" los Documentos Recuperados**

-   **Descripción:**

    El sistema debe asignar un *ranking* a los documentos recuperados en función de su relevancia con respecto a la consulta realizada. La clasificación debe ser transparente y basada en algoritmos que consideren diversos factores como la frecuencia y proximidad de términos clave.

-   **Criterios de Aceptación:**

    -   Aplicar algoritmo para reordamiento y evaluar las métricas de desempeño.

    -   Se debe proporcionar una opción para ordenar los resultados de búsqueda según diferentes criterios, como relevancia o fecha.

6.  **Generar Recomendaciones**

-   **Descripción:**

    El sistema debe ofrecer recomendaciones de contenido relevante basadas en similitud que tenga un documento con los otros que conforman el Corpus.

-   **Criterios de Aceptación:**

    -   Las recomendaciones deben ser presentadas de manera clara indicando el título de la investigación recomendada con un vínculo al trabajo presentado.

7.  **Actualizar Periódicamente las Investigaciones**

-   **Descripción:**

    El sistema debe realizar actualizaciones periódicas de la información almacenada, garantizando que los resultados de búsqueda sean siempre actuales y reflejen los documentos incorporados a Saber U.C.V. y que estos sean clasificados por área de conocimiento.

-   **Criterios de Aceptación:**

    -   Deben establecerse intervalos regulares de actualización de la base de datos.

    -   Las actualizaciones no deben afectar negativamente el rendimiento del sistema durante las operaciones normales.

8.  **Cuadros de Ayuda (Tooltips)**

-   **Descripción:**

    El sistema debe proporcionar cuadros de ayuda (tooltips) contextuales para guiar a los usuarios durante la interacción con la interfaz. Estos cuadros deben explicar términos técnicos, funciones específicas y proporcionar orientación sobre el uso eficaz del sistema.

-   **Criterios de Aceptación:**

    -   Deben existir cuadros de ayuda accesibles en puntos clave de la interfaz de usuario.

    -   Los tooltips deben ser informativos y fácilmente comprensibles.

9.  **Accesibilidad Web**

-   **Descripción:**

    El acceso a la aplicación se debe realizar mediante desde cualquier sitio mediante el uso de navegadores web.

-   **Criterios de Aceptación:**

    -   Se debe garantizar la compatibilidad con los navegadores web más utilizados, como Chrome, Firefox, Safari y Edge.

###### **Requerimientos No Funcionales del SCSU:**

1.  **Rendimiento del Sistema**

-   **Descripción:**

    El sistema debe ser capaz de manejar eficientemente los volúmenes de datos proporcionando tiempos de respuesta rápidos. El rendimiento del sistema debe optimizarse para garantizar una experiencia de usuario fluida y satisfactoria, independientemente de la complejidad de las consultas o la cantidad de usuarios concurrentes.

-   **Criterios de Aceptación:**

    -   El tiempo de respuesta promedio para una consulta estándar no debe superar un segundo.

2.  **Utilización de Componentes Open Source**

-   **Descripción:**

    El SCSU debe utilizar componentes de software (bibliotecas, frameworks y herramientas) que estén bajo licencias *open source*.

-   **Criterios de aceptación:**

    -   Se debe realizar una evaluación de la licencia de los componentes utilizados para garantizar su compatibilidad con el modelo de desarrollo y distribución del proyecto.

3.  **Reproducibilidad del SCSU**

-   **Descripción:**

    El sistema debe ser reproducible, lo que significa que la construcción, implementación y ejecución del software deben proporcionar resultados consistentes en diferentes entornos y momentos, siguiendo prácticas y estándares que faciliten la reproducibilidad del proceso de desarrollo.

-   **Criterios de aceptación:**

    -   Todas las dependencias externas, incluyendo bibliotecas y componentes *open source*, deben estar claramente especificadas y gestionadas.

    -   La aplicación debe ser capaz de ejecutarse correctamente en diferentes sistemas operativos y configuraciones de infraestructura.

4.  **Preprocesamientos:**

-   **Descripción:**

    El Sistema, bien sea al momento de conformar la base de datos o al realizar actualizaciones, en la mayor medida posible debe ejecutar el preprocesamiento de los textos (PLN-POS, remoción stop words, índice invertido, etc.) para minimizar los cómputos al momento de la demanda de recursos cuando el usuario realiza las búsquedas.

-   **Criterios de aceptación:**

    -   El sistema debe realizar el preprocesamiento de textos de manera eficiente durante la construcción inicial de la base de datos y las actualizaciones periódicas.

5.  **Modularidad del Sistema**

-   **Descripción:**

    El sistema debe ser modular, dividiendo sus componentes en piezas simples que utilizan contenedores . Estos componentes deben estar orquestados de manera eficiente, permitiendo el intercambio, la asignación de recursos y la modificación de módulos individuales con un impacto mínimo en el resto del sistema.

-   **Criterios de aceptación:**

    -   Los componentes del sistema deben estar encapsulados en contenedores, facilitando su independencia y despliegue consistente.

    -   Los componentes del sistema deben estar orquestados de manera eficiente, permitiendo una coordinación efectiva entre ellos y permitiendo la definición de asignación de recursos.

    -   Los componentes deben ser intercambiables y modificables de manera independiente, fomentando la flexibilidad y la evolución del sistema a lo largo del tiempo.

6.  **Mantenibilidad del Sistema**

-   **Descripción:**

    El sistema debe ser diseñado de manera que permita la realización periódica de mantenimiento sin degradar el rendimiento ni la operatividad del Sistema.

-   **Criterios de aceptación:**

    -   La realización de mantenimiento no debe causar una degradación significativa en el rendimiento del sistema garantizando la continuidad operativa.

    -   La realización del mantenimiento debe definirse con carácter periódico en la configuración.

###### Diagramas de Caso de Uso:

El SCSU cuenta con los casos de uso que se observan en las figuras \@ref(fig:uc1), \@ref(fig:uc12), \@ref(fig:uc2), \@ref(fig:uc3), \@ref(fig:uc31), \@ref(fig:uc4) ; seguido de las tablas \@ref(tab:tablauc1) ,\@ref(tab:tablauc11) , \@ref(tab:tablauc2), \@ref(tab:tablauc3), \@ref(tab:tablauc321), \@ref(tab:tablauc322), \@ref(tab:tablauc4) que contienen la descripción de cada caso.

```{r uc1,echo=FALSE, fig.align='center', fig.cap='SCSU: Diagrama de Casos de Uso 1, Nivel 1.', out.width="40%"}
knitr::include_graphics("images/05-desarrollo/4_ciclo/UC/SCSU_UC1_nivel1.png")
```

<br><br>

```{r uc12,echo=FALSE, fig.align='center', fig.cap='SCSU: Diagrama de Casos de Uso 1, Nivel 1.', out.width="50%"}
knitr::include_graphics("images/05-desarrollo/4_ciclo/UC/SCSU_UC1_nivel2.png")
```

<br><br>

```{r uc2,echo=FALSE, fig.align='center', fig.cap='SCSU: Diagrama de Casos de Uso 2.', out.width="40%"}
knitr::include_graphics("images/05-desarrollo/4_ciclo/UC/SCSU_UC2_nivel1.png")
```

<br><br>

```{r uc3,echo=FALSE, fig.align='center', fig.cap='SCSU: Diagrama de Casos de Uso 3, Nivel 1', out.width="40%"}
knitr::include_graphics("images/05-desarrollo/4_ciclo/UC/SCSU_UC3_nivel1.png")
```

<br><br>

```{r uc31,echo=FALSE, fig.align='center', fig.cap='SCSU: Diagrama de Casos de Uso 3, Nivel 2.', out.width="50%"}
knitr::include_graphics("images/05-desarrollo/4_ciclo/UC/SCSU_UC3_nivel2.png")
```

<br><br>

```{r uc4,echo=FALSE, fig.align='center', fig.cap='SCSU: Diagrama de Casos de Uso 4.', out.width="40%"}
knitr::include_graphics("images/05-desarrollo/4_ciclo/UC/SCSU_UC4.png")
```

<br><br>

\newpage

\newpage

```{r tablauc1, echo=FALSE}
####################################################################################
# Datos Entrada
####################################################################################

titulo_tabla <- 'SCSU UC.1.'

nombre_uc <- 'UC.1: Realizar proceso de recuperación de información (query)'

descripcion_uc <- 'El usuario realiza búsquedas sobre los textos que conforman el Corpus'

actor_uc <- 'usuario'

flujo_basico <- 'El caso de uso inicia cuando el usuario ingresa a la aplicación: \n 1) Introduce el texto a buscar. \n 2) El usuario hace "clic" en "búsqueda" \n 4) El SCSU presenta los resultados obtenidos \n 4) El caso de uso termina'

#######flujo alternativo
flu_alt_titulo <- c('Aplica filtros')
flu_alt_descrip <- c('El usuario aplica filtros para restringir la búsqueda')

#######Precondiciones
precond_titulo <- c('N/A')
precond_descri <- c('N/A')

#######Postcondiciones
postcond_titulo <- c('Éxito','Fracaso')
postcond_descri <- c('El prototipo presenta: \n 1)Tabla con los resultados que incluye los campos "fecha", "título", "palabras claves", "autor", "facultad", "dependencia" y "tutor"  \n 2) Gráfico con frecuencia de aparición del texto del query por año','No presenta ningún resultado')

####################################################################################
# Otras variables
####################################################################################

df_encabezado=data.frame(`a`=c('Nombre',
                               'Descripción',
                               'Actor',
                               'Flujo de Eventos',
                               'Flujo Básico',
                               flujo_basico,
                               'Flujo Alternativo',
                               'Título'),
                         `b`=c(nombre_uc,
                               descripcion_uc,
                               actor_uc,
                               '',
                               '',
                               '',
                               '',
                               'Descripción'))


df_flujo_alt <- data.frame(`a`=flu_alt_titulo,
                           `b`=flu_alt_descrip)

df_precondiciones <- data.frame(`a`=c('Precondiciones',
                                      'Título',
                                      precond_titulo),
                                `b`=c('',
                                      'Descripción',
                                      precond_descri))

df_postcondiciones <- data.frame(`a`=c('Postcondiciones',
                                       'Título',
                                       postcond_titulo),
                                 `b`=c('',
                                       'Descripción',
                                       postcond_descri))


indice_1=nrow(df_encabezado)+nrow(df_flujo_alt)+1
indice_2=nrow(df_encabezado)+nrow(df_flujo_alt)+nrow(df_precondiciones)+1
######

df_salida <- bind_rows(df_encabezado,
                df_flujo_alt,
                df_precondiciones,
                df_postcondiciones)


flextable(df_salida)%>%
  delete_part(part = "header")%>%
  theme_box()%>%
  fontsize(size = 10)%>%
  bg(i=1,j=1, bg = "#C2C2C2", part = "body")%>%
  bg(i=2,j=1, bg = "#C2C2C2", part = "body")%>%
  bg(i=3,j=1, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=4)%>%
  bg(i=4, bg = "#8F8F8F", part = "body")%>%
  merge_at(i=5)%>%
  bg(i=5, bg = "#A3A3A3", part = "body")%>%
  merge_at(i=6)%>%
  merge_at(i=7)%>%
  bg(i=7, bg = "#A3A3A3", part = "body")%>%
  bg(i=8, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=indice_1)%>%
  bg(i=indice_1, bg = "#A3A3A3", part = "body")%>%
  bg(i=indice_1+1, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=indice_2)%>%
  bg(i=indice_2, bg = "#A3A3A3", part = "body")%>%
  bg(i=indice_2+1, bg = "#C2C2C2", part = "body")%>%
  set_caption(caption=titulo_tabla)%>%
  align(i=4, align = 'center')%>%
  # align(i=5, align = 'center')%>%
  align(i=indice_1, align = 'center')%>%
  align(i=indice_2, align = 'center')%>%
  width(j = 1, width = 1)%>%
  width(j = 2, width = 6)%>%
  height(height = .1)

```

\newpage

<br> <br>

```{r tablauc11, echo=FALSE}
####################################################################################
# Datos Entrada
####################################################################################

titulo_tabla <- 'SCSU UC.1, Nivel 2.'

nombre_uc <- 'UC.1. Nivel 2: Realizar proceso de recuperación de información (query) aplicando filtros.'

descripcion_uc <- 'El usuario realiza búsquedas sobre los textos que conforman el Corpus aplicando filtros'

actor_uc <- 'usuario'

flujo_basico <- 'El caso de uso inicia cuando el usuario ingresa a la aplicación y decide aplicar filtros al realizar la búsqueda. Los campos que se muestran son: \n 1) Introducir el texto a buscar. 2) Filtrar el nivel académico. 3) Filtrar la Facultad  4) Filtrar la escuela o postgrado  5) Definir rango de fechas 6) Seleccionar "generación de coocurrencias (mapas de conocimiento)"  7) El usuario hace "clic" en el "búsqueda"  8) El SCSU presenta los resultados obtenidos 9) El caso de uso termina'

#######flujo alternativo
flu_alt_titulo <- c('Sin filtros')
flu_alt_descrip <- c('El usuario no aplica ningún filtro y aparecen todos los resultados que contienen el texto de query')

#######Precondiciones
precond_titulo <- c('N/A')
precond_descri <- c('N/A')

#######Postcondiciones
postcond_titulo <- c('Éxito','Fracaso')
postcond_descri <- c('El SCSU presenta: \n 1)Tabla con los resultados que incluye los campos "fecha", "título", "palabras claves", "autor", "facultad", "dependencia" y "tutor"  \n 3) Gráfico con frecuencia de aparición del texto del query por año \n 4) Gráfico con "Mapas de Conocimiento"', 'No presenta ningún resultado')

####################################################################################
# Otras variables
####################################################################################

df_encabezado=data.frame(`a`=c('Nombre',
                               'Descripción',
                               'Actor',
                               'Flujo de Eventos',
                               'Flujo Básico',
                               flujo_basico,
                               'Flujo Alternativo',
                               'Título'),
                         `b`=c(nombre_uc,
                               descripcion_uc,
                               actor_uc,
                               '',
                               '',
                               '',
                               '',
                               'Descripción'))


df_flujo_alt <- data.frame(`a`=flu_alt_titulo,
                           `b`=flu_alt_descrip)

df_precondiciones <- data.frame(`a`=c('Precondiciones',
                                      'Título',
                                      precond_titulo),
                                `b`=c('',
                                      'Descripción',
                                      precond_descri))

df_postcondiciones <- data.frame(`a`=c('Postcondiciones',
                                       'Título',
                                       postcond_titulo),
                                 `b`=c('',
                                       'Descripción',
                                       postcond_descri))


indice_1=nrow(df_encabezado)+nrow(df_flujo_alt)+1
indice_2=nrow(df_encabezado)+nrow(df_flujo_alt)+nrow(df_precondiciones)+1
######

df_salida <- bind_rows(df_encabezado,
                df_flujo_alt,
                df_precondiciones,
                df_postcondiciones)


flextable(df_salida)%>%
  delete_part(part = "header")%>%
  theme_box()%>%
  fontsize(size = 10)%>%
  bg(i=1,j=1, bg = "#C2C2C2", part = "body")%>%
  bg(i=2,j=1, bg = "#C2C2C2", part = "body")%>%
  bg(i=3,j=1, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=4)%>%
  bg(i=4, bg = "#8F8F8F", part = "body")%>%
  merge_at(i=5)%>%
  bg(i=5, bg = "#A3A3A3", part = "body")%>%
  merge_at(i=6)%>%
  merge_at(i=7)%>%
  bg(i=7, bg = "#A3A3A3", part = "body")%>%
  bg(i=8, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=indice_1)%>%
  bg(i=indice_1, bg = "#A3A3A3", part = "body")%>%
  bg(i=indice_1+1, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=indice_2)%>%
  bg(i=indice_2, bg = "#A3A3A3", part = "body")%>%
  bg(i=indice_2+1, bg = "#C2C2C2", part = "body")%>%
  set_caption(caption=titulo_tabla)%>%
  align(i=4, align = 'center')%>%
  # align(i=5, align = 'center')%>%
  align(i=indice_1, align = 'center')%>%
  align(i=indice_2, align = 'center')%>%
  width(j = 1, width = 1)%>%
  width(j = 2, width = 6)%>%
  height(height = .1)

```

\newpage

<br> <br>

```{r tablauc2, echo=FALSE}
####################################################################################
# Datos Entrada
####################################################################################

titulo_tabla <- 'SCSU UC. 2.'

nombre_uc <- 'UC.2: Realizar Inspección de Recomendaciones'

descripcion_uc <- 'El usuario inspecciona un documento de interés haciendo "clic" y se muestran las títulos y vínculos a investigaciones recomendadas'

actor_uc <- 'usuario'

flujo_basico <- 'El caso de uso inicia cuando \n 1) el usuario inspecciona la tabla con los resultados de la búsqueda y hace "clic" sobre una fila y se expande la fila asociada a una investigación. \n 2) el caso de uso termina'

#######flujo alternativo
flu_alt_titulo <- c('no inspecciona')
flu_alt_descrip <- c('No se despliega el área que muestra las recomendaciones')

#######Precondiciones
precond_titulo <- c('Realizar query')
precond_descri <- c('Haber realizado el UC. 1.')

#######Postcondiciones
postcond_titulo <- c('Éxito','Fracaso')
postcond_descri <- c('El SCSU presenta: \n 1) listado con hasta cinco títulos de documentos recomendados que contienen el hipervínculo a la investigación alojada en Saber UCV ','No se muestran recomendaciones por no disponer de documentos similares en el Corpus')

####################################################################################
# Otras variables
####################################################################################

df_encabezado=data.frame(`a`=c('Nombre',
                               'Descripción',
                               'Actor',
                               'Flujo de Eventos',
                               'Flujo Básico',
                               flujo_basico,
                               'Flujo Alternativo',
                               'Título'),
                         `b`=c(nombre_uc,
                               descripcion_uc,
                               actor_uc,
                               '',
                               '',
                               '',
                               '',
                               'Descripción'))


df_flujo_alt <- data.frame(`a`=flu_alt_titulo,
                           `b`=flu_alt_descrip)

df_precondiciones <- data.frame(`a`=c('Precondiciones',
                                      'Título',
                                      precond_titulo),
                                `b`=c('',
                                      'Descripción',
                                      precond_descri))

df_postcondiciones <- data.frame(`a`=c('Postcondiciones',
                                       'Título',
                                       postcond_titulo),
                                 `b`=c('',
                                       'Descripción',
                                       postcond_descri))


indice_1=nrow(df_encabezado)+nrow(df_flujo_alt)+1
indice_2=nrow(df_encabezado)+nrow(df_flujo_alt)+nrow(df_precondiciones)+1
######

df_salida <- bind_rows(df_encabezado,
                df_flujo_alt,
                df_precondiciones,
                df_postcondiciones)


flextable(df_salida)%>%
  delete_part(part = "header")%>%
  theme_box()%>%
  fontsize(size = 10)%>%
  bg(i=1,j=1, bg = "#C2C2C2", part = "body")%>%
  bg(i=2,j=1, bg = "#C2C2C2", part = "body")%>%
  bg(i=3,j=1, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=4)%>%
  bg(i=4, bg = "#8F8F8F", part = "body")%>%
  merge_at(i=5)%>%
  bg(i=5, bg = "#A3A3A3", part = "body")%>%
  merge_at(i=6)%>%
  merge_at(i=7)%>%
  bg(i=7, bg = "#A3A3A3", part = "body")%>%
  bg(i=8, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=indice_1)%>%
  bg(i=indice_1, bg = "#A3A3A3", part = "body")%>%
  bg(i=indice_1+1, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=indice_2)%>%
  bg(i=indice_2, bg = "#A3A3A3", part = "body")%>%
  bg(i=indice_2+1, bg = "#C2C2C2", part = "body")%>%
  set_caption(caption=titulo_tabla)%>%
  align(i=4, align = 'center')%>%
  # align(i=5, align = 'center')%>%
  align(i=indice_1, align = 'center')%>%
  align(i=indice_2, align = 'center')%>%
  width(j = 1, width = 1)%>%
  width(j = 2, width = 6)%>%
  height(height = .1)

```

\newpage

<br> <br>

```{r tablauc3, echo=FALSE}
####################################################################################
# Datos Entrada
####################################################################################

titulo_tabla <- 'SCSU UC. 3.'

nombre_uc <- 'UC.3: Realizar inspección de "Mapas de Conocimiento"'

descripcion_uc <- 'El usuario revisa los "Mapas de Conocimiento" generados con los documentos que fueron recuperados en el query'

actor_uc <- 'usuario'

flujo_basico <- 'El caso de uso inicia cuando el usuario: \n 1) Hace "clic" en la pestaña de nombre "Coocurrencias". \n 2) Se muestra un gráfico interactivo con los "Mapas de Conocimiento" generados. \n 3) El caso de uso termina'

#######flujo alternativo
flu_alt_titulo <- c('Sin Coocurrencia')
flu_alt_descrip <- c('El usuario no selecciona la ventana "coocurrencia" y no se muestran los "Mapas de Conocimiento"')

#######Precondiciones
precond_titulo <- c('Realizar query')
precond_descri <- c('Haber realizado el UC. 1.')

#######Postcondiciones
postcond_titulo <- c('Éxito','Fracaso')
postcond_descri <- c('El usuario navega con las flechas del teclado o con la rueda de scroll del mouse haciendo zoom in o zoom out sobre el gráfico de "Mapas de Conocimiento" para ver el detalle de los datos representados','No se muestran "Mapas de Conocimiento" por no disponer de datos para que sea generado el gráfico')

####################################################################################
# Otras variables
####################################################################################

df_encabezado=data.frame(`a`=c('Nombre',
                               'Descripción',
                               'Actor',
                               'Flujo de Eventos',
                               'Flujo Básico',
                               flujo_basico,
                               'Flujo Alternativo',
                               'Título'),
                         `b`=c(nombre_uc,
                               descripcion_uc,
                               actor_uc,
                               '',
                               '',
                               '',
                               '',
                               'Descripción'))


df_flujo_alt <- data.frame(`a`=flu_alt_titulo,
                           `b`=flu_alt_descrip)

df_precondiciones <- data.frame(`a`=c('Precondiciones',
                                      'Título',
                                      precond_titulo),
                                `b`=c('',
                                      'Descripción',
                                      precond_descri))

df_postcondiciones <- data.frame(`a`=c('Postcondiciones',
                                       'Título',
                                       postcond_titulo),
                                 `b`=c('',
                                       'Descripción',
                                       postcond_descri))


indice_1=nrow(df_encabezado)+nrow(df_flujo_alt)+1
indice_2=nrow(df_encabezado)+nrow(df_flujo_alt)+nrow(df_precondiciones)+1
######

df_salida <- bind_rows(df_encabezado,
                df_flujo_alt,
                df_precondiciones,
                df_postcondiciones)


flextable(df_salida)%>%
  delete_part(part = "header")%>%
  theme_box()%>%
  fontsize(size = 10)%>%
  bg(i=1,j=1, bg = "#C2C2C2", part = "body")%>%
  bg(i=2,j=1, bg = "#C2C2C2", part = "body")%>%
  bg(i=3,j=1, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=4)%>%
  bg(i=4, bg = "#8F8F8F", part = "body")%>%
  merge_at(i=5)%>%
  bg(i=5, bg = "#A3A3A3", part = "body")%>%
  merge_at(i=6)%>%
  merge_at(i=7)%>%
  bg(i=7, bg = "#A3A3A3", part = "body")%>%
  bg(i=8, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=indice_1)%>%
  bg(i=indice_1, bg = "#A3A3A3", part = "body")%>%
  bg(i=indice_1+1, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=indice_2)%>%
  bg(i=indice_2, bg = "#A3A3A3", part = "body")%>%
  bg(i=indice_2+1, bg = "#C2C2C2", part = "body")%>%
  set_caption(caption=titulo_tabla)%>%
  align(i=4, align = 'center')%>%
  # align(i=5, align = 'center')%>%
  align(i=indice_1, align = 'center')%>%
  align(i=indice_2, align = 'center')%>%
  width(j = 1, width = 1)%>%
  width(j = 2, width = 6)%>%
  height(height = .1)
```

\newpage

<br> <br>

```{r tablauc321, echo=FALSE}
####################################################################################
# Datos Entrada
####################################################################################

titulo_tabla <- 'SCSU UC. 3. Nivel 2.1.'

nombre_uc <- 'UC.3. Nivel 2.1: Realizar inspección "Mapa de Conocimiento"'

descripcion_uc <- 'El usuario inspecciona un arco del "Mapa de Conocimiento"'

actor_uc <- 'usuario'

flujo_basico <- 'El caso de uso inicia cuando el usuario: \n 1) Hace "clic" sobre un arco de los "Mapas de Conocimiento". \n 2) Aparece un "popup" con resultados. \n 3) El caso de uso termina'

#######flujo alternativo
flu_alt_titulo <- c('Sin clic arco')
flu_alt_descrip <- c('El usuario no selecciona ningún arco en "Mapas de Conocimiento"')

#######Precondiciones
precond_titulo <- c('1) Realizar "query"', 'Selecc ventana "Coocurrencia"')
precond_descri <- c('2) Haber realizado el UC. 1.', 'Haber realizado el UC. 3.')

#######Postcondiciones
postcond_titulo <- c('Éxito','Fracaso')
postcond_descri <- c('Ventana popup con el listado de documentos que contienen las palabras que las une el arco sobre el cual se realizó el "clic" . En el listado se muestra el "título","fecha","palabras claves","autor","facultad","escuela/postgrado"','No se muestra tabla por no disponer de datos para generarla')

####################################################################################
# Otras variables
####################################################################################

df_encabezado=data.frame(`a`=c('Nombre',
                               'Descripción',
                               'Actor',
                               'Flujo de Eventos',
                               'Flujo Básico',
                               flujo_basico,
                               'Flujo Alternativo',
                               'Título'),
                         `b`=c(nombre_uc,
                               descripcion_uc,
                               actor_uc,
                               '',
                               '',
                               '',
                               '',
                               'Descripción'))


df_flujo_alt <- data.frame(`a`=flu_alt_titulo,
                           `b`=flu_alt_descrip)

df_precondiciones <- data.frame(`a`=c('Precondiciones',
                                      'Título',
                                      precond_titulo),
                                `b`=c('',
                                      'Descripción',
                                      precond_descri))

df_postcondiciones <- data.frame(`a`=c('Postcondiciones',
                                       'Título',
                                       postcond_titulo),
                                 `b`=c('',
                                       'Descripción',
                                       postcond_descri))


indice_1=nrow(df_encabezado)+nrow(df_flujo_alt)+1
indice_2=nrow(df_encabezado)+nrow(df_flujo_alt)+nrow(df_precondiciones)+1
######

df_salida <- bind_rows(df_encabezado,
                df_flujo_alt,
                df_precondiciones,
                df_postcondiciones)


flextable(df_salida)%>%
  delete_part(part = "header")%>%
  theme_box()%>%
  fontsize(size = 10)%>%
  bg(i=1,j=1, bg = "#C2C2C2", part = "body")%>%
  bg(i=2,j=1, bg = "#C2C2C2", part = "body")%>%
  bg(i=3,j=1, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=4)%>%
  bg(i=4, bg = "#8F8F8F", part = "body")%>%
  merge_at(i=5)%>%
  bg(i=5, bg = "#A3A3A3", part = "body")%>%
  merge_at(i=6)%>%
  merge_at(i=7)%>%
  bg(i=7, bg = "#A3A3A3", part = "body")%>%
  bg(i=8, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=indice_1)%>%
  bg(i=indice_1, bg = "#A3A3A3", part = "body")%>%
  bg(i=indice_1+1, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=indice_2)%>%
  bg(i=indice_2, bg = "#A3A3A3", part = "body")%>%
  bg(i=indice_2+1, bg = "#C2C2C2", part = "body")%>%
  set_caption(caption=titulo_tabla)%>%
  align(i=4, align = 'center')%>%
  # align(i=5, align = 'center')%>%
  align(i=indice_1, align = 'center')%>%
  align(i=indice_2, align = 'center')%>%
  width(j = 1, width = 1)%>%
  width(j = 2, width = 6)%>%
  height(height = .1)
```

\newpage

<br> <br>

```{r tablauc322, echo=FALSE}
####################################################################################
# Datos Entrada
####################################################################################

titulo_tabla <- 'SCSU UC. 3. Nivel 2.2.'

nombre_uc <- 'UC.3. Nivel 2.2: modificar cantidad de coocurrencias en "Mapa de Conocimiento"'

descripcion_uc <- 'El usuario modifica el valor correspondiente a la cantidad de coocurrencias a ser representadas en el "Mapa de Conocimiento"'

actor_uc <- 'usuario'

flujo_basico <- 'El caso de uso inicia cuando el usuario: \n 1) Modifica el valor que aparece por defecto (60) de coocurrencias a representar. \n 2) El caso de uso termina'

#######flujo alternativo
flu_alt_titulo <- c('No modifica el valor')
flu_alt_descrip <- c('El usuario no modifica el valor que aparece por defecto')

#######Precondiciones
precond_titulo <- c('1) Realizar "query"', 'Selecc ventana "Coocurrencia"')
precond_descri <- c('2) Haber realizado el UC. 1.', 'Haber realizado el UC. 3.')

#######Postcondiciones
postcond_titulo <- c('Éxito','Fracaso')
postcond_descri <- c('Se modifica el "Mapa de Conocimiento" representado.','No se muestra tabla por no disponer datos suficientes para generarla')

####################################################################################
# Otras variables
####################################################################################

df_encabezado=data.frame(`a`=c('Nombre',
                               'Descripción',
                               'Actor',
                               'Flujo de Eventos',
                               'Flujo Básico',
                               flujo_basico,
                               'Flujo Alternativo',
                               'Título'),
                         `b`=c(nombre_uc,
                               descripcion_uc,
                               actor_uc,
                               '',
                               '',
                               '',
                               '',
                               'Descripción'))


df_flujo_alt <- data.frame(`a`=flu_alt_titulo,
                           `b`=flu_alt_descrip)

df_precondiciones <- data.frame(`a`=c('Precondiciones',
                                      'Título',
                                      precond_titulo),
                                `b`=c('',
                                      'Descripción',
                                      precond_descri))

df_postcondiciones <- data.frame(`a`=c('Postcondiciones',
                                       'Título',
                                       postcond_titulo),
                                 `b`=c('',
                                       'Descripción',
                                       postcond_descri))


indice_1=nrow(df_encabezado)+nrow(df_flujo_alt)+1
indice_2=nrow(df_encabezado)+nrow(df_flujo_alt)+nrow(df_precondiciones)+1
######

df_salida <- bind_rows(df_encabezado,
                df_flujo_alt,
                df_precondiciones,
                df_postcondiciones)


flextable(df_salida)%>%
  delete_part(part = "header")%>%
  theme_box()%>%
  fontsize(size = 10)%>%
  bg(i=1,j=1, bg = "#C2C2C2", part = "body")%>%
  bg(i=2,j=1, bg = "#C2C2C2", part = "body")%>%
  bg(i=3,j=1, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=4)%>%
  bg(i=4, bg = "#8F8F8F", part = "body")%>%
  merge_at(i=5)%>%
  bg(i=5, bg = "#A3A3A3", part = "body")%>%
  merge_at(i=6)%>%
  merge_at(i=7)%>%
  bg(i=7, bg = "#A3A3A3", part = "body")%>%
  bg(i=8, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=indice_1)%>%
  bg(i=indice_1, bg = "#A3A3A3", part = "body")%>%
  bg(i=indice_1+1, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=indice_2)%>%
  bg(i=indice_2, bg = "#A3A3A3", part = "body")%>%
  bg(i=indice_2+1, bg = "#C2C2C2", part = "body")%>%
  set_caption(caption=titulo_tabla)%>%
  align(i=4, align = 'center')%>%
  # align(i=5, align = 'center')%>%
  align(i=indice_1, align = 'center')%>%
  align(i=indice_2, align = 'center')%>%
  width(j = 1, width = 1)%>%
  width(j = 2, width = 6)%>%
  height(height = .1)

```

\newpage

<br> <br>

```{r tablauc4, echo=FALSE}
####################################################################################
# Datos Entrada
####################################################################################

titulo_tabla <- 'SCSU UC. 4.'

nombre_uc <- 'UC.4: ver cuadros de ayuda.'

descripcion_uc <- 'El usuario al pasar el mouse sobre áreas de introducción de datos o pestañas, ve menús de ayuda contextuales para obtener información'

actor_uc <- 'usuario'

flujo_basico <- 'El caso de uso inicia cuando el usuario: \n 1) Pasa el mouse, sin hacer clic, sobre una determinada área de la interfaz de usuario \n 2) El caso de uso termina'

#######flujo alternativo
flu_alt_titulo <- c('N/A')
flu_alt_descrip <- c('N/A')

#######Precondiciones
precond_titulo <- c('N/A')
precond_descri <- c('N/A')

#######Postcondiciones
postcond_titulo <- c('Éxito','Fracaso')
postcond_descri <- c('aparece un recuadro en el lateral próximo al puntero del mouse','No se muestra el menú contextual')

####################################################################################
# Otras variables
####################################################################################

df_encabezado=data.frame(`a`=c('Nombre',
                               'Descripción',
                               'Actor',
                               'Flujo de Eventos',
                               'Flujo Básico',
                               flujo_basico,
                               'Flujo Alternativo',
                               'Título'),
                         `b`=c(nombre_uc,
                               descripcion_uc,
                               actor_uc,
                               '',
                               '',
                               '',
                               '',
                               'Descripción'))


df_flujo_alt <- data.frame(`a`=flu_alt_titulo,
                           `b`=flu_alt_descrip)

df_precondiciones <- data.frame(`a`=c('Precondiciones',
                                      'Título',
                                      precond_titulo),
                                `b`=c('',
                                      'Descripción',
                                      precond_descri))

df_postcondiciones <- data.frame(`a`=c('Postcondiciones',
                                       'Título',
                                       postcond_titulo),
                                 `b`=c('',
                                       'Descripción',
                                       postcond_descri))


indice_1=nrow(df_encabezado)+nrow(df_flujo_alt)+1
indice_2=nrow(df_encabezado)+nrow(df_flujo_alt)+nrow(df_precondiciones)+1
######

df_salida <- bind_rows(df_encabezado,
                df_flujo_alt,
                df_precondiciones,
                df_postcondiciones)


flextable(df_salida)%>%
  delete_part(part = "header")%>%
  theme_box()%>%
  fontsize(size = 10)%>%
  bg(i=1,j=1, bg = "#C2C2C2", part = "body")%>%
  bg(i=2,j=1, bg = "#C2C2C2", part = "body")%>%
  bg(i=3,j=1, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=4)%>%
  bg(i=4, bg = "#8F8F8F", part = "body")%>%
  merge_at(i=5)%>%
  bg(i=5, bg = "#A3A3A3", part = "body")%>%
  merge_at(i=6)%>%
  merge_at(i=7)%>%
  bg(i=7, bg = "#A3A3A3", part = "body")%>%
  bg(i=8, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=indice_1)%>%
  bg(i=indice_1, bg = "#A3A3A3", part = "body")%>%
  bg(i=indice_1+1, bg = "#C2C2C2", part = "body")%>%
  merge_at(i=indice_2)%>%
  bg(i=indice_2, bg = "#A3A3A3", part = "body")%>%
  bg(i=indice_2+1, bg = "#C2C2C2", part = "body")%>%
  set_caption(caption=titulo_tabla)%>%
  align(i=4, align = 'center')%>%
  # align(i=5, align = 'center')%>%
  align(i=indice_1, align = 'center')%>%
  align(i=indice_2, align = 'center')%>%
  width(j = 1, width = 1)%>%
  width(j = 2, width = 6)%>%
  height(height = .1)

```

\newpage

<br> <br>

###### Modelo Entidad Relación:

El modelo "entidad-relación" que se ve en detalle en la figura \@ref(fig:diagramer), se construyó teniendo como base el diseño del Prototipo. En esta nueva versión hay otras tablas que sirven de apoyo a la entidad "Corpus" como la tabla "Facultad", "Escuela_Postgrado" y "jerarquia".

```{r diagramer, echo=FALSE, fig.align='center', fig.cap='SCSU: Diagrama Entidad Relación', out.width="80%"}

knitr::include_graphics("images/05-desarrollo/4_ciclo/diagrama_entidadrel_SCSU.png")
```

Adicionalmente a la tabla "Corpus" se le añade una columna que se corresponde con las "recomendaciones" generadas para cada documento, donde se almacena una *string* que corresponde a una estructura de datos "html".

###### Diagrama General del Sistema- versión contenedores:

En la figura \@ref(fig:diagramacontenedores) se define la estructura que será implementada, teniendo en cuenta los Requerimientos Funcionales y No Funcionales indicados anteriormente. La propuesta funciona como un Sistema Distribuido, ver \@ref(SD), donde mediante el uso de contenedores, ver \@ref(contenedores), se crean instancias en las que son ejecutadas los servicios necesarios para que el Sistema Complementario Saber UCV puede cumplir con los objetivos propuestos. La integración y la asignación de recursos a cada contenedor se hace mediante el uso de un orquetador, ver \@ref(orquestador).

```{r diagramacontenedores, echo=FALSE, fig.align='center', fig.cap='SCSU: Diagrama General del Sistema en Contenedores', out.width="80%"}

knitr::include_graphics("images/05-desarrollo/4_ciclo/digrama_contenedores_modulos.png")
```

En la fase de colaboración de este ciclo \@ref(implemencolab) se especificarán las tareas y funciones asignadas a cada contenedor.

###### Esquema General de Clasificación y Extracción de Datos del SCSU:

En la figura \@ref(fig:diagramaextra) se observa el proceso de extracción y clasificación de datos que realizará el SCSU, obteniendo los valores desde el repositorio Saber UCV, teniendo dos ramas principales que alimentan a la base de datos.

La lógica de este proceso es que en la primera rama se descarga el documento anexo a cada investigación, cuando se cumplan las condiciones, se hace la lectura del documento y se inicia el proceso de clasificación por facultad y área de estudio, como se vio en la "Iteración- Extracción y Clasificación de las Investigaciones" en \@ref(asignacion).

En la segunda rama se hace el "etiquetado del discurso", que fue revisado en la "Iteración- Preparación del Corpus" en \@ref(iternlp).

Ambos procesos alimentarán a las correspondientes tablas en la Base de Datos.

El proceso indicado se ejecuta inicialmente para realizar el poblado de la Base de Datos e igualmente se replica periódicamente, según parámetro definido en el archivo de configuración del orquestador, para actualizar e incorporar las nuevas investigaciones que se encuentren disponibles en el repositorio Saber UCV.

```{r diagramaextra, echo=FALSE, fig.align='center', fig.cap='Esquema Extracción y Clasificación de Datos', out.width="90%"}

knitr::include_graphics("images/05-desarrollo/4_ciclo/esquema extraccion.png")
```

###### Interfaz Visual (Mock Up):

La interfaz cuenta con tres componentes:

1.  **Sidebar:** en la barra lateral que se ubicará en la parte izquierda de la aplicación y es el área donde el usuario definirá el texto de búsqueda así como los paremetros que le acompañan. En la figura \@ref(fig:sidebar2) se puede ver el diseño propuesto.

    ```{r sidebar2,echo=FALSE, fig.align='center', fig.cap='Interfaz de Usuario - Sidebar', out.width="20%"}

    knitr::include_graphics("images/05-desarrollo/4_ciclo/UI/sidebar.jpg")
    ```

2.  **Main Tabs:** es la sección de pestañas que se encontrará en la parte media alta de la aplicación y es donde el usuario podrá seleccionar la visualización de los "tabla resultados", bien sea para la tabla de los documentos o para ver los "mapas del conocimiento". En la figura \@ref(fig:maintab) se representa la propuesta. Al seleccionar una pestaña está cambiará el color de fondo verde a gris.

    ```{r maintab, echo=FALSE, fig.align='center', fig.cap='Interfaz de Usuario - Pestañas ', out.width="90%"}

    knitr::include_graphics("images/05-desarrollo/4_ciclo/UI/maintab.png")
    ```

3.  **Resultados:** debajo de "Main Tabs" se presentan los resultados obtenidos en el proceso de búsqueda. La representación es contextual basada en la selección de pestaña que está seleccionada ("tabla resultados" o "mapas de conocimiento").

    1.  En las figura \@ref(fig:tablaresultados2) se puede ver la representación de la tabla de resultados de una búsqueda.

        ```{r tablaresultados2,echo=FALSE, fig.align='center', fig.cap='Interfaz de Usuario - Tabla Resultados de Búsqueda', out.width="90%"}
        knitr::include_graphics("images/05-desarrollo/4_ciclo/UI/tablaresultado.png")
        ```

    2.  En las figura \@ref(fig:tablaresultados3) se puede ver la representación propuesta de "mapas de conocimiento".

        ```{r tablaresultados3,echo=FALSE, fig.align='center', fig.cap='Interfaz de Usuario - Mapas de Conocimiento', out.width="70%"}
        knitr::include_graphics("images/05-desarrollo/4_ciclo/UI/uimapas.png")
        ```

Adicional a estos tres componentes la propuesta de intefaz incluye:

1.  **Tooltips**: al hacer hoover por áreas de introducción o selección de valores se despliegará un texto de ayuda según se ve en la \@ref(fig:tooltip).

    ```{r tooltip, echo=FALSE, fig.align='center', fig.cap='Interfaz de Usuario - Tooltips ', out.width="30%"}
    knitr::include_graphics("images/05-desarrollo/4_ciclo/UI/tooltip.png")
    ```

2.  **Inspección documento:** la tabla de resultados dispondrá de un ícono "+" que expande la tabla para mostrar el detalle y recomendaciones de un documento, como se observa en la figura \@ref(fig:detalledoc).

    ```{r detalledoc, echo=FALSE, fig.align='center', fig.cap='Interfaz de Usuario -Inspección Documento ', out.width="80%"}
    knitr::include_graphics("images/05-desarrollo/4_ciclo/UI/uiinspecciontabla.png")
    ```

3.  **Inspección "mapas de conocimiento":** la propuesta de visualización del detalle de aparición en documentos de una determinada coocurrencia de dos palabrasque se encuentran unidas por un arco se ven en la figura \@ref(fig:detallemc). La tabla que se muesta aparecerá mediante una ventana desplegable.

    ```{r detallemc, echo=FALSE, fig.align='center', fig.cap='Interfaz de Usuario- Inspección Mapas de Conocimiento ', out.width="60%"}
    knitr::include_graphics("images/05-desarrollo/4_ciclo/UI/uiinspeccionmapas.png")
    ```

###### Módulos Aplicación Web:

La aplicación web, que será desarrollada en el *framework* "Shiny", según las prácticas de desarrollo recomendadas, se hace bajo la arquitectura de módulos denominados "Shiny Modules", descomponiendo en partes independientes cada par de funciones relacionadas en la *UI* (define la interfaz de usuario) y el *Server* (define la lógica del servidor), facilitando la comprensión del funcionamiento de la aplicación, la legibilidad del código, pruebas unitarias y el mantenimiento [@wickham2021]. En la figura \@ref(fig:shinymodulo) se puede ver la arquitectura de la aplicación que se integrará dentro del Sistema distribuido.

```{r shinymodulo,echo=FALSE, fig.align='center', fig.cap='Diagrama Aplicación Web Shiny Versión Modular', out.width="60%"}


knitr::include_graphics("images/05-desarrollo/4_ciclo/digrama_shinyapp_modulos.png")
```

##### Colaboración: {#implemencolab}

Según lo propuesto en la fase de Especulación \@ref(implemenesp) se hizo el desarrollo del Sistema mediante un conjunto de contenedores orquestados, tomando en consideración que es necesario implementar los mecanismos de comunicación necesarios con la definición de una red y de disponer de un volumen compartido con la máquina *host*.

En la implementación se usaron ampliamente los conocimientos adquiridos en ciclos anteriores, no obstante también se tuvieron que revisar documentaciones, probar distintas opciones de librerías e imágenes de contenedores que permitiesen llevar a un entorno de producción, con la debida integración de los componentes, el Sistema planificado.

###### Contenedores:

Para realizar la implementación del Sistema, apoyado en contenedores para los distintos servicios que lo conforman, se eligió docker como herramienta. Mediante un archivo denominado "dockerfile" para cada servicio se especificó el entorno, dependencias y configuraciones necesarias para crear la imagen que servirá para instanciar cada contenedor.

A continuación se indican los contenedores que forman parte del Sistema y los puertos que disponden para interactuar unos con otros:

1.  **Nginx:** Es el servidor web/proxy inverso de código abierto que en este Sistema se usa para redireccionar las peticiones del cliente recibidas en el puerto 80 y 443 al puerto 8080. En este contenedor también se almacena el certificado SSL para permitir conexiones por el protocolo HTTPS en caso de configurar un dominio[^05-desarrollo-7]. Este contenedor fue creado desde la imagen oficial de NGINX que se encuentra en el "docker hub" sin añadir ninguna capa (layer) adicional.

2.  **Cerbot:** es una herramienta de código abierto que permite tramitar y habilitar las conexiones mediante el protocolo HTTPS con el uso de un certificado "Let´s Encrypt". El uso de este certificado está asociado al uso de un dominio en el *deploy* de la aplicación. Este contenedor fue instanciado desde una imagen de CERBOT del "docker hub" sin realizar ninguna modificación.

3.  **Shinyproxy:** es una solución de código abierto para alojar aplicaciones Shiny [@shinyproxy2023] que cuenta con una tecnología estable y probada. El uso de este contenedor se justifica en el hecho de que una aplicación Shiny funciona como "single thread" y es necesario que ante cada petición de acceso, al servidor despliegue un *workspace* completamente aislado, es decir, un contenedor distinto. Shinyproxy es una implementación del servidor "*Spring boo*t" que tiene la capacidad de hacer la replicación indicada y permite controlar los recursos de memoria y cpu asignados, así como el timeout a cada instancia que se despliega. Otras ventajas que aporta, no implementadas en esta versión del Sistema, es que permite establecer *login* en el uso de la aplicación y la creación de grupos de usuarios con perfiles distintos, contando con soporte para distintos métodos de autenticación. Si bien en estos momentos la aplicación está concebida para el libre acceso, en algún momento se pude restringir y no sería necesario hacer modificaciones en la arquitectura, más allá de variaciones en el archivo de configuración. Este contenedor habilita el puerto 8080 para escuchar las peticiones y fue generado desde la imagen del docker hub *Shinyproxy*.

4.  **"*Shiny Web App*"**: en la imagen creada se encuentra la aplicación web con todas las dependencias y librerías necesarias para remitir los *querys* al contenedor del manejador de la base de datos, presentar los resultados y generar las visualizaciones correspondientes. Como se indicó anteriormente, cada vez que ocurre desde el navegador del cliente una petición de acceso, desde el contenedor *shinyproxy* , se crea una instancia de esta imagen con todos los elementos necesarios para que la *app* funcione. En caso de presentar alguna falla, el sistema es tolerante a los mismos, porque se pueden seguir recibiendo peticiones que replicarían un contenedor nuevo desde la imagen sin afectar al contenedor que presentase el fallo, o viceversa. Desde cada contenedor instanciado de esta imagen se realiza el acceso de lectura al contenedor que contiene *PostgreSQL,* donde reposa la base de datos que contiene los textos ya procesados. La imagen que se usa en este servidor fue definida a medida de las necesidades.

    **Lógica de la aplicación:**

    El esquema general de la aplicación es similar al que se presentó en el Prototipo en la figura \@ref(fig:esqshinyproto). A continuación se mencionan las entradas y salidas que se generan dentro del contenedor.

    1.  **Entradas:**

        1.  **Defición query:** Contiene un campo para la entrada de texto con el que se generará el query.- Contiene un selector para indicar si se quiere generar la coocurrencia de palabras- Contiene tablas para seleccionar: 1) Nivel académico del trabajo. Opciones (pregrado, especialización, maestría, doctorado). 2) Facultad o Centro de adscripción. Opciones: 11 Facultades más un centro (CENDES). 3) Nombre del pregrado o postgrado. En total son 412 las opciones. Cada una de las tablas anteriores se actualiza según se vayan seleccionando las relaciones y la disponibilidades. Por ejemplo, al seleccionar pregrado solo se mostrarán los nombres de las carreras de pregrado, pero si se selecciona también el nombre de la Facultad, sólo se mostrarán las carreras de pregrado dentro de la Facultad seleccionada. Para una determinado filtro se permiten selecciones múltiples dando una total flexibilidad al momento de ejecutar los *querys*. El texto del *query* es procesado convirtiéndolo a minúsculas y removiendo signos de puntuación.

        2.  **Selector "generar coocurrencias":** es una casilla que sirve para seleccionar si se van a representar los Mapas de Conocimiento o no. Por defecto está desseleccionado ya que la representación de los mapas incrementa el tiempo de procesamiento.

        3.  **"Clic" inspeccionar documento:** al obtener los resultados en una tabla se puede inspeccionar un documento particular para ver el texto resumen y las recomendaciones de documentos similares.

        4.  **Cantidad de coocurrencias:** teniendo como condición que previamente se seleccionara "generar coocurrencias",dentro de la pestaña de "Coocurrencias", correspondientes a los Mapas de Conocimiento, se puede modificar la cantidad de coocurrencias a representar. El valor por defecto representado es 60.

        5.  **"Clic" inspeccionar Mapa de Conocimiento:** dentro de la pestaña de "Coocurrencias", correspondientes a los Mapas de Conocimiento, al tener representadas dos palabras mediante nodos, existe un arco que las une. Sobre el arco se puede hacer clic e inspeccionar los documentos que contienen las palabras asociadas por el arco.

    2.  **Salidas:** Ante el *query* se genera:

        1.  **Resultados query:** en una tabla creada con la librería "DT" se muestra el listado de documentos recuperados mostrando en cada fila los siguientes atributos: autor, fecha, palabras clave, texto resumen, nombre tutor. Los documentos a presentar y el orden en que son presentados viene desde el contenedor "PostgreSQL". Adicionalmente se muestra un enlace al repositorio Saber UCV donde se encuentra alojado el respectivo trabajo (el documento en PDF). Igualmente se presentan los textos que tienen mayor similitud con el documento seleccionado. Un gráfico con la frecuencia por año de los trabajos extraídos mediante el *query*. El gráfico se generá con la librería "apexcharter", por lo cual tiene ciertas interactividades mostrando con el *hoover* el valor de la cantidad por año que está representada en cada columna.

        2.  **Mapas de Conocimiento dinámico:** Gráfico de coocurrencia interactivo de palabras que se genera mediante la librería de "VisNetwork*"* . Este gráfico permite distintas interacciones con el usuario como hacer zoom (in-out), seleccionar un determinado nodo, usar las teclas izquierda, derecha, arriba y abajo del teclado, así como también permite seleccionar un arco de unión entre dos palabras coocurrentes. Al realizar la selección se filtran un subconjunto de los documentos que contienen ambas palabras representadas por nodos. Los documentos filtrados se mostrarán en una tabla contigua, también generada en"DT", donde sólo se incluye el texto resumen de cada trabajo.

        3.  **Mapas de Conocimiento Estático:** gráfico de coocurrencias estático de palabras, donde mediante la librería "ggraph*"* son generados un par de gráficos con distintas granularidades. El primero exhibe la misma coocurrencia de palabras expuesta en el punto con una visualización estática. En cuanto a la granularidad se muestran las palabras que coocurren dentro de todo el resumen independientemente de la proximidad que tengan. El segundo gráfico también muestra la coocurrencia, pero solo de palabras que se encuentran en el texto resumen una seguida de otra representando los resultados con una menor granularidad.Con la librería "UdPipe*"* se generan las estructuras de datos necesarias para generar los grafos (arcos y nodos).

        En el Apéndice se listan los paquetes que usa este contenedor.

5.  **"PostgreSQL":** es el contenedor donde se encuentra el manejador de PostgreSQL versión 16 y en él se almacenan todas las tablas que usa el Sistema. También recae la funcionalidad de generar el "`ts_vector`", convertir el query a un "`ts_query`" y mediante la función "`ts_rank`" reordenar los resultados extraídos con base en un ranking junto con el indexado de las distintas tablas. En el ciclo "Iteración- Implementación Prototipo" \@ref(desarrollociclos3) se indican más detalles sobre las funciones señaladas. En este contenedor se instanció desde una imagen de *PostgreSQL* versión 16.1 extraída del "docker hub", a la cual no le fue realizada ninguna modificación distinta a la configuración para la definición de usuarios, contraseña y la ruta de un volumen compartido con el *host* para garantizar que se tengan "datos persistentes". Este contenedor recibe consultas del contenedor **"*S****hiny Web App*" y escritura-lectura desde el contenedor "R imagen Servicios" teniendo habilitado el puerto 5432.

6.  **"R Imagen Servicios"**: En este contenedor se creó una imagen con todos los servicios necesarios para realizar el *web crawling*, el procesamientos de textos y la descarga de los archivos desde Saber UCV para realizar la clasificación de las Tesis y demás trabajos. Al iniciar la configuración del Sistema, contiene las funcionalidades que permiten realizar la creación de la base de datos, las tablas y el poblado de estas. Periódicamente es invocado un *script* mediante un "cron job" para realizar los procesos de incorporación de aquellos documentos nuevos que se detecte que están disponibles en Saber UCV. La imagen base que se usa es la del proyecto Rocker [@RJ-2017-065:2017], la cual es una versión ampliamente probada y optimizada por la comunidad de usuarios de R.

    **Procesos:**

    1.  **Poblado base de datos:** se ejecutan los procesos para hacer el poblado inicial de base de dato así como a la creación del indexado de la base de datos en el contenedor "**PostgreSQL".** En la fase de especulación de este ciclo se presentó un diagrama que describe el esquema de"extracción y clasificación de los datos", ver figura \@ref(fig:diagramaextra).
    2.  **Descarga de datos:** proceso que fue abordado en el "Ciclo Conformación del Conjunto de Datos".
    3.  **Text Mining y NLP:** en el "Ciclo Prototipo SCSU", ver \@ref(desarrollociclos3), en la iteración "Preparación del Corpus", ver \@ref(iternlp), se detallan los procesamientos a los textos que ahora son ejecutados en este contenedor. La diferencia es que para usar "spacyr", al depender esta librería de una ambiente virtual en python , es necesario configurar otro contenedor con las dependencias y librerías que permitan el llamado al etiquetado del discurso. El contenedor que se integra para realizar estos procesos es **"Python Spacy".**
    4.  **Generación de recomendaciones:** se corresponde con detallado en el "Ciclo Prototipo de SCSU" en la iteración "Recomendación de Documentos", ver \@ref(imrecomendacion).

    En el Apéndice se listan las librerías que usa este contenedor

7.  **"Python Spacy":** Se creó una imagen que contiene un "Ubuntu 22" con "python 3.10", la librería "spacy v.3" y el modelo de Spacy "es_core_news_lg" . Su función es que mediante un volumen compartido pueda ser invocado desde el contenedor "R Imagen Servicios" para así realizar los procesamientos de NLP descritos.

[^05-desarrollo-7]: Al momento de presentar este Trabajo de Grado no se dispone de un dominio para realizar la configuración del certificado para conexiones HTTPS, no obstante en un momento previo en que se tenía disponible un dominio se hicieron las pruebas para comprobar la integración y correcto funcionamiento.

###### Orquestador:

Todos los contenedores mencionados es necesario que actúen de forma coordinada, compartiendo recursos como una red y volúmenes de almacenamiento. Para lograr esto se acude a la herramienta "Docker Compose" que se instala en el *host* y permite simplificar y automatizar el despliegue de aplicaciones compuestas por múltiples servicios y contenedores en entornos de desarrollo y producción.

Se define la infraestructura, configuración y gestión de una aplicación a través de un archivo YAML, denominado "docker-compose.yml" donde se detallan los servicios, contenedores, redes y volúmenes necesarios para la aplicación, así como las relaciones y configuraciones entre ellos, simplificando la orquestación de los contenedores.

También la adopción de Docker Compose facilita la replicación del entorno de desarrollo en diferentes máquinas, mejorando la consistencia en el desarrollo.

La responsabilidad del administrador queda reflejada en la definición de distintas variables de entorno contenidas en el archivo docker compose, como lo es la definición del período de actualización de los textos.

###### Mantenimiento del SCSU:

De acuerdo a la implementación del Sistema, los procesos de mantenimiento que corresponde ejecutar al administrador, son los que se detallan a continuación:

1.  Respaldos regulares de la base de datos:

    -   Se estableció un "*cron job*" en el contenedor **"R Imagen Servicios"** para realizar respaldos automáticos de la base de datos PostgreSQL.

2.  Monitoreo del rendimiento:

    -   Se utilizan herramientas de monitoreo para vigilar el rendimiento del contenedor PostgreSQL, Shinyproxy y Shiny.

3.  Logs y registro de eventos:

    -   Registro de logs de contenedores en un volumen compartido con el *host*.

4.  Gestión de dependencias:

    -   Se documentan las dependencias específicas de la versión de PostgreSQL, Shinyproxy y Shiny.

5.  Seguridad de red:

    -   Se configuró el *firewall* y las reglas de red para limitar el acceso no autorizado a los contenedores.

6.  Escalabilidad:

    -   Se realizaron pruebas de carga para identificar posibles cuellos de botella.

Eventualmente en caso del Sistema quedar en un entorno de Producción permanente, sería necesario agregar los siguientes procesos al Mantenimiento:

1.  Actualizaciones de seguridad:

    -   Supervisar actualizaciones de seguridad para PostgreSQL y la imagen de Shiny.

2.  Control de versiones:

    -   Utilizar un sistema de control de versiones para el código del SCSU.

3.  Documentación:

    -   Mantener actualizada la documentación del sistema, incluyendo la configuración, dependencias y procedimientos de recuperación ante desastres.

###### Requerimientos mínimos de hardware:

Para un concurrencia de 4 usuarios simultáneos, estos son los requerimientos de hardware:

-   2 CPU virtual
-   4 GB de memoria RAM
-   50 GB de disco duro

##### Aprender: {#implemenapre}

Este Ciclo trajo un profundo y largo proceso de aprendizaje ya que el desarrollo implicó integrar distintos elementos abordados previamente de forma aislada, más el reto de encontrar componentes que fuesen compatibles y adaptados a los requerimientos funcionales y no funcionales planteados.

La utilidad de esta integración también representa en que se cuenta con un *framework* que permite desplegar otro tipo de aplicaciones que pueden ser fácilmente escalables en entornos de producción o añadir módulos distintos al SCSU como pudiera ser una aplicación para la administración del sistema o consultar estadísticas de uso.

Como la cantidad de ciclos e iteraciones ejecutados previamente había sido extenso, en esta Ciclo, más allá del reto de la integración y evaluación de componentes, no fue de distinta índole el aprendizaje adquirido, sin embargo se decidió incorporar en la interfaz del usuario un gráfico que muestre la frecuencia de aparición de *query* en el tiempo así como una representación estática de los "mapas de conocimiento" ya que en esta versión se presenta con una mejor perspectiva la coocurrencia de términos, a diferencia de la versión interactiva que está diseñada para inspeccionar en detalle una determinada aparición de términos.

###### Objetivos alcanzados:

-   Se implementó el Sistema que cumple con el Objetivo General \@ref(objegeneral).

-   Se dispone de una versión del Sistema que es reproducible.

-   Se integraron con éxito los distintos componentes del Sistema.

\newpage

<br> <br>

### Ciclo Incorporación de Otras Investigaciones: {#desarrollociclos6}

En este ciclo se aborda la incorporación al "Sistema Complementario Saber UCV" publicaciones distintas a las investigaciones de pregrado y postgrado de la Universidad Central de Venezuela.

Lo propuesto en este Ciclo únicamente tiene como finalidad evaluar si el SCSU se puede usar para alojar revistas de investigación producidas dentro de la Universidad Central de Venezuela o fuera de ella.

##### Especulación: {#dcseisespe}

Para los centros de estudio que generen investigaciones que sean alojadas en un repositorio de datos y se cumpla la estructura donde se disponga de un título para la publicación, una fecha, un autor, palabras claves (opcional) y un texto, es posible replicar los métodos descritos anteriormente para obtener los datos e incorporarlos al SCSU.

Las publicaciones que se incorporarán son:

1.  Archivo histórico de la Revista "**Gestión I+D**" editada dentro de la Universidad Central de Venezuela por el Postgrado en Gestión de Investigación y Desarrollo de la Facultad de Ciencias Económicas y Sociales.

2.  Archivo histórico de la Revista "**Episteme NS**" editada dentro de la Universidad Central de Venezuela por el Instituto de Filosofía de la Facultad de Humanidades y Educación.

3.  Archivo histórico de la Revista "**Observador del Conocimiento**" editada por el Observatorio Nacional de Ciencia, Tecnología e Innovación, adscrito al Ministerio del Poder Popular para Ciencia y Tecnología.

4.  Artículo "**El Dorado Revisitado**" del Boletín Atropológico editado por la Universidad de los Andes.

##### Colaboración: {#dcseiscola}

Siguiendo las técnicas descritas en "Iteración-"Extracción de Datos web Saber UCV" \@ref(scrapeo), se hizo la descarga de los datos y se introdujeron a la base de datos sin alterar la estructura que se había propuesto en el "Ciclo de Integración de los Componentes" \@ref(desarrollociclos4) lo que motiva a que no se entre en detalles sobre los métodos aplicados para incorporar este nuevo lote de investigaciones.

Luego de hacer la descarga y procesamiento de los datos se obtuvieron la siguiente cantidad de artículos por Revista:

1.  Gestión I+D (UCV): 129

2.  Revista Episteme (UCV): 68

3.  Revista Observador del Conocimiento (ONCTI): 197

4.  Boletín Antropológico (ULA): 1

##### Aprender: {#dcseisapre}

En caso de querer expandir el Sistema a otro tipo de publicaciones o universidades puede resultar de utilidad crear un módulo para la incorporación de estas donde se determine previamente la estructura de las etiquetas *css* o *html* , y posteriomente ser realice el *scrapy* y la asignación de categorías, no obstante con este método no se resolvería, en caso de ser necesario, el proceso de tener que realizar clasificaciones por área de conocimiento como se hizo *ad hoc* para Saber UCV.

En los históricos de algunas publicaciones existen atributos que no cumplen con las etiquetas *css* identificadas para hacer la descarga y se presentan algunas fallas, no obstante es para un mínimo de artículos y se considera que sí es viable realizar las incorporaciones bajo el método propuesto.

\newpage

<br> <br>

### Ciclo Buscador Semántico: {#desasarrollociclos5}

Si bien en el "Ciclo de Integración de Componentes de Software" se cumplió con el objetivo general propuesto en esta investigación, en este ciclo se evalúa que el Sistema incorpore la búsqueda semántica \@ref(busquedasemantica).

Los procesos incorporados en este Ciclo se hacen con fines experimentales y no serán sometidos a las distintas pruebas y mediciones a las que sí será sometido más adelante la propuesta de software que se desarrolló en el ciclo "Integración de Componentes de Software"

Tampoco se aplicarán los distintos métodos de ingeniería de software para esta versión con busqueda semática, sino unicamente se expondrá el método adoptado para añadir al Sistema este tipo de búsqueda, que para la fecha forma parte del "estado del arte" en los sistemas de recuperación de información.

##### Especulación: {#semanespe}

La búsqueda semántica trata de mejorar la precisión de la búsqueda entendiendo el contenido de la consulta. A diferencia de los motores de búsqueda tradicionales, que sólo encuentran documentos a partir de coincidencias léxicas, la búsqueda semántica también puede encontrar sinónimos.

Adicionalmente se puede hacer el *reranking* con modelos de *machine learning* entrenados para tales fines [@gökçe2020], [@nogueira2019] pudiendo mejorar el ordenamiento con criterios de relevancia distintos a los vistos en el creado por la función "ts_rank" de postgreSQL en \@ref(iterbol).

Para incorporar estas funcionalidades se propone crear una "API" que sea implementada mediante el microframework "FastAPI" donde se permitirán recibir peticiones para:

1.  **Registrar Embedding**: Con el texto resumen de cada investigación, convertirlo en distintos trozos de texto, generar embeddings para cada trozo y que estos sean almacenados en una tabla de *embeddings* dentro de la base de datos, asociando el código de identificación del documento.

    ```{r semanticoregistrar,echo=FALSE, fig.align='center', fig.cap='Diagrama API- Registrar Embedding', out.width="45%"}


    knitr::include_graphics("images/05-desarrollo/5_ciclo/diagramapiregistrar.png")
    ```

2.  **Consultar:** Recibir el texto del query, convertirlo en un *embedding*, buscar los trozos de texto que presenten mayor similitud coseno, recuperar los identificadores de esos documentos, hacer un proceso de *reranking* con un modelo de *machine learning* y dar como resultado los identificadores de los veinte documentos más relevantes.

    ```{r semanticoconsultar,echo=FALSE, fig.align='center', fig.cap='Diagrama API- Consultar Embedding', out.width="45%"}

    knitr::include_graphics("images/05-desarrollo/5_ciclo/diagramapiconsultar.png")
    ```

##### Colaboración: {#semancola}

La API es una implementación realizada con el lenguaje de programación Python mediante el *microframework* "FastAPI". Como ya se había implementado una versión orquestada en contenedores se añadió como un contenedor adicional este componente.

Se hizo la evaluación de varios modelos de aprendizaje automático para generar los embeddigs los cuales se encuentran de libre acceso en el repositorio "Hugging Face" [@hfmodels2023].

Para realizar el proceso del "splitting" del texto resumen se usaron distintos modelos preentrenados para segmentar el texto acorde a las distintas ideas representadas, no necesariamente basándose en signos de puntuación. Dentro de los modelos revisados está "What it's the point" [@minixhofer-etal-2023-wheres] , "Text to Sentence Splitter" [@sensplit22023] seleccionando este último. Este proceso es necesario ejecutarlo porque se busca segmentar el texto en ideas, que queden registradas en *embeddings* para que en el momento de hacer el *query* se busque el trozo de texto que presente una mayor similitud con el *embedding* generado a partir del *query*.

Para poder registrar los vectores de *embeddings* y realizar la búsqueda por similaridad en el gestor de base de datos PostgreSQL fue necesario añadir la extensión "pgvector" [@pgvector2023].

El modelo usado para crear los *embeddings* es el "intfloat/multilingual-e5-base" [@wang2022], alojado en el repositorio de hugging face [@e5base], el cual limita la cantidad de tokens a 512, equivalente a 360 palabras por *embedding* y tiene un tamaño de 768 atributos. Previo a ser seleccionado se hizo una evaluación de otros modelos que tienen la misma funcionalidad como "dccuchile/bert-base-spanish-wwm-cased" [@canete2020].

El modelo "e5-base" presenta la mejor relación entre cantidad de descargas y peso del modelo. Teniendo presente que la cantidad de recursos computacionales que se disponen para hacer el *deploy* de todo el Sistema es limitado, se decidió usar este modelo que ocupa aproximadamente 1 GB de memoria RAM al estar en producción.

Para hacer los proceso de *reranking* se evaluaron dos modelos que son el "amberoad/bert-multilingual-passage-reranking-msmarco" [@erankingmsmarco] y el "IIC/roberta-base-bne-ranker" [@hfranker2023], siendo seleccionado segundo por presentar un menor peso, tener mayor cantidad de descargas y ayudar a minimizar el consumo de recursos dentro del sistema.

Es necesario señalar que los modelos de *machine learning* evaluados y seleccionados están entrenados para funcionar con textos en el idioma español.

En la aplicación Shiny se hicieron las modificaciones para hacer el llamado a la API y obtener de ella el listado de id´s de documentos seleccionados y posteriormente añadirlo a la lista de documentos recuperados mediante el método de índice invertido. Con la finalidad de evitar que se dupliquen documentos que sean obtenidos mediante el índice invertido y la búsqueda semántica se procedió a remover los documentos repetidos. Igualmente en la interfaz de usuario en la tabla que muestra los resultados se añadió una columna para señalar el método con el que fue recuperado el documento.

##### Aprender: {#semanapre}

Usar la "búsqueda semántica" amplia considerablemente las posibilidades de obtener documentos que resulten de interés para el investigador, ya que no sólo se queda circunscrita la recuperación de información a términos exactos, sino se extraen documentos que pueden estar altamente relacionados al tema de búsqueda. En la figura se presentan los resultados del *query* "problemas que enfrentan las mujeres".

```{r resultsemantico, echo=FALSE, fig.align='center', fig.cap='Resultados de Búsqueda Semántica', out.width="90%"}

knitr::include_graphics("images/05-desarrollo/5_ciclo/resultsemantico.png")
```

En este *query* se extrajeron 25 documentos y sino se hubiese usado esta técnica solo 5 los hubiesen sido extraídos.

Un elemento a tener en cuenta es que la búsqueda semántica y la ejecución del *reranking*, incrementa considerablemente el tiempo de ejecución del *query*, sin embarfo, es necesario señalar que el método que implementa postrgreSQL para hacer la comparación vectorial no es parte del "estado del arte" en la materia.

El uso de postgreSQL para hacer el proceso se hizo para simplificar la instalación de componentes adicionales dentro del Sistema, pero en caso de ser adoptada la búsqueda semántica en otra versión del SCSU, es necesario encontrar métodos que proporcionen un mejor desempeño en lo referente a los tiempos de ejecución del *query*.

En esta implementación se evaluaron algunos modelos para crear los *embeddings* sin entrar en consideración sobre cuáles presentan mejores resultados al evaluar la relevancia de los documentos extraídos. Este es un punto en que en futuras investigaciones es necesario ahondar para comparar cuáles pueden ser de mayor utilidad.

Igualmente es necesario en futuros trabajos establecer un umbral de similitud o cantidad de documentos a recuperar, óptimo, ya que al no hacerlo y simplemente reordenar los documentos por similitud, se extraerá todo el Corpus y se afectará directamente las métricas de precisión y exactitud (recall).

Otro aspecto que resulta factible es someter todos los resultados obtenidos en la búsqueda, tanto los obtenidos por el método de "full text search", como los recuperados por "búsqueda semántica", a un proceso de "reranking" mediante el modelo mostrado. La tendencia de incorporar ambos métodos de búsqueda se denomina "Búsqueda Híbrida".

\newpage

<br> <br>

## Pruebas de aceptación: {#pruebas}

La fase de "Pruebas de Aceptación" en el desarrollo del Sistema Complementario Saber UCV representa el punto culminante en los ciclos de desarrollo abordados ya que se mide si las expectativas y necesidades del usuario final fueron satisfechas con la solución propuesta. Estas pruebas verifican la conformidad del sistema con los requisitos previamente establecidos y evalúa la capacidad para satisfacer las demandas del entorno operativo.

En esta sección, se muestran los resultados de las distintas pruebas que fueron ejecutadas, diseñadas para validar no solo la funcionalidad técnica de la aplicación, sino también su capacidad para integrarse al contexto de uso previsto.

### Funcionales: {#pruebas1}

Las Pruebas Funcionales constituyen la columna vertebral en la validación del software, evaluando su comportamiento según los requisitos que fueron especificados.

En la tabla \@ref(tab:tablapruebasa) se muestran las pruebas de caja negra realizadas para evaluar si el Sistema se comporta según lo que fue definido para cada caso de uso sin evaluar los procesos, sino en de una forma concreta si la salida creada por el Sistema se corresponde a lo descrito en los "Casos de Uso" del "Ciclo Integración de Componentes de Software" \@ref(desarrollociclos4).

En la tabla \@ref(tab:tablapruebasb) se muestran las pruebas de caja negra realizadas para evaluar si el Sistema cumple con todos los requerimientos funcionales también definidos en Ciclo precitado.

\newpage

<br> <br>

```{r tablapruebasa, echo=FALSE}
listado_UC <- c('UC.1: Realizar proceso de recuperación de información (query)',
                'UC.1. Nivel 2: Realizar proceso de recuperación de información (query) aplicando filtros',
                'UC.2: Realizar Inspección de Recomendaciones',
                'UC.3: Realizar inspección de "Mapas de Conocimiento"',
                'UC.3. Nivel 2.1: Realizar inspección "Mapa de Conocimiento"',
                'UC.3. Nivel 2.2: modificar cantidad de coocurrencias en "Mapa de Conocimiento"'
                )

df_pruebas_UC <- data.frame(col1=listado_UC,
                            col2='verdad')



flextable(df_pruebas_UC)%>%
  theme_box()%>%
  fontsize(size = 10)%>%
  set_header_labels(
    values = list(
    col1="Caso de Uso",
    col2="Se realiza el comportamiento esperado"))%>%
  width(j= 1 ,width  = 8, unit = 'cm')%>%
  width(j= 2, width  = 4, unit = 'cm')%>%
  bg(bg = "#A3A3A3", part = "header")%>%
  align(j=2, align = 'center')%>%
  align_text_col( align = "center", header = TRUE, footer = TRUE)%>%
  set_caption(caption='Pruebas de Caja Negra: Casos de Uso')
 
```

\newpage

<br> <br>

```{r tablapruebasb, echo=FALSE}
listado_RF <- c('Interactividad',
                'Búsqueda de Contenido',
                'Filtrado de Búsquedas',
                'Generar Mapas del Conocimiento',
                '"Rankiar" los Documentos Recuperados',
                'Generar Recomendaciones',
                'Actualizar Periódicamente las Investigaciones',
                'Cuadros de Ayuda (Tooltips)',
                'Accesibilidad Web')

df_pruebas_rf <- data.frame(col1=listado_RF,
                            col2='verdad')

flextable(df_pruebas_UC)%>%
  theme_box()%>%
  fontsize(size = 10)%>%
  set_header_labels(
    values = list(
    col1="Requerimiento Funcional",
    col2="Se realiza el comportamiento esperado"))%>%
  width(j= 1 ,width  = 8, unit = 'cm')%>%
  width(j= 2, width  = 4, unit = 'cm')%>%
  bg(bg = "#A3A3A3", part = "header")%>%
  align(j=2, align = 'center')%>%
  align_text_col( align = "center", header = TRUE, footer = TRUE)%>%
  set_caption(caption='Pruebas de Caja Negra: Requerimientos Funcionales')

```

### Rendimiento:

La primera prueba de rendimiento que se realizó fue mediante la librería Shinytest [@shinytest], la cual está diseñada para facilitar a los desarrolladores verificar el comportamiento de sus aplicaciones de manera sistemática y reproducible mediante pruebas automatizadas en aplicaciones Shiny. Las pruebas se hacen con un *driver* que simula la interacción del usuario con el sistema, definiendo previamente cuáles son los comportamientos que se quieren simular. Igualmente se define en qué momento se quiere tomar una "snapshot" para posteriormente evaluar si se adapta al comportamiento esperado.

Concretamente se evalúo haciendo simulaciones de *query* con cada uno de los textos de los títulos de las investigaciones, evaluando que se generará al menos la extracción de un documento ante cada *query*, lo cual demostraría que todos los documentos que conforman el Corpus son extraídos.

El resultado obtenido fue el esperado con al menos un documento recuperado por *query*.

La segunda prueba aplicada fue la que se ejecuta con el paquete "Shinyloadtest" [@shinyloadtest] el cual permite evaluar el rendimiento y la escalabilidad de aplicaciones Shiny, al permitir simular múltiples usuarios interactuando simultáneamente con una aplicación y así evaluar el comportamiento bajo carga.

Adicionalmente este paquete genera informes detallados sobre el rendimiento, identificando posibles cuellos de botella y proporcionando métricas clave, como tiempos de respuesta y tasas de error.

Al realizar el shinytest no se detectaron cuellos de botella ni tiempos de carga que estuviesen fuera de lo esperado.

### Pruebas de Usabilidad:

En estas pruebas se mide que el Sistema disponga criterios de usabilidad: (texto y contenido pendiente por validar)

```{r tablapruebasc, echo=FALSE}

listado_PU <- c('Menús de ayuda (tooltips)',
                'Pantalla de Bienvenida')

df_pruebas_rf <- data.frame(col1=listado_PU,
                            col2='verdad')

flextable(df_pruebas_rf)%>%
  theme_box()%>%
  fontsize(size = 10)%>%
  set_header_labels(
    values = list(
    col1="Usabilidad",
    col2="Se realiza el comportamiento esperado"))%>%
  width(j= 1 ,width  = 8, unit = 'cm')%>%
  width(j= 2, width  = 4, unit = 'cm')%>%
  bg(bg = "#A3A3A3", part = "header")%>%
  align(j=2, align = 'center')%>%
  align_text_col( align = "center", header = TRUE, footer = TRUE)%>%
  set_caption(caption='Pruebas de Caja Negra: Usabilidad')
```

#### Interacción Usuario Sistema:

Se hizo una encuesta a cinco usuarios que interactuaron con el SCSU, donde cada uno de estos tiene un perfil de ser profesores universitarios de la Escuela de Computación o Postgrado de Ciencias de la Computación, de la Facultad de Ciencias de la Universidad Central de Venezuela.

Se les preguntó:

1.  ¿Ha hecho búsquedas en el repositorio Saber UCV? (sí - no).

2.  ¿Pudo realizar una búsqueda en el SCSU? (sí - no).

3.  ¿Los resultados de un query en el SCSU satisfacen sus expectativas ? (sí - no). se entiende por expectativa que los resultados se adaptan al *query* formulado.

4.  ¿Comprendió las funcionalidades de cada pestaña del SCSU apoyándose en el menú de ayuda y en los tooltips? (sí - no).

5.  ¿En caso de ser negativa la respuesta anterior cuál funcionalidad no comprendió? (realizar query, inspeccionar documentos recuperados, generar mapas de conocimiento, inspeccionar mapas de conocimiento, cambiar cantidad de coocurrencias, acceder a menú ayuda).

6.  ¿Los colores de la interfaz del SCSU resultan agradables? (sí - no).

Preguntas sobre lo "intuitivo de la interfaz" se omitieron en el diseño de la encuesta ya que el diseño no se considera que sea "intuitivo" al incorporar funcionalidades como la inspección de los Mapas del Conocimiento.

Los resultados obtenidos son:

Para la pregunta ¿Ha hecho búsquedas en el repositorio Saber UCV? se muestran en la figura \@ref(fig:preguntaa) donde se aprecia que indica ????, lo que conlleva a que

```{r preguntaa, echo=FALSE, fig.align='center', fig.cap=' ',  out.width="35%"}

df_muesta_clasificacion <- readRDS('data/muestra_clasificacion.rds')

df_muesta_clasificacion_totales <- df_muesta_clasificacion%>%
  group_by(jerarquia)%>%
  count()%>%
  rename(total= n)%>%
  as.data.frame()

labs <- paste0(df_muesta_clasificacion_totales$jerarquia,
               " (", df_muesta_clasificacion_totales$total, "%)")


ggpie(df_muesta_clasificacion_totales,
      "total",
      label = labs,
      lab.pos = "out",
      # lab.font = "white",
      fill = "jerarquia",
      # color = "white",
      palette = c("#00AFBB", "#E7B800", "#FC4E07","#4785FF") )+
  # theme(text=element_text(face='bold',size=20))+
  theme(legend.position="none")
```

Los resultados obtenidos para la pregunta ¿Pudo realizar una búsqueda en el SCSU? se muestran en la figura \@ref(fig:preguntab) donde se aprecia que indica ????, lo que conlleva a .

```{r preguntab, echo=FALSE, fig.align='center', fig.cap=' ',  out.width="35%"}

df_muesta_clasificacion <- readRDS('data/muestra_clasificacion.rds')

df_muesta_clasificacion_totales <- df_muesta_clasificacion%>%
  group_by(jerarquia)%>%
  count()%>%
  rename(total= n)%>%
  as.data.frame()

labs <- paste0(df_muesta_clasificacion_totales$jerarquia,
               " (", df_muesta_clasificacion_totales$total, "%)")


ggpie(df_muesta_clasificacion_totales,
      "total",
      label = labs,
      lab.pos = "out",
      # lab.font = "white",
      fill = "jerarquia",
      # color = "white",
      palette = c("#00AFBB", "#E7B800", "#FC4E07","#4785FF") )+
  # theme(text=element_text(face='bold',size=20))+
  theme(legend.position="none")
```

Los resultados obtenidos para la pregunta ¿Los resultados de un query en el SCSU satisfacen sus expectativas ? se muestran en la figura \@ref(fig:preguntac) donde se aprecia que indica ????, lo que conlleva a .

```{r preguntac, echo=FALSE, fig.align='center', fig.cap=' ',  out.width="35%"}

df_muesta_clasificacion <- readRDS('data/muestra_clasificacion.rds')

df_muesta_clasificacion_totales <- df_muesta_clasificacion%>%
  group_by(jerarquia)%>%
  count()%>%
  rename(total= n)%>%
  as.data.frame()

labs <- paste0(df_muesta_clasificacion_totales$jerarquia,
               " (", df_muesta_clasificacion_totales$total, "%)")


ggpie(df_muesta_clasificacion_totales,
      "total",
      label = labs,
      lab.pos = "out",
      # lab.font = "white",
      fill = "jerarquia",
      # color = "white",
      palette = c("#00AFBB", "#E7B800", "#FC4E07","#4785FF") )+
  # theme(text=element_text(face='bold',size=20))+
  theme(legend.position="none")
```

Los resultados obtenidos para la pregunta ¿Comprendió las funcionalidades de cada pestaña del SCSU apoyándose en el menú de ayuda y en los tooltips? se muestran en la figura \@ref(fig:preguntad) donde se aprecia que indica ????, lo que conlleva a.

```{r preguntad, echo=FALSE, fig.align='center', fig.cap=' ',  out.width="35%"}

df_muesta_clasificacion <- readRDS('data/muestra_clasificacion.rds')

df_muesta_clasificacion_totales <- df_muesta_clasificacion%>%
  group_by(jerarquia)%>%
  count()%>%
  rename(total= n)%>%
  as.data.frame()

labs <- paste0(df_muesta_clasificacion_totales$jerarquia,
               " (", df_muesta_clasificacion_totales$total, "%)")


ggpie(df_muesta_clasificacion_totales,
      "total",
      label = labs,
      lab.pos = "out",
      # lab.font = "white",
      fill = "jerarquia",
      # color = "white",
      palette = c("#00AFBB", "#E7B800", "#FC4E07","#4785FF") )+
  # theme(text=element_text(face='bold',size=20))+
  theme(legend.position="none")
```

Los resultados obtenidos para la pregunta ¿En caso de ser negativa la respuesta anterior cuál funcionalidad no comprendió? se muestran en la figura \@ref(fig:preguntae) donde se aprecia que indica ????, lo que conlleva a .

```{r preguntae, echo=FALSE, fig.align='center', fig.cap=' ',  out.width="35%"}

df_muesta_clasificacion_totales <- df_muesta_clasificacion%>%
  group_by(jerarquia)%>%
  count()%>%
  rename(total= n)%>%
  as.data.frame()

labs <- paste0(df_muesta_clasificacion_totales$jerarquia,
               " (", df_muesta_clasificacion_totales$total, "%)")


ggpie(df_muesta_clasificacion_totales,
      "total",
      label = labs,
      lab.pos = "out",
      # lab.font = "white",
      fill = "jerarquia",
      # color = "white",
      palette = c("#00AFBB", "#E7B800", "#FC4E07","#4785FF") )+
  # theme(text=element_text(face='bold',size=20))+
  theme(legend.position="none")
```

Los resultados obtenidos para la pregunta ¿Los colores de la interfaz del SCSU resultan agradables? ( se muestran en la figura \@ref(fig:preguntaf) donde se aprecia que indica ????, lo que conlleva a .

```{r preguntaf, echo=FALSE, fig.align='center', fig.cap=' ',  out.width="35%"}
df_muesta_clasificacion_totales <- df_muesta_clasificacion%>%
  group_by(jerarquia)%>%
  count()%>%
  rename(total= n)%>%
  as.data.frame()

labs <- paste0(df_muesta_clasificacion_totales$jerarquia,
               " (", df_muesta_clasificacion_totales$total, "%)")


ggpie(df_muesta_clasificacion_totales,
      "total",
      label = labs,
      lab.pos = "out",
      # lab.font = "white",
      fill = "jerarquia",
      # color = "white",
      palette = c("#00AFBB", "#E7B800", "#FC4E07","#4785FF") )+
  # theme(text=element_text(face='bold',size=20))+
  theme(legend.position="none")

```

### Validación de las Clasificaciones:

Para hacer la validación sobre las Clasificaciones por área de conocimiento, aplicación del Algoritmo Smith Waterman y para el nombre del tutor se adoptó la siguiente metodología:

1.  **Muestra aleatoria:** del total de trabajos que son investigaciones de pregrado y postgrado, se tomó una muestra aleatoria del 1 % de los trabajos, equivalente a 100 investigaciones. La cantidad de trabajos por categoría se observa en la figura \@ref(fig:muestracategoria).

    ```{r muestracategoria,echo=FALSE, fig.align='center', fig.cap='Muestra- Seleccionados por categoría',  out.width="50%" }
    df_muesta_clasificacion <- readRDS('data/muestra_clasificacion.rds')

    df_muesta_clasificacion_totales <- df_muesta_clasificacion%>%
      group_by(jerarquia)%>%
      count()%>%
      rename(total= n)%>%
      as.data.frame()

    labs <- paste0(df_muesta_clasificacion_totales$jerarquia,
                   " (", df_muesta_clasificacion_totales$total, "%)")


    ggpie(df_muesta_clasificacion_totales,
          "total",
          label = labs,
          lab.pos = "out",
          # lab.font = "white",
          fill = "jerarquia",
          # color = "white",
          palette = c("#00AFBB", "#E7B800", "#FC4E07","#4785FF") )+
      # theme(text=element_text(face='bold',size=20))+
      theme(legend.position="none")
    ```

    Para validar las clasificaciones realizadas mediante el algoritmo Smith Waterman se selección otra muestra como 17 investigaciones.

2.  **Validación manual:** Se procede con la muestra a validar manualmente si la clasificación efectuada y si el nombre del tutor extraído, se corresponden con el que aparece en el documento digital.

3.  **Calcular Métrica de Evaluación:** para este proceso se adoptó la métrica de exactitud "accuracy" que mide la proporción entre la cantidad de número de predicciones correctas dividida entre la cantidad total de instancias.

El resultado de exactitud obtenido para el proceso de categorización fue \_\_\_, mientras que para la extracción del nombre del tutor fue \_\_\_\_ y para la aplicación de Smith Waterman fue \_\_\_\_

### Relevancia: {#pruebas3}

Para hacer las mediciones de Relevancia se adoptó el siguiente método:

1.  **Definir Conjunto de Datos de Prueba:**
    -   Se seleccionaron aleatoriamente 50 documentos de trabajos de pregrado y postgrado asociados a la Escuela de Computación y el postgrado en Ciencias de la Computación. Se denomina conjunto A.
    -   Se seleccionaron aleatoriamente 50 documentos de trabajos de pregrado y postgrado de documentos que no pertenezcan a la Escuela de Computación ni al postgrado en Ciencias de la Computación. Se denomina conjunto B.
    -   Con el conjunto A y B se conformó el conjunto C.
2.  **Crear Consultas de Prueba:**
    -   Se diseñaron 10 consultas con términos asociados a los documentos del conjunto A.
3.  **Ejecutar Consultas en el Sistema:**
    -   Se hicieron las consultas para el conjunto C y se guardaron los resultados junto con el término del query.
4.  **Definir Relevancia Esperada por expertos:**
    -   A un grupo de expertos, sobre el conjunto de documentos C, se les pidió que indicaran para cada consulta, de las 10 definidas, cuáles son los 5 (o un número menor) documentos más relevantes. Observación: no se solicitó indicar algún tipo de ranking entre los documentos seleccionados.
5.  **Recopilar Resultados:**
    -   Para cada consulta evaluada por un experto se comparó si el Sistema introducía dentro de los 10 primeros lugares los cinco (o menos) documentos señalados por el experto. En esta evaluación no se toma en consideración el orden, ni del sistema ni de los expertos.
6.  **Calcular Métricas de Evaluación:**
    -   Con los resultados se calcularon las métricas *precisión* y *recall* y F1 ya que ellas son claves para la evaluación de sistemas de recuperación de información.
        -   **Precisión (Precision):** Número de documentos relevantes recuperados dividido por el número total de documentos recuperados.
        -   **Recall (***exhaustividad*)**:** Número de documentos relevantes recuperados dividido por el número total de documentos relevantes en la colección.
        -   **F1:** es una medida que combina precisión y exhaustividad en la evaluación de un modelo. Se calcula como la media armónica de precisión y exhaustividad.
7.  **Comparar cantidad de resultados de query obtenidos para Saber UCV y el S.C.S.U.:**
    -   Se comparan en tablas y gráficos 10 ejemplos de los querys entre la cantidad total de resultados obtenidos en Saber UCV y el S.C.S.U.
    -   Proceso de especulación sobre los resultados, teniendo presente el gran recall que presenta Saber UCV.

Los resultados obtenidos fueron:

-   **Precisión:**

-   **Recall**:

-   **F1**:

Las cifras obtenidas indican que el desempeño del Sistema \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Se hizo sobre el subcojunto de trabajos definidos ya que sería imposible abordar las distintas áreas de conocimiento que conforman el Corpus, que se vio que ascienden a --- , y crear la base de dato de documentos relevantes está fuera del alcance de esta investigación.

Con esta prueba se da por concluido el Desarrollo de la Solución.
