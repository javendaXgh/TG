<!DOCTYPE html>
<html lang="es-ES" xml:lang="es-ES">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 3 Marco Teórico-Referencial | RECUPERACIÓN, EXTRACCIÓN Y CLASIFICACIÓN DE INFORMACIÓN DE SABER UCV</title>
  <meta name="description" content="Trabajo de Grado de Maestría" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 3 Marco Teórico-Referencial | RECUPERACIÓN, EXTRACCIÓN Y CLASIFICACIÓN DE INFORMACIÓN DE SABER UCV" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/images/UCV.png" />
  <meta property="og:description" content="Trabajo de Grado de Maestría" />
  <meta name="github-repo" content="javendaXgh/TG" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 3 Marco Teórico-Referencial | RECUPERACIÓN, EXTRACCIÓN Y CLASIFICACIÓN DE INFORMACIÓN DE SABER UCV" />
  
  <meta name="twitter:description" content="Trabajo de Grado de Maestría" />
  <meta name="twitter:image" content="/images/UCV.png" />

<meta name="author" content="José Miguel Avendaño Infante" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="capproblema.html"/>
<link rel="next" href="mm.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/tabwid-1.1.3/tabwid.css" rel="stylesheet" />
<script src="libs/tabwid-1.1.3/tabwid.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://tgucv.netlify.app">SCSU</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#estructura"><i class="fa fa-check"></i><b>1.1</b> Estructura</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="capproblema.html"><a href="capproblema.html"><i class="fa fa-check"></i><b>2</b> El Problema</a>
<ul>
<li class="chapter" data-level="2.1" data-path="capproblema.html"><a href="capproblema.html#desproblema"><i class="fa fa-check"></i><b>2.1</b> Descripción del Problema</a></li>
<li class="chapter" data-level="2.2" data-path="capproblema.html"><a href="capproblema.html#delimitacion"><i class="fa fa-check"></i><b>2.2</b> Delimitación del Problema</a></li>
<li class="chapter" data-level="2.3" data-path="capproblema.html"><a href="capproblema.html#antecedentes"><i class="fa fa-check"></i><b>2.3</b> Antecedentes:</a></li>
<li class="chapter" data-level="2.4" data-path="capproblema.html"><a href="capproblema.html#descripcion"><i class="fa fa-check"></i><b>2.4</b> Descripción de la Solución</a></li>
<li class="chapter" data-level="2.5" data-path="capproblema.html"><a href="capproblema.html#justificacion"><i class="fa fa-check"></i><b>2.5</b> Justificación e Importancia</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="capproblema.html"><a href="capproblema.html#objegeneral"><i class="fa fa-check"></i><b>2.5.1</b> Objetivo General</a></li>
<li class="chapter" data-level="2.5.2" data-path="capproblema.html"><a href="capproblema.html#objeespe"><i class="fa fa-check"></i><b>2.5.2</b> Objetivos Específicos</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="capproblema.html"><a href="capproblema.html#aporte"><i class="fa fa-check"></i><b>2.6</b> Aportes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="teorico.html"><a href="teorico.html"><i class="fa fa-check"></i><b>3</b> Marco Teórico-Referencial</a>
<ul>
<li class="chapter" data-level="3.1" data-path="teorico.html"><a href="teorico.html#alghist"><i class="fa fa-check"></i><b>3.1</b> Reseña histórica</a></li>
<li class="chapter" data-level="3.2" data-path="teorico.html"><a href="teorico.html#infret"><i class="fa fa-check"></i><b>3.2</b> Recuperación de Información</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="teorico.html"><a href="teorico.html#SRI"><i class="fa fa-check"></i><b>3.2.1</b> Sistemas de Recuperación de Información (SRI)</a></li>
<li class="chapter" data-level="3.2.2" data-path="teorico.html"><a href="teorico.html#ejemplos-de-sri"><i class="fa fa-check"></i><b>3.2.2</b> Ejemplos de SRI</a></li>
<li class="chapter" data-level="3.2.3" data-path="teorico.html"><a href="teorico.html#MRI"><i class="fa fa-check"></i><b>3.2.3</b> Modelos de Recuperación de Información</a>
<ul>
<li class="chapter" data-level="3.2.3.1" data-path="teorico.html"><a href="teorico.html#MRIbol"><i class="fa fa-check"></i><b>3.2.3.1</b> Recuperación boleana</a></li>
<li class="chapter" data-level="3.2.3.2" data-path="teorico.html"><a href="teorico.html#invind"><i class="fa fa-check"></i><b>3.2.3.2</b> Índices Invertidos</a></li>
</ul></li>
<li class="chapter" data-level="3.2.4" data-path="teorico.html"><a href="teorico.html#relevancia"><i class="fa fa-check"></i><b>3.2.4</b> Relevancia</a></li>
<li class="chapter" data-level="3.2.5" data-path="teorico.html"><a href="teorico.html#ranking"><i class="fa fa-check"></i><b>3.2.5</b> Re Ordenamiento (re-ranking)</a>
<ul>
<li class="chapter" data-level="3.2.5.1" data-path="teorico.html"><a href="teorico.html#learning-to-rank-ltr"><i class="fa fa-check"></i><b>3.2.5.1</b> Learning to Rank (LTR)</a></li>
<li class="chapter" data-level="3.2.5.2" data-path="teorico.html"><a href="teorico.html#bm25"><i class="fa fa-check"></i><b>3.2.5.2</b> BM25</a></li>
</ul></li>
<li class="chapter" data-level="3.2.6" data-path="teorico.html"><a href="teorico.html#evaluacion"><i class="fa fa-check"></i><b>3.2.6</b> Medidas y Métodos de Evaluación de Desempeño de los SRI</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="teorico.html"><a href="teorico.html#PT"><i class="fa fa-check"></i><b>3.3</b> Procesamientos a los textos</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="teorico.html"><a href="teorico.html#nlproc"><i class="fa fa-check"></i><b>3.3.1</b> Procesamiento del Lenguaje Natural (Natural Language Processing- NLP)</a>
<ul>
<li class="chapter" data-level="3.3.1.1" data-path="teorico.html"><a href="teorico.html#token"><i class="fa fa-check"></i><b>3.3.1.1</b> Tokenizador</a></li>
<li class="chapter" data-level="3.3.1.2" data-path="teorico.html"><a href="teorico.html#pos"><i class="fa fa-check"></i><b>3.3.1.2</b> Etiquetado de Partes del Discurso <em>(Part of speech tagging-POS)</em></a></li>
<li class="chapter" data-level="3.3.1.3" data-path="teorico.html"><a href="teorico.html#steaming"><i class="fa fa-check"></i><b>3.3.1.3</b> Stemming</a></li>
<li class="chapter" data-level="3.3.1.4" data-path="teorico.html"><a href="teorico.html#lemma"><i class="fa fa-check"></i><b>3.3.1.4</b> Lematización</a></li>
</ul></li>
<li class="chapter" data-level="3.3.2" data-path="teorico.html"><a href="teorico.html#textmin"><i class="fa fa-check"></i><b>3.3.2</b> Minería de Texto</a>
<ul>
<li class="chapter" data-level="3.3.2.1" data-path="teorico.html"><a href="teorico.html#tdm"><i class="fa fa-check"></i><b>3.3.2.1</b> Term-Document Matrix</a></li>
<li class="chapter" data-level="3.3.2.2" data-path="teorico.html"><a href="teorico.html#coocurrencia"><i class="fa fa-check"></i><b>3.3.2.2</b> Coocurrencia de Palabras</a>
<ul>
<li class="chapter" data-level="3.3.2.2.1" data-path="teorico.html"><a href="teorico.html#mapacon"><i class="fa fa-check"></i><b>3.3.2.2.1</b> Mapas de Conocimiento</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3.3.3" data-path="teorico.html"><a href="teorico.html#similitud"><i class="fa fa-check"></i><b>3.3.3</b> Similitud de Documentos</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="teorico.html"><a href="teorico.html#SD"><i class="fa fa-check"></i><b>3.4</b> Sistemas Distribuidos</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="teorico.html"><a href="teorico.html#contenedores"><i class="fa fa-check"></i><b>3.4.1</b> Contenedores</a></li>
<li class="chapter" data-level="3.4.2" data-path="teorico.html"><a href="teorico.html#orquestador"><i class="fa fa-check"></i><b>3.4.2</b> Orquestadores</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="teorico.html"><a href="teorico.html#sota"><i class="fa fa-check"></i><b>3.5</b> Estado del Arte</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="teorico.html"><a href="teorico.html#embed"><i class="fa fa-check"></i><b>3.5.1</b> Embeddings</a></li>
<li class="chapter" data-level="3.5.2" data-path="teorico.html"><a href="teorico.html#trans"><i class="fa fa-check"></i><b>3.5.2</b> Arquitectura de Redes Neuronales <em>Transformers</em></a></li>
<li class="chapter" data-level="3.5.3" data-path="teorico.html"><a href="teorico.html#LLM"><i class="fa fa-check"></i><b>3.5.3</b> Largos Modelos de Lenguaje</a></li>
<li class="chapter" data-level="3.5.4" data-path="teorico.html"><a href="teorico.html#int"><i class="fa fa-check"></i><b>3.5.4</b> Integración</a></li>
<li class="chapter" data-level="3.5.5" data-path="teorico.html"><a href="teorico.html#busquedasemantica"><i class="fa fa-check"></i><b>3.5.5</b> Búsqueda Semántica</a></li>
<li class="chapter" data-level="3.5.6" data-path="teorico.html"><a href="teorico.html#tendencias-actuales"><i class="fa fa-check"></i><b>3.5.6</b> Tendencias Actuales</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mm.html"><a href="mm.html"><i class="fa fa-check"></i><b>4</b> Marco Metodológico</a>
<ul>
<li class="chapter" data-level="4.1" data-path="mm.html"><a href="mm.html#mmmetodologia"><i class="fa fa-check"></i><b>4.1</b> Método de Trabajo Kanban</a></li>
<li class="chapter" data-level="4.2" data-path="mm.html"><a href="mm.html#mmasd"><i class="fa fa-check"></i><b>4.2</b> Desarrollo Adaptable de Software</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="mm.html"><a href="mm.html#características"><i class="fa fa-check"></i><b>4.2.1</b> Características</a></li>
<li class="chapter" data-level="4.2.2" data-path="mm.html"><a href="mm.html#ciclos-de-desarrollo"><i class="fa fa-check"></i><b>4.2.2</b> Ciclos de Desarrollo</a>
<ul>
<li class="chapter" data-level="4.2.2.1" data-path="mm.html"><a href="mm.html#especulación"><i class="fa fa-check"></i><b>4.2.2.1</b> <strong>Especulación</strong></a></li>
<li class="chapter" data-level="4.2.2.2" data-path="mm.html"><a href="mm.html#colaboración"><i class="fa fa-check"></i><b>4.2.2.2</b> <strong>Colaboración</strong></a></li>
<li class="chapter" data-level="4.2.2.3" data-path="mm.html"><a href="mm.html#aprendizaje"><i class="fa fa-check"></i><b>4.2.2.3</b> <strong>Aprendizaje</strong></a></li>
</ul></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="desarrollo.html"><a href="desarrollo.html"><i class="fa fa-check"></i><b>5</b> Desarrollo de la Solución</a>
<ul>
<li class="chapter" data-level="5.1" data-path="desarrollo.html"><a href="desarrollo.html#desarollodescripcion"><i class="fa fa-check"></i><b>5.1</b> Descripción General de la Solución</a></li>
<li class="chapter" data-level="5.2" data-path="desarrollo.html"><a href="desarrollo.html#desarrolloarquitectura"><i class="fa fa-check"></i><b>5.2</b> Arquitectura de la Solución</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="desarrollo.html"><a href="desarrollo.html#modelo"><i class="fa fa-check"></i><b>5.2.1</b> <strong>Modelo</strong></a></li>
<li class="chapter" data-level="5.2.2" data-path="desarrollo.html"><a href="desarrollo.html#vista"><i class="fa fa-check"></i><b>5.2.2</b> <strong>Vista</strong></a></li>
<li class="chapter" data-level="5.2.3" data-path="desarrollo.html"><a href="desarrollo.html#controlador"><i class="fa fa-check"></i><b>5.2.3</b> <strong>Controlador</strong></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="desarrollo.html"><a href="desarrollo.html#desarrollociclos"><i class="fa fa-check"></i><b>5.3</b> Ciclos de Desarrollo</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="desarrollo.html"><a href="desarrollo.html#desarrollociclos1"><i class="fa fa-check"></i><b>5.3.1</b> Ciclo - Conformación del Conjunto de Datos</a>
<ul>
<li class="chapter" data-level="5.3.1.1" data-path="desarrollo.html"><a href="desarrollo.html#scrapeo"><i class="fa fa-check"></i><b>5.3.1.1</b> Iteración- “Extracción de Datos Web Saber UCV”</a></li>
<li class="chapter" data-level="5.3.1.2" data-path="desarrollo.html"><a href="desarrollo.html#labels"><i class="fa fa-check"></i><b>5.3.1.2</b> Iteración- Levantamiento de Categorías</a></li>
<li class="chapter" data-level="5.3.1.3" data-path="desarrollo.html"><a href="desarrollo.html#asignacion"><i class="fa fa-check"></i><b>5.3.1.3</b> Iteración- Extracción y Clasificación de las Investigaciones</a></li>
<li class="chapter" data-level="5.3.1.4" data-path="desarrollo.html"><a href="desarrollo.html#objetivos-alcanzados"><i class="fa fa-check"></i><b>5.3.1.4</b> Objetivos Alcanzados:</a></li>
</ul></li>
<li class="chapter" data-level="5.3.2" data-path="desarrollo.html"><a href="desarrollo.html#desarrollociclos3"><i class="fa fa-check"></i><b>5.3.2</b> Ciclo-Prototipo del SCSU</a>
<ul>
<li class="chapter" data-level="5.3.2.1" data-path="desarrollo.html"><a href="desarrollo.html#iternlp"><i class="fa fa-check"></i><b>5.3.2.1</b> Iteración- Preparación del Corpus</a></li>
<li class="chapter" data-level="5.3.2.2" data-path="desarrollo.html"><a href="desarrollo.html#imrecomendacion"><i class="fa fa-check"></i><b>5.3.2.2</b> Iteración - Recomendación Documentos</a></li>
<li class="chapter" data-level="5.3.2.3" data-path="desarrollo.html"><a href="desarrollo.html#iterbol"><i class="fa fa-check"></i><b>5.3.2.3</b> Iteración- Implementación Prototipo</a></li>
<li class="chapter" data-level="5.3.2.4" data-path="desarrollo.html"><a href="desarrollo.html#objetivos-alcanzados-1"><i class="fa fa-check"></i><b>5.3.2.4</b> Objetivos Alcanzados:</a></li>
</ul></li>
<li class="chapter" data-level="5.3.3" data-path="desarrollo.html"><a href="desarrollo.html#desarrollociclos4"><i class="fa fa-check"></i><b>5.3.3</b> Ciclo Integración de Componentes del Software</a>
<ul>
<li class="chapter" data-level="5.3.3.1" data-path="desarrollo.html"><a href="desarrollo.html#objetivos-alcanzados-2"><i class="fa fa-check"></i><b>5.3.3.1</b> Objetivos Alcanzados:</a></li>
</ul></li>
<li class="chapter" data-level="5.3.4" data-path="desarrollo.html"><a href="desarrollo.html#desarrollociclos6"><i class="fa fa-check"></i><b>5.3.4</b> Ciclo Incorporación de Otras Investigaciones</a>
<ul>
<li class="chapter" data-level="5.3.4.1" data-path="desarrollo.html"><a href="desarrollo.html#objetivos-alcanzados-3"><i class="fa fa-check"></i><b>5.3.4.1</b> Objetivos Alcanzados:</a></li>
</ul></li>
<li class="chapter" data-level="5.3.5" data-path="desarrollo.html"><a href="desarrollo.html#desasarrollociclos5"><i class="fa fa-check"></i><b>5.3.5</b> Ciclo Buscador Semántico</a>
<ul>
<li class="chapter" data-level="5.3.5.1" data-path="desarrollo.html"><a href="desarrollo.html#objetivos-alcanzados-4"><i class="fa fa-check"></i><b>5.3.5.1</b> Objetivos Alcanzados:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="desarrollo.html"><a href="desarrollo.html#pruebas"><i class="fa fa-check"></i><b>5.4</b> Pruebas de Aceptación</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="desarrollo.html"><a href="desarrollo.html#pruebas1"><i class="fa fa-check"></i><b>5.4.1</b> Funcionales</a></li>
<li class="chapter" data-level="5.4.2" data-path="desarrollo.html"><a href="desarrollo.html#rendimiento"><i class="fa fa-check"></i><b>5.4.2</b> Rendimiento</a></li>
<li class="chapter" data-level="5.4.3" data-path="desarrollo.html"><a href="desarrollo.html#pruebas-de-usabilidad"><i class="fa fa-check"></i><b>5.4.3</b> Pruebas de Usabilidad</a>
<ul>
<li class="chapter" data-level="5.4.3.1" data-path="desarrollo.html"><a href="desarrollo.html#interacción-usuario-sistema"><i class="fa fa-check"></i><b>5.4.3.1</b> Interacción Usuario Sistema</a></li>
</ul></li>
<li class="chapter" data-level="5.4.4" data-path="desarrollo.html"><a href="desarrollo.html#validación-de-las-clasificaciones"><i class="fa fa-check"></i><b>5.4.4</b> Validación de las Clasificaciones</a></li>
<li class="chapter" data-level="5.4.5" data-path="desarrollo.html"><a href="desarrollo.html#pruebas3"><i class="fa fa-check"></i><b>5.4.5</b> Relevancia</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="conclusiones.html"><a href="conclusiones.html"><i class="fa fa-check"></i><b>6</b> Conclusiones</a>
<ul>
<li class="chapter" data-level="6.1" data-path="conclusiones.html"><a href="conclusiones.html#conclusionescontri"><i class="fa fa-check"></i><b>6.1</b> Contribución</a></li>
<li class="chapter" data-level="6.2" data-path="conclusiones.html"><a href="conclusiones.html#conclusionestrabafutu"><i class="fa fa-check"></i><b>6.2</b> Trabajos Futuros</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="apéndice-librerías-usadas-y-créditos.html"><a href="apéndice-librerías-usadas-y-créditos.html"><i class="fa fa-check"></i>Apéndice: Librerías Usadas y Créditos</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">RECUPERACIÓN, EXTRACCIÓN Y CLASIFICACIÓN DE INFORMACIÓN DE SABER UCV</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="teorico" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Capítulo 3</span> Marco Teórico-Referencial<a href="teorico.html#teorico" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>En este capítulo se exponen los fundamentos teóricos que sustentan los procesos y métodos aplicados en la investigación <strong>Recuperación, Extracción y Clasificación de Información de SABER UCV</strong>.</p>
<div id="alghist" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Reseña histórica<a href="teorico.html#alghist" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>El profesor Donald Knuth señala, dentro del campo de las ciencias de la computación, que la <strong>búsqueda</strong> “<em>es el proceso de recolectar información que se encuentra en la memoria del computador de la forma más rápida posible, esto cuando tenemos una cantidad N de registros y nuestro problema es encontrar el registro apropiado de acuerdo a un criterio de búsqueda”</em> <span class="citation">(<a href="#ref-knuth1997">Knuth 1997, 392</a>)</span>. Este capítulo se inicia con esta cita ya que la recuperación de información gira en torno a un problema central de las ciencias de la computación que es la búsqueda.</p>
<p>En la década de 1940, cuando aparecieron las computadoras, las búsquedas no representaban mayor problema debido a que estas máquinas disponían de poca memoria <em>RAM</em> pudiendo almacenar solo moderadas cantidades de datos. No obstante, con el desarrollo e incremento del almacenamiento en memoria <em>RAM</em> o en dispositivos de almacenamiento permanentemente, ya en la década de 1950 empezaron a surgir los problemas de búsqueda y consecuentemente las primeras investigaciones para afrontarla.</p>
<p>Fue de esta manera que inicialmente se aplicaron estrategias de “búsqueda de fuerza bruta”, donde dado un texto <em>T</em> y una subcadena <em>P,</em> se va recorriendo cada elemento de la cadena <em>T</em> para detectar la aparición de la subcadena <em>P</em>. Si bien esta estrategia no presentaba el mejor desempeño, sí constituía una forma válida de enfrentar el problema de la búsqueda de subcadenas de texto.</p>
<p>Siguientemente, en la década de 1960 se adoptan estrategias basadas en arboles para resolver los problemas de búsqueda. De los primeros algoritmos que sirvieron para localizar la aparición de una frase dentro de un texto se tienen los de “<em>Pattern-Matching</em>” <span class="citation">(<a href="#ref-goodrich2013">Goodrich, Tamassia, y Goldwasser 2013</a>)</span>.</p>
<p>Avanzando con el recorrido histórico, en 1976 se introdujo el algoritmo “Knuth-Morris-Pratt<em>”</em> que tenía como novedad el que se agregó una función que permitía ir almacenando en una tabla las”previas coincidencias parciales”. Con mayor nivel de detalle, en esta tabla se registraban cuántos caracteres coincidentes se habían encontrado en una posición determinada cuando en la detección del patrón se generaban fallos. Con esto se logró que al momento de realizar un desplazamiento, se tomara en cuenta cuántos caracteres se podían reusar, logrando que se evitara retroceder más allá de lo necesario en el recorrido por la cadena de texto. Fue este enfoque el que permitió mejorar el rendimiento en lo relativo a los tiempos de ejecución comparado con las estrategias citadas previamente.</p>
<p>Seguidamente, en 1977 el problema de la búsqueda se enfrenta con un nuevo algoritmo que es el de “Boyer-Moore” en el cual se implementan dos heurísticas denominadas <em>looking-glass</em> y <em>character-jump,</em> las cuales permiten ir realizando algunos saltos en la búsqueda ante la no coincidencia de la subcadena con la cadena y adicionalmente el orden en el que se va realizando la comparación se invierte, trayendo como consecuencia que se obtuviese un mejor desempeño en el proceso de búsqueda.</p>
<p>Es importante mencionar que, sobre una modificación al algoritmo “Boyer-Moore” se sustenta la utilidad <em>grep</em> de la línea de comandos UNIX, la cual también da soporte a diversos lenguajes de programación para ejecutar búsquedas de texto, en un proceso que comúnmente es conocido como <em>grepping</em>. En particular, esta utilidad fue ampliamente usada para resolver distintos problemas de extracción de información en esta investigación.</p>
<p>Otra de las técnicas a considerar, ya que a ella se acudió para procesar las búsquedas de texto, fue el uso de la programación lineal, donde bajo la premisa “<em>divida et impera”,</em> los problemas que requieren tiempo exponencial para ser resueltos son descompuestos en polinomios y por lo tanto se disminuye la complejidad en tiempo para encontrar la solución.</p>
<p>Dentro de los algoritmos que recurren a la programación lineal está el llamado “Smith-Waterman” <span class="citation">(<a href="#ref-smith1981">Smith y Waterman 1981</a>)</span>, el cual se desarrolló para efectuar la alineación de cadenas del ADN de forma parcial o total dentro de una cadena mayor. Aunque su uso no estaba destinado a trabajar con caracteres, al poco tiempo de su aparición se identificó que el enfoque que adoptaba era extrapolable a la identificación de subcadenas de texto dentro de una cadena mayor, lo cual motivó a adoptar su uso en los procesos de búsqueda. Es importante señalar que este algoritmo se incorporó con éxito a los métodos implementados en esta investigación para resolver el problema de hacer coincidir las etiquetas de clasificaciones por área académica con el texto de las investigaciones y en el capítulo <a href="desarrollo.html#desarrollo">5</a>, “Desarrollo de la Solución”, se indicará en detalle cómo fue usado.</p>
<p>Avanzando con el recorrido histórico, corresponde mencionar los algoritmos “<em>tries”</em> <span class="citation">(<a href="#ref-aho1975">Aho y Corasick 1975</a>)</span>, en los que se representan los textos mediante estructuras jerárquicas de datos en un árbol compuesto por <em>tries,</em> donde se tienen nodos conectados representando cada uno un carácter y de esta manera la ruta desde la raíz hasta un nodo dado, forma una palabra o cadena. Esta estructura permite hacer búsquedas rápidas basadas en patrones y su eficiencia depende de la longitud de la palabra y no del tamaño total del conjunto de datos. Otro elemento a destacar de este algoritmo es que, los textos son sometidos a procesamientos que se hacen previamente a que ocurra el requerimiento de información, con lo cual al momento de hacer la búsqueda ya se dispone de una parte del trabajo realizado y de esta manera, al no tener que ejecutar todo el proceso sobre la marcha, se logran disminuir los tiempos de respuesta.</p>
</div>
<div id="infret" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Recuperación de Información<a href="teorico.html#infret" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Christopher Manning, uno de los investigadores con mayor dominio sobre el tema de recuperación de información (RI), la define como el proceso de encontrar materiales que satisfacen una necesidad de información cuando estos se encuentran dentro de grandes colecciones, y generalmente son textos almacenados de forma no estructurada en computadores <span class="citation">(<a href="#ref-manning2008">C. D. Manning, Raghavan, y Schütze 2008</a>)</span>. Adicionalmente, el autor Charu Aggarwal menciona que el objetivo que tiene realizar la recuperación de información es conectar la información correcta, con los usuarios correctos en el momento correcto <span class="citation">(<a href="#ref-miningt2012">Aggarwal y Zhai 2012</a>)</span>.</p>
<p>En consecuencia, el eje central sobre el cual gira el proceso de recuperación de información es satisfacer las necesidades de información relevante que sean expresadas por un usuario mediante una consulta de texto a la cual se denomina <strong><em>query</em></strong>.</p>
<p>En tal sentido, a los efectos de delimitar el espacio de búsqueda sobre el que se realiza la acción de la consulta, se tiene el <strong>corpus</strong>, al cual se define como el conjunto cerrado de documentos codificados electrónicamente que se encuentra integrado en un sistema de almacenamiento <span class="citation">(<a href="#ref-martiaurora">Martín de Santa Olalla Sánchez 1994</a>)</span> o entendido desde otra perspectiva, es el conjunto de datos en el cuál se hará la búsqueda, generando de esta manera el proceso de recuperación de información.</p>
<p>Ahondando un poco más en el tema se tiene que, satisfacer una necesidad de recuperación de información no solo se circunscribe a un problema de búsqueda de un texto dentro de un corpus. En la mayoría de los casos se deberá cumplir con ciertos criterios o restricciones, como por ejemplo, que la aparición del <em>query</em> en los documentos esté dentro de un período de fechas o que se encuentre limitado a otras restricciones, siendo esto a lo que se le denomina “búsqueda multi atributo”.</p>
<p>Por otra parte, dentro de los fundamentos de la RI se tiene que el orden en que sean presentados los distintos documentos recuperados en un proceso de búsqueda, dependerá de la aparición, parcial o total y de la frecuencia, de las palabras del query dentro de un documento. Lo antes mencionado, junto con otros criterios determina la denominada “relevancia”, concepto que será abordado con mayor detalle más adelante en <a href="teorico.html#relevancia">3.2.4</a>, “Relevancia”.</p>
<p>Adicionalmente, en los procesos de recuperación de información es válido incorporar documentos que no coincidan exactamente con los términos buscados sino otros que contengan palabras que sean sinónimos o que presenten alguna similitud con el texto del <em>query</em>. Lo antes mencionado permite incorporar formalmente dentro del proceso de recuperación de información algo de imprecisión con la intención última de enriquecer el proceso <span class="citation">(<a href="#ref-kraft2017">Kraft y Colvin 2017</a>)</span>. En <a href="teorico.html#similitud">3.3.3</a>, “Similitud de Documentos”<strong>,</strong> y en <a href="teorico.html#embed">3.5.1</a>, “<em>Embeddings”</em><strong>,</strong> se mencionan y especifican algunas de las técnicas con las cuales se incorporan este lote de documentos en los resultados de una búsqueda.</p>
<p>Recapitulando, se tiene que el proceso de recuperación de información está compuesto principalmente por los siguientes elementos:</p>
<ul>
<li><p>Un <em><strong>query</strong>:</em> el texto a buscar.</p></li>
<li><p>Un <strong>corpus</strong>: los documentos sobre los cuales se efectuará la búsqueda de información.</p></li>
<li><p>Una función de <strong>relevancia</strong>: la que permite ordenar los documentos recuperados de mayor a menor importancia para el usuario.</p></li>
</ul>
<div id="SRI" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Sistemas de Recuperación de Información (SRI)<a href="teorico.html#SRI" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Como se vio en la sección anterior, el desarrollo de algoritmos y métodos que permiten realizar procesos de búsqueda, teniendo en paralelo el crecimiento exponencial de datos disponibles en formato digital <span class="citation">(<a href="#ref-worldde2016"><em>World Development Report 2016: Digital Dividends</em> 2016</a>)</span>, así como también la necesidad de resolver los problemas asociados a la búsqueda con múltiples atributos en tiempos que resulten aceptables, fue lo que abonó las condiciones para la creación de los sistemas de recuperación de información (SRI).</p>
<p>Estos sistemas son los dispositivos (<em>software</em> y/o <em>hardware</em>) que median entre un potencial usuario que requiere información y la colección de documentos que puede contener la información solicitada <span class="citation">(<a href="#ref-kraft2017">Kraft y Colvin 2017</a>)</span> 1. El SRI se encargará de la representación, el almacenamiento y el acceso a los datos que están estructurados, teniendo presente que las búsquedas que sobre él recaigan conllevan distintos costos, siendo el principal el tiempo que tarde en efectuarse la misma.</p>
<p>Dentro de este contexto, es conocido que los datos estructurados son gestionados mediante un sistema gestor de base de datos, no obstante en el caso de los textos, se manejan por medio de un motor de búsqueda (<em>search engines</em>), motivado a que estos en un estado crudo carecen propiamente de estructura <span class="citation">(<a href="#ref-miningt2012">Aggarwal y Zhai 2012</a>)</span>. Son estos motores los que permiten que un usuario pueda encontrar fácilmente la información que resulte de utilidad mediante un <em>query</em> usando las estructuras de datos, los algoritmos de búsqueda y la aplicación de funciones de relevancia que resulten óptimos para el proceso de recuperación de información.</p>
</div>
<div id="ejemplos-de-sri" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Ejemplos de SRI<a href="teorico.html#ejemplos-de-sri" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A continuación se mencionan dos sitios de internet que funcionan como SRI sobre corpus de investigaciones científicas.</p>
<ol style="list-style-type: decimal">
<li><p>Arxiv alojado en <a href="https://arxiv.org/" class="uri">https://arxiv.org/</a>: es un repositorio de trabajos de investigación. Al momento del usuario hacer un requerimiento de información, adicional al texto de la búsqueda, se pueden indicar distintos filtros a aplicar como puede ser el área del conocimiento (física, matemática, computación, etc.), si se quiere ejecutar la busqueda sólo, o de forma combinada dentro de: el título, el nombre autor, el resumen <em>o</em> en las referencias.</p></li>
<li><p>Portal de la <em>Asociation Computery Machine</em> (ACM) alojado en <a href="https://dl.acm.org" class="uri">https://dl.acm.org</a>: incorpora un motor de búsqueda con particulares características ya que los resultados son acompañados por distintas representaciones gráficas que le dan un valor agregado. En la figura <a href="teorico.html#fig:busquedasacm">3.1</a> se ve una de estas representaciones que incluye la frecuencia de aparición de los términos del <em>query</em> dentro del corpus en el tiempo.</p></li>
</ol>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:busquedasacm"></span>
<img src="images/03-marco-teorico/busquedaacm.png" alt="Gráfico que acompaña resultados de búsqueda de un término en la biblioteca digital de la Association for Computing Machinery (https://dl.acm.org/)" width="30%" />
<p class="caption">
Figura 3.1: Gráfico que acompaña resultados de búsqueda de un término en la biblioteca digital de la Association for Computing Machinery (<a href="https://dl.acm.org/" class="uri">https://dl.acm.org/</a>)
</p>
</div>
</div>
<div id="MRI" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Modelos de Recuperación de Información<a href="teorico.html#MRI" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="MRIbol" class="section level4 hasAnchor" number="3.2.3.1">
<h4><span class="header-section-number">3.2.3.1</span> Recuperación boleana<a href="teorico.html#MRIbol" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>En este modelo ante una búsqueda de información se recorre linealmente todo el documento para retornar un valor boleano indicando la presencia o no del término buscado. Es uno de los primeros modelos que se usó y está asociado a técnicas de <em>grepping</em> <span class="citation">(<a href="#ref-manning2008">C. D. Manning, Raghavan, y Schütze 2008</a>)</span>. El desarrollo de este modelo apareció entre 1960 y 1970.</p>
<p>El ojetivo planteado por este modelo es que el usuario final obtenga como respuesta a un <em>query</em> solo aquellos textos que contengan el término. Es un modelo muy cercano a los típicos <em>querys</em> de bases de datos con el uso de operadores lógicos “<em>AND</em>”, “<em>OR</em>” y “<em>NOT</em>”.</p>
<p>Más especificamente, el modelo boleano en el procesamiento de los textos genera una matriz de incidencia binaria término-documento, donde cada término que conforma el vocabulario, ocupa una fila <em>i</em> de la matriz, mientras que cada columna <em>j</em> se asocia a un documento. La presencia del término <em>i</em> en el documento <em>j</em> se denotará con un valor verdadero o un “1”.</p>
<p>De acuerdo a lo anterior, la recuperación boleana si bien representa una buena aproximación al procesamiento de <em>querys</em> con mayor rapidez, también hace que se presente una gran desventaja y es que al crecer la cantidad de documentos, junto con el vocabulario (palabras únicas contenidas dentro del corpus), se obtiene una matriz dispersa de una alta dimensionalidad que hace poco efectiva su implementación.</p>
<p>Igualmente, las deficiencias de este modelo también reacaen en que los resultados que se obtienen ante una búsqueda no tienen representado ningún criterio de relevancia. Si por ejemplo, el término sobre el cual se realiza el <em>query</em> aparece 100 veces en un documento y en otro solo aparece una vez, en la presentación de los resultados ambos documentos se mostrarán al mismo nivel, no pudiendo indicar la mayor relevancia que puede tener uno sobre el otro.</p>
<p>De la misma forma, también se tiene como otra de desventaja que en este modelo no se registra el contexto semántico de las palabras e incluso se pierde el orden en que aparecen los términos dentro de cada texto.</p>
</div>
<div id="invind" class="section level4 hasAnchor" number="3.2.3.2">
<h4><span class="header-section-number">3.2.3.2</span> Índices Invertidos<a href="teorico.html#invind" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Es ampliamente conocido el proceso de indexación como aquel en el cual se generan índices dentro de las bases de datos, sin embargo, en los sistemas de recuperación de información se genera otro tipo de índice que recibe el nombre de “índice invertido”. En él, en vez de guardar los nombres de los documentos junto con las palabras que aparecen, se procede a registrar en una lista cada palabra y a continuación se indican los nombres o se guardan apuntadores a los documentos en los cuales se encuentra la misma. Adicionalmente, también se puede registrar la posición en que aparece cada una de estas, relativo a algún criterio, como puede ser el inicio del documento o del párrafo. Igualmente se puede registrar la frecuencia con que se presenta cada término en el documento.</p>
<p>De esta manera, se tiene que cuando un SRI funciona mediante un índice invertido, esto conlleva a que se puedan realizar las denominadas “búsquedas de texto completa” (<em>full text search</em>) así como también a realizar las búsquedas de texto aproximado <em>(approximate text searching)</em>, donde se flexibiliza la coincidencia entre el texto requerido y el resultado.</p>
<p>No obstante, hay que tener presente que la creación del índice inverso conlleva a algunos costos computacionales, siendo el primero de estos el mayor espacio de almacenamiento que se consume al guardar estos datos adicionales, incrementando el tamaño del registro de un 5% al 100% del valor inicial, según las configuraciones que se decida adoptar al crearlo. El segundo costo a tener en cuenta está determinado por los cálculos informáticos que se deben hacer al momento de actualizar el índice, una vez que se incorporan nuevos documentos <span class="citation">(<a href="#ref-Mahapatra2011">Mahapatra y Biswas 2011</a>)</span>. En el capítulo <a href="desarrollo.html#desarrollo">5</a>, “Desarrollo de la Solución”, se indicará en cuánto se incremento el espacio de almacenamiento en disco con la generación de el índice inverso del SCSU.</p>
<p>La situación expuesta motiva a que existan diversos tipos de índices invertidos y a que constantemente se estén realizando investigaciones que permitan mejorar su desempeño, motivado a que sobre ellos recae, en gran parte, la efectividad que se puede obtener ejecutando los <em>querys</em>. Algunos ejemplos de estos índices son el <em>Generalized Inverted Index</em> (GIN), RUM <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> y VODKA <a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> y en el trabajo de <span class="citation">(<a href="#ref-Mahapatra2011">Mahapatra y Biswas 2011</a>)</span> se encuentran detalles adicionales sobre ellos.</p>
<p>En contraparte, el espacio de almacenamiento que ocupa la implementación de estos índices se puede ver reducido mediante el preprocesamiento que se haga a las palabras buscando las raices de ellas, siendo uno de los métodos usados la aplicación del <em>stemming</em>, el cual se expondrá más adelante o también mediante la remoción de las <em>stopwords</em>, que son las palabras que no aportan mayor valor semántico dentro de un texto, como pueden ser los términos: la, el, tu, ella, son, entre varios otros.</p>
<p>Siguiendo adelante en el tema, existen estrategias de implementación de un sistemas de recuperación de información donde se generan dos o más índices inversos, conteniendo, por ejemplo, uno de estos la lista de documentos y la frecuencia de la palabra, mientras que en el otro se registra la lista con las posiciones de la palabra.</p>
<p>Para finalizar lo referente a los índices invertidos, se tiene que cuando la base de datos en que se soporta el sistema de recuperación de información crece y no es viable almacenarla en un único computador, es necesario acudir al uso de algoritmos y tecnologías que permitan distribuir los datos en sistemas distribuidos y paralelos como pueden ser Spark, Hadoop o Apache Storm.</p>
</div>
</div>
<div id="relevancia" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Relevancia<a href="teorico.html#relevancia" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Refiere la medida en que un documento o recurso recuperado satisface las necesidades de información del usuario. En otras palabras, un documento es relevante si contiene información que es útil y está relacionada con el <em>query</em> realizado por el usuario <span class="citation">(<a href="#ref-büttcher2010a">Büttcher, Clarke, y Cormack 2010a</a>)</span>. La relevancia no es una propiedad intrínseca del documento, sino que depende del contexto y de las necesidades de información del usuario en un momento específico.</p>
<p>De esta manera, para establecer la relevancia que presente un documento sobre los otros, se usan distintos métodos, los cuales también han variado según las representaciones computacionales que se hagan de los textos. Bajo el modelo de recuperación boleano se puede dar un mayor peso a la aparición de la frase del <em>query</em> dentro del título de un texto o en las palabras clave. Otro método que se adopta es determinar la proximidad o cercanía entre las palabras contenidas en el texto recuperado, según la condición que establece el <em>query</em>. También se acude a realizar el cálculo de la frecuencia de aparición de una palabra, o varias, dentro del documento y su relación con el <em>query</em>, dando una mayor jerarquía a aquellos documentos que presenten una frecuencia de aparición mayor de los términos.</p>
<p>También se tiene que, otro método usado para establecer la relevancia, es determinar las referencias (citas), que contengan otros documentos a ese determinado escrito, similar a la propuesta del algoritmo <em>PageRank</em> <span class="citation">(<a href="#ref-brin1998">Brin y Page 1998</a>)</span>.</p>
<p>Recientemente, una de las propuestas adoptadas para establecer los criterios de relevancia es efectuar la comparación vectorial, detallada en <a href="teorico.html#similitud">3.3.3</a>, “Similitud de Documentos” , de los <em>embeddings</em>, por definir en <a href="teorico.html#embed">3.5.1</a>, “Embeddings”, que generan frases de un documento dentro del corpus con el <em>embedding</em> generado desde la frase del <em>query</em>.</p>
</div>
<div id="ranking" class="section level3 hasAnchor" number="3.2.5">
<h3><span class="header-section-number">3.2.5</span> Re Ordenamiento (re-ranking)<a href="teorico.html#ranking" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Es una técnica utilizada para mejorar la precisión y lograr extraer los documentos que tengan mayor relevancia en los resultados de una búsqueda.</p>
<p>Cuando los usuarios realizan el <em>query</em> a menudo se encuentran con una gran cantidad de documentos que coinciden con sus consultas, sin embargo, no todos estos documentos son igualmente relevantes para el usuario. El <em>re-ranking</em> implica reorganizar los resultados de búsqueda originales para que los documentos más relevantes aparezcan en las primeras posiciones, mejorando así la experiencia del usuario.</p>
<p>Cabe destacar que, en algunos casos el sistema de recuperación de información en la función de relevancia ejecuta el proceso de re ordenamiento mientras que en otros este proceso es ejecutado con un método distinto que puede estar basado en técnicas soportadas en aprendizaje automático.</p>
<div id="learning-to-rank-ltr" class="section level4 hasAnchor" number="3.2.5.1">
<h4><span class="header-section-number">3.2.5.1</span> Learning to Rank (LTR)<a href="teorico.html#learning-to-rank-ltr" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Los algoritmos de aprendizaje para la clasificación (LTR, por sus siglas en inglés) son comúnmente utilizados para el re-ranking. En ellos se utilizan técnicas de aprendizaje automático para modelar la relevancia de los documentos basándose en características específicas <span class="citation">(<a href="#ref-büttcher2010">Büttcher, Clarke, y Cormack 2010b</a>)</span>. Los atributos pueden incluir la frecuencia de palabras clave, la proximidad de términos en el documento y otros factores que indican la relevancia.</p>
<p>El siguiente aspecto a considerar es que los modelos LTR pueden ser entrenados con conjuntos de datos que contienen consultas y documentos etiquetados con su relevancia, y luego aplicados para re-ordenar los resultados de búsqueda en función de las características aprendidas.</p>
</div>
<div id="bm25" class="section level4 hasAnchor" number="3.2.5.2">
<h4><span class="header-section-number">3.2.5.2</span> BM25<a href="teorico.html#bm25" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Es un algoritmo que apareció a mediados de la década de 1990 y contiene una función matemática compleja de puntuación basada en un modelo probabilístico <span class="citation">(<a href="#ref-zhai2016">Zhai y Massung 2016</a>)</span> la cual es utilizada para calcular la relevancia de un documento con respecto a una consulta <span class="citation">(<a href="#ref-robertson2009">Robertson y Zaragoza 2009</a>)</span> determinando la frecuencia de aparición de los términos de la búsqueda junto con la longitud del documento. Ha demostrado ser efectivo en la práctica para clasificar documentos según la relevancia que estos presenten, llegando en algún momento a decirse que obtenía un rendimiento similar al de un humano experto al hacer el proceso que de jerarquización en el proceso de recuperación de información.</p>
</div>
</div>
<div id="evaluacion" class="section level3 hasAnchor" number="3.2.6">
<h3><span class="header-section-number">3.2.6</span> Medidas y Métodos de Evaluación de Desempeño de los SRI<a href="teorico.html#evaluacion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Las siguientes métricas son usadas en el campo de la recuperación de información para evaluar el desempeño de un SRI:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Exactitud (<em>accuracy</em>):</strong> mide la proporción de documentos relevantes recuperados por el sistema con respecto al total de documentos recuperados.</p></li>
<li><p><strong>Precisión <em>(precision)</em>:</strong> es la proporción de documentos relevantes recuperados por el sistema con respecto a todos los documentos recuperados. Cuanto mayor es la precisión, menos documentos irrelevantes se recuperan.</p></li>
<li><p><strong>Recuperación (<em>recall</em></strong> <strong>):</strong> es la proporción de documentos relevantes recuperados por el sistema con respecto a todos los documentos relevantes presentes en la base de datos. Un alto “recall” indica que el sistema encuentra la mayoría de los documentos relevantes.</p></li>
<li><p><strong><em>F1 Score</em>:</strong> es la media armónica de la precisión (<em>precision)</em> y la recuperación (<em>recall)</em>. Esta medida proporciona un equilibrio entre los resultados que aportan las dos medidas que tiene de insumo. Un <em>F1 Score</em> alto indica un buen equilibrio entre la precisión y la capacidad para encontrar todos los documentos relevantes.</p></li>
</ol>
<p>Una vez enunciados los conceptos de estas tres medidas es necesario determinar el proceso con que se puede determinar la “relevancia” de los documentos recuperados por un SRI. Para explicar esto se expondrá brevemente el origen de este método.</p>
<p>Posterior a la segunda guerra mundial se incrementó considerablemente la publicación de investigaciones en el ámbito científico y se hizo necesario contar con sistemas analógicos que fuesen eficientes para la indexación de los documentos. En el estudio denominado “<em>Cranfield Tests</em>”<span class="citation">(<a href="#ref-harman2011">Harman 2011</a>)</span>, que fue conducido por Cyril Cleverdon, a partir de 1958 se empezaron a definir los estándares para evaluar la efectividad de los índices disponibles para aquel momento.</p>
<p>Para ese entonces se definió la “relevancia” como lo que lo que actualmente se conoce como “exactitud” y la estrategia que se adoptó para poder determinarla fue usar el <em>“known-item searching”</em> (búsqueda del elemento conocido), que consistía en encontrar un documento que garantizara ser relevante ante una determinada pregunta. Para obtener la dupla “pregunta - nombre o identificación del documento con respuesta correcta” , acudieron a los autores de 1.500 trabajos y les pidieron que formulasen una pregunta que satisfactoriamente iba a ser respondida en el texto de su autoría <span class="citation">(<a href="#ref-harman2011">Harman 2011</a>)</span>.</p>
<p>Avanzando en el tiempo tenemos que desde inicios de la década de 1990, con las reuniones periódicas de la denominada “<em>Text Retrieval Evaluation Conference</em>-TREC”, se crean distintos conjuntos de datos con diversos documentos agrupados por temas donde expertos anotan con una expression binaria: “relevante” o “no relevante”, los juicios de relevancia para así poder indicar cuáles son los documentos más destacados para cada uno de los tópicos.</p>
<p>Es por esta razón que los conjuntos de datos constituidos por la dupla antes detallada, se les llama “<em>standard test collections</em>”, “<em>golden standard</em>” o “<em>ground truth judgment of relevance</em>” (juicio de pertinencia basado en la verdad). Contar con estas colecciones permitió diseñar un método para poder evaluar el desempeño de un sistema al ejecutar el proceso de generación de “relevancia”, comparando el criterio de los expertos con el obtenido desde el sistema.</p>
<p>No obstate, el problema que presenta el enfoque mencionado es que ante métodos de recuperación de documentos más avanzados, como la búsqueda semántica, la cual será presentada en <a href="teorico.html#busquedasemantica">3.5.5</a>, “Búsqueda Semántica”, así como con la aparición de temas de investigación más especializados y también ante el incremento de documentos digitales, este tipo de mediciones se queda un tanto rezagada y no muestra la real efectividad en los procesos de RI que pueden disponer los sistemas.</p>
<p>Igualmente es necesario señalar que, las medidas “<em>precisión</em>”y “<em>recall”</em> en algunos casos no llegan a reflejar la verdadera satisfacción del usuario, ya que el diseño de la interfaz del sistema afecta positiva o negativamente, lo que realmente debe ser la medición de la relevancia que dispone el SRI sometido a evaluación <span class="citation">(<a href="#ref-manning2008">C. D. Manning, Raghavan, y Schütze 2008</a>)</span>.</p>
</div>
</div>
<div id="PT" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Procesamientos a los textos<a href="teorico.html#PT" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>En esta sección se exponen diversos métodos que comúnmente se usan para procesar textos, teniendo presente que son estos el insumo con el cual se conforma el corpus anotado <span class="citation">(<a href="#ref-desagulier2017">Desagulier 2017</a>)</span> de cualquier SRI y que efectuar óptimas manipulaciones sobre los documentos determinará en gran medida la propia calidad del sistema que se obtenga.</p>
<p>Primero que todo, es necesario contextualizar que la aplicación de las técnicas que serán revisadas tienen absoluta dependencia del idioma usado a diferencia de procesamientos que se pueden hacer a otros tipos de datos. Las herramientas que se seleccionan van a analizar, categorizar y extraer información de las palabras y oraciones para poder obtener estructuras gramaticales y morfológicas, haciendo que estos recursos estén directamente asociados a la lengua que posean los textos.</p>
<p>En tal sentido, previo al año 2016 eran escasas las herramientas computacionales para la manipulación de documentos en el idioma español. Los <em>frameworks</em> disponibles para realizar las tareas de procesamiento, se sustentaron en la adopción de las bases que da el proyecto <em>“Universal Dependencies”</em> <span class="citation">(<a href="#ref-demarneffe2021">Marneffe et al. 2021</a>)</span>, tal es el caso del “coreNLP” de la Universidad de Stanford <span class="citation">(<a href="#ref-manning-etal-2014-stanford">C. Manning et al. 2014</a>)</span>, que fue uno de los primeros en incluir dos métodos para el procesamiento de los textos con su tokenizador y también con el separador de oraciones (<em>sentences splitting</em>), sin disponer de otras utilidades como la identificación de las parte del discurso (<em>part of speech tagging),</em> el análisis morfológico (<em>morphological analysis)</em> <span class="citation">(<a href="#ref-straka2017">Straka y Straková 2017</a>)</span> o el reconocimiento de entidades nombradas (<em>named entity recognigtion),</em> que sí se encontraban disponibles para el idioma inglés.</p>
<p>Sin embargo, un caso aparte para la época, es el esfuerzo de la Universidad Politécnica de Cataluña quienes crearon la herramienta <em>FreeLing</em> <a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>, la cual tuvo como entrada de datos para el entrenamiento del modelo el Corpus AnCora <a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> anotado por el “<em>CLiC- Centre de Llenguatge i Computación</em>”. Este software. soportado en un modelo de aprendizaje automático, fue uno de los primeros en poner a disposición de los usuarios de textos en español, la identificación de las parte del discurso y el análisis morfológico de las palabras.</p>
<p>Con el trancurrir del tiempo, este proyecto fue desplazado ya que otros <em>frameworks</em> con mejores integraciones de cadenas de trabajo (<em>pipelines</em>), así como el uso de otras arquitecturas de modelos de redes neuronales más potentes <span class="citation">(<a href="#ref-chen2014fast">Chen y Manning 2014b</a>)</span>, permitieron la aparición de diversas herramientas que sí dieron soporte a la lengua española hacia finales de la década del 2010. En la sección <a href="teorico.html#sota">3.5</a>, “Estado del Arte”, se revisaran algunos de estos avances.</p>
<div id="nlproc" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Procesamiento del Lenguaje Natural (Natural Language Processing- NLP)<a href="teorico.html#nlproc" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>El procesamiento del lenguaje natural (PNL) es el conjunto de técnicas computacionales desarrolladas para permitir al computador representar e interactuar de una forma más efectiva con los textos. La <em>tokenización</em>, el etiquetado de partes del discurso, el <em>stemming</em> y la <em>lematización</em> son algunos de los métodos que lo componen. Es necesario destacar que cada uno de los métodos que se detallan a continuación fueron aplicados sobre el corpus del SCSU.</p>
<div id="token" class="section level4 hasAnchor" number="3.3.1.1">
<h4><span class="header-section-number">3.3.1.1</span> Tokenizador<a href="teorico.html#token" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Básicamente un tokenizador es una herramienta que permite separar un documento en palabras, o unidades semánticas que tengan algún significado. Las unidades obtenidas se les llama <em>tokens</em> <span class="citation">(<a href="#ref-straka2017">Straka y Straková 2017</a>)</span>.</p>
<p>En complemento de lo anterior, realizar este procesamiento para el idioma español no representa un mayor reto, ya que generalmente se puede usar el espacio como delimitador de palabras, no así en otros idiomas como el chino donde el problema se aborda de manera distinta.</p>
<p>De esta forma, al obtener las palabras como entidades separadas de un texto se permite, por ejemplo, calcular la frecuencia de uso de las mismas dentro del corpus.</p>
<p>En este contexto, las librerías de procesamiento de lenguaje natural para el idioma español disponen de tokenizadores que comúnmente presentan un 100% de precisión en la ejecución de separar las palabras.</p>
<p>Igualmente hay que destacar que los tokenizadores que se usan para generar <em>embedding</em>s, ver <a href="teorico.html#embed">3.5.1</a>, “Embeddings”, tienen un comportamiento distinto al hacer la separación de las unidades que conforman el texto basándose en reglas.</p>
</div>
<div id="pos" class="section level4 hasAnchor" number="3.3.1.2">
<h4><span class="header-section-number">3.3.1.2</span> Etiquetado de Partes del Discurso <em>(Part of speech tagging-POS)</em><a href="teorico.html#pos" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Consiste en asignar un rol sintáctico a cada palabra dentro de una frase <span class="citation">(<a href="#ref-eisenstein2019">Eisenstein 2019</a>)</span>, siendo necesario para ello evaluar cómo cada palabra se relaciona con las otras que están contenidas en una oración y así se revela la estructura sintáctica.</p>
<p>En este sentido, los roles sintácticos principales de interés en la elaboración de esta investigación son los sustantivos, adjetivos y verbos. Al recordar muy brevemente cuáles son estos roles se tiene que:</p>
<ul>
<li><p>Los sustantivos tienden a describir entidades y conceptos.</p></li>
<li><p>Los verbos generalmente señalan eventos y acciones.</p></li>
<li><p>Los adjetivos describen propiedades de las entidades.</p></li>
</ul>
<p>Igualmente, dentro del POS se identifican otros roles sintácticos como los adverbios, nombres propios, interjecciones, por solo mencionar algunos.</p>
<p>En específico, en esta investigación la ejecución del POS permite que se obtenga una tabla que sirve de insumo para determinar la coocurrencia de palabras, que es una de las formas en que se representan los resultados de los <em>querys</em> en el sistema desarrollado.</p>
<p>Se destaca que en el estado del arte, este proceso de etiquetado para el idioma español alcanza un 98% de precisión.</p>
</div>
<div id="steaming" class="section level4 hasAnchor" number="3.3.1.3">
<h4><span class="header-section-number">3.3.1.3</span> Stemming<a href="teorico.html#steaming" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>El <em>Stemming</em> es un algoritmo que persigue encontrar la raíz de una palabra, teniendo como el de mayor uso el Algoritmo de Porter <span class="citation">(<a href="#ref-willett2006">Willett 2006</a>)</span>. Al ser usado se puede reducir considerablemente el número de palabras que conforman el vocabulario del corpus y así consecuentemente mejorar los tiempos en que se ejecuta la búsqueda de un texto, ya que se disminuye el espacio de búsqueda.</p>

<p>Sin embargo, se hace la consideración de que, la aplicación de este tipo de algoritmos no toma en consideración el contexto en el que aparece la palabra a la que se le extrae la raíz. Como ejemplo se muestra que “yo canto, tú cantas, ella canta, nosotros cantamos, ellos cantan” en todos los casos se tendrá como raíz la cadena de letras “cant”.</p>
<p>Finalmente, es necesario tener presente que al crear el índice invertido son las raíces de las palabras las que se guardarán y no propiamente la palabra que aparece en el texto.</p>
</div>
<div id="lemma" class="section level4 hasAnchor" number="3.3.1.4">
<h4><span class="header-section-number">3.3.1.4</span> Lematización<a href="teorico.html#lemma" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Es el proceso en que se consigue el <em>lema</em> de una palabra, entendiendo que el <em>lema</em> es la forma que por convenio se acepta como representante de todas las formas flexionadas de una misma palabra <span class="citation">(<a href="#ref-demarneffe2021">Marneffe et al. 2021</a>)</span>. Los lemas, o lexemas, constituyen la parte principal de la palabra, la que transmite el significado. Los morfemas son el elemento variable de la palabra y son los que se busca desechar en el proceso de lematización.</p>
<p>En este sentido, al buscar el <em>lema</em> se tiene presente la función sintáctica que tiene la palabra, es decir que se evalúa el contexto en el que ocurre. Una de las ventajas de aplicar esta técnica es que se reduce el vocabulario del corpus y eso conlleva a que también se reduzca el espacio de búsqueda.</p>
<p>Un ejemplo de lematización se puede representar con estas tres palabras: “bailaré, bailamos, bailando” que tienen el mismo <em>lema</em> que es “bailar”.</p>
<p>Una consideración final sobre este tema es que, en el estado del arte este proceso alcanza un 96% de precisión en varios de los modelos de aprendizaje automático preentrenados, no obstante no se disponen datos puntuales de esta métrica para el idioma español.</p>
</div>
</div>
<div id="textmin" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Minería de Texto<a href="teorico.html#textmin" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La extracción de ideas útiles derivadas de textos mediante la aplicación de algoritmos estadísticos y computacionales se conoce con el nombre de minería de texto (<em>text mining)</em>, analítica de texto (<em>text analytics)</em> o aprendizaje automático para textos <em>(machine learning from text</em>). Se quiere con ella representar el conocimiento en una forma más abstracta y así poder detectar relaciones y patrones en los textos <span class="citation">(<a href="#ref-aggarwal2018a">Aggarwal 2018a</a>)</span>.</p>
<p>De esta manera se tiene que, la minería de texto surge para dar respuesta a la necesidad de tener métodos y algoritmos que permitan procesar estos datos no estructurados <span class="citation">(<a href="#ref-miningt2012">Aggarwal y Zhai 2012</a>)</span> y ella ha ganado atención en recientes años motivado a las grandes cantidades de textos digitales que están disponibles. Los procesamientos inherentes al NLP mencionados anteriormente son insumo para la minería de texto.</p>
<p>A continuación se procede a exponer algunos de los métodos que pertenecen a la minería de texto:</p>
<div id="tdm" class="section level4 hasAnchor" number="3.3.2.1">
<h4><span class="header-section-number">3.3.2.1</span> Term-Document Matrix<a href="teorico.html#tdm" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Una vez que se tiene conformado un corpus, se procede a conformar una matriz dispersa de una alta dimensionalidad que se denominará <em>“Sparce Term-Document Matrix)”</em> de tamaño <em>n X d,</em> donde <em>n</em> es el número total de documentos y <em>d</em> es la cantidad de términos o vocabulario (palabras distintas) presentes entre todos los documentos. Formalmente se sabe que la entrada <em>(i,j)</em> de nuestra matriz es la frecuencia (cantidad de veces que aparece) de la palabra <em>j</em> en el documento <em>i</em>. Este procedimiento es similar al que fue revisado en <a href="teorico.html#MRIbol">3.2.3.1</a>, “Recuperación Boleana”.</p>
<p>Cabe destacar, que uno de los problemas que presenta la matriz obtenida es la alta dimensionalidad y lo dispersa que es, llegando a estar conformada en un 98% por ceros, los cuales indican la ausencia de aparición de una palabra en un determinado documento.</p>
<p>Sin embargo, para mejorar un tanto este tipo de representación del corpus, se aplican otras técnicas que en principio puedan colaborar a reducir la dimensionalidad, por medio de simplificar los atributos, es decir, disminuyendo el vocabulario aplicando el stemming como se vio anteriormente.</p>
</div>
<div id="coocurrencia" class="section level4 hasAnchor" number="3.3.2.2">
<h4><span class="header-section-number">3.3.2.2</span> Coocurrencia de Palabras<a href="teorico.html#coocurrencia" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>En esta investigación se usará un método denominado “coocurrencia de palabras” para la detección de patrones en los textos y se hará la representación de aparición de las coocurrencias mediante grafos. El método se explica en que se evalúan las palabras que coocurren, es decir, aquellas que forman parte del conjunto de palabras obtenidas de la intersección de los documentos que conforman el corpus<em>,</em> o del subconjunto de documentos recuperados mediante un determinado <em>query</em>.</p>
<p>Adicionalmente, se tiene que también se puede establecer el nivel al que se quiere determinar la coocurrencia, por ejemplo, las palabras que coocurren una seguida de otra en los textos, las que coocurren dentro de la misma oración, dentro de un párrafo o dentro de todo el texto de cada documento.</p>
<p>Por otra parte, para la representación de las coocurrencias se usan grafos, donde cada palabra se representa un nodo y la coocurrencia de una palabra con otra implica que se extienda un arco entre ellas. Las palabras dispuestas para representarse en el grafo serán exclusivamente las que tengan la función dentro del discurso (POS) de adjetivos y sustantivos, es decir que cada coocurrencia será un sustantivo con el adjetivo que la acompaña, donde es posible tener una relación de 1 sustantivo con un conjunto de {0,1,…,n} adjetivos. Para lograr esto, la selección de las funciones gramaticales propuestas se hace para disminuir el espacio de representación y se considera que los sustantivos, al contar con el adjetivo que las acompaña, logran hacer una representación que muestra proximidad semántica y se representan los tópicos más relevantes <span class="citation">(<a href="#ref-segev2021">Segev 2021</a>)</span>.</p>
<p>En la figura <a href="teorico.html#fig:coocejem">3.2</a> se visualiza lo expuesto de una manera gráfica, al ver la representación en un grafo la coocurrencia de palabras sobre los textos de los resúmenes de las tesis y trabajos especiales de grado de la Escuela de Física de la U.C.V.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:coocejem"></span>
<img src="images/03-marco-teorico/cooc.png" alt="Coocurrencia de palabras" width="90%" />
<p class="caption">
Figura 3.2: Coocurrencia de palabras
</p>
</div>
<div id="mapacon" class="section level5 hasAnchor" number="3.3.2.2.1">
<h5><span class="header-section-number">3.3.2.2.1</span> Mapas de Conocimiento<a href="teorico.html#mapacon" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>La representación gráfica y el método de extracción de sustantivos y adjetivos, resulta similar a la propuesta metodológica realizada por <span class="citation">(<a href="#ref-dueñas2011">Dueñas, Rojas, y Morales 2011</a>)</span> para crear “mapas de conocimiento” con “las palabras claves obtenidas a través de búsquedas recurrentes y relacionadas”. En esta investigación se simplificará la obtención y representación de estos mapas, asumiendo que las palabras clave son los sustantivos adjetivizados, equivalente a visualizar las personas, cosas o ideas que se mencionan y que son modificados por los adjetivos, al cambiar sus propiedades o atributos; seleccionando aquellas palabras que muestran una mayor aparición en el <em>query</em> realizado y que se interconectan en un grafo mediante arcos.</p>
</div>
</div>
</div>
<div id="similitud" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Similitud de Documentos<a href="teorico.html#similitud" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Para poder realizar la recomendación de documentos, una de las técnicas que se usa es medir la similitud que presenta un documento con los otros contenidos en el corpus <span class="citation">(<a href="#ref-aggarwal2018a">Aggarwal 2018a</a>)</span> . Un ejemplo de esta técnica es el uso de la similitud coseno que se explica con esta fórmula.</p>
<span class="math display">\[\begin{equation}
\cos ({\bf t},{\bf e})= {{\bf t} {\bf e} \over \|{\bf t}\| \|{\bf e}\|} = \frac{ \sum_{i=1}^{n}{{\bf t}_i{\bf e}_i} }{ \sqrt{\sum_{i=1}^{n}{({\bf t}_i)^2}} \sqrt{\sum_{i=1}^{n}{({\bf e}_i)^2}} }
\end{equation}\]</span>
<p><br></p>
<p>En este sentido, en la fórmula, <em>t</em> representa un documento y <em>e</em> representa otro documento. Ambos documentos se asumen que están en un espacio con <em>i</em> atributos, o dimensiones, y la intención es calcular un índice de similitud entre ambos documentos.</p>
<p>En este orden de ideas se tiene que este es uno de los métodos más usados para detectar similitudes en los textos, aunque existen otras fórmulas para el cálculo de la similitud como lo es el índice de Jaccard.</p>
<p>Adicionalmente se tiene que, hacer la comparación de un documento <em>i</em> del corpus que contiene <em>n</em> documentos, en un proceso iterativo con otra cantidad de (<em>n-1)</em> documentos, de se se obtienen (<em>n-</em>1) índices de similitud. Aquel que obtenga un mayor valor se puede inferir que presenta una mayor similitud con el documento <em>i.</em></p>
<p>En el mismo orden de ideas se tiene que, otro elemento de gran importancia a evaluar en el resultado que se obtenga de esta medición, es la representación computacional que se haga del documento. Son distintas las técnicas que existen, estando entre ellas la representación mediante “bolsas de palabras” o <em>bag of words,</em> similar a lo que se explicó en <a href="teorico.html#tdm">3.3.2.1</a>, “<em>Term Document Matrix</em>”, donde un documento <em>i</em> es el vector correspondiente a una fila de la matriz y la cantidad de dimensiones que presenta es equivalente al tamaño del vocabulario.</p>
<p>Un elemento importante a destacar de realizar este tipo de comparaciones, la estimación de la similitud, es que ante un proceso de <em>query</em> también pueden ser recuperados, o sugerir al investigador, aquellos documentos que presenten alguna mínima similitud con los documentos recuperados.</p>
<p>Finalmente, se procede a resaltar que recientemente se han creado formas más complejas para la representación de los documentos, como lo son los <em>word embeddings</em> que son obtenidos mediante el entrenamiento de redes neuronales de aprendizaje profundo, lo que será expuesto en <a href="teorico.html#embed">3.5.1</a>,“Embeddings”.</p>
</div>
</div>
<div id="SD" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Sistemas Distribuidos<a href="teorico.html#SD" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Los distintos procesos y componentes de la solución propuesta han sido diseñados e implementados como un sistema distribuido y por eso se hace la mención a este tema.</p>
<p>En tal sentido, una definición formal que se le puede dar a los sistemas distribuidos es “cuando los componentes de hardware y/o sofware se encuentran localizados en una red de computadores y estos coordinan sus acciones solo mediante el pase de mensajes” <span class="citation">(<a href="#ref-distribu2012">Coulouris 2012</a>)</span>.</p>
<p>De acuerdo a lo anterior, se tiene que algunas de las principales características que poseen los sistemas distribuidos es la tolerancia a fallos, compartir recursos, concurrencia, ser escalables <span class="citation">(<a href="#ref-czaja2018">Czaja 2018</a>)</span> entre otras. Mencionamos estas, en particular, al ser propiedades que están presentes en la solución que se implementa:</p>
<ol style="list-style-type: decimal">
<li><p>Fiabilidad (tolerancia a fallos): al fallar un componente del sistema los otros se deben mantener en funcionamiento.</p></li>
<li><p>Compartir recursos: un conjunto de usuarios pueden compartir recursos como archivos o base de datos.</p></li>
<li><p>Concurrencia: poder ejecutar varios trabajos en simultáneo.</p></li>
<li><p>Escalable: al ser incrementada la escala del sistema se debe mantener en funcionamiento el sistema sin mayores contratiempos.</p></li>
</ol>
<div id="contenedores" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Contenedores<a href="teorico.html#contenedores" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Un contenedor es una abstracción de una aplicación que se crea en un ambiente virtual, en el cual se encuentran “empaquetados” todos los componentes (sistema operativo, librerías, dependencias, etc.), que una aplicación necesita para poder ejecutarse. En su diseño se tiene presente que sean ligeros y que con otros contenedores pueden compartir el <em>kernel</em>, usando un sistema de múltiples capas, que también pueden ser compartidas entre diversos contenedores, ahorrando espacio en disco del <em>host</em> donde se alojan los contenedores <span class="citation">(<a href="#ref-nüst2020">Nüst et al. 2020</a>)</span>.</p>
<p>De lo indicado en el punto anterior se tiene que, el uso de los contenedores permite crear, distribuir y colocar en producción aplicaciones de software de una forma sencilla, segura y reproducible. También a cada contenedor se le puede realizar una asignación de recursos (memoria, CPU, almacenamiento) que garantice un óptimo funcionamiento de la aplicación que contienen. Es importante señalar que, el uso de esta tecnología también añade un entorno de seguridad al estar cada contenedor en una ambiente isolado.</p>
<p>Igualemente hay que resaltar que, para instanciar cada contenedor es necesario disponer de una imagen donde previamente se definen las dependencias (sistema operativo, librerías, lenguajes) necesarias para su funcionamiento.</p>
</div>
<div id="orquestador" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Orquestadores<a href="teorico.html#orquestador" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Cuando se tienen diversos contenedores para sustentar un sistema, donde cada uno aloja una aplicación o servicio distinto, puede resultar necesario que todos se integren y compartan recursos optimizadamente. Para que esta integración sea viable es necesario contar con un orquestador <span class="citation">(<a href="#ref-cook2017">Cook 2017</a>)</span>. Su uso permitirá lograr altos grados de portabilidad y reproducibilidad, pudiendo colocarlos en la nube o en centros de datos, garantizando que se pueda hacer el <em>deploy</em> de forma sencilla y fiel a lo que se implementó en el ambiente de desarrollo.</p>
<p>Así mismo, en el caso de la solución propuesta se adoptará el uso de <em>Docker Compose</em> como orquestador y en el capítulo <a href="desarrollo.html#desarrollociclos4">5.3.3</a>, “Ciclos de Desarrollo”, serán expuestas las funcionalidades de cada contenedor y del orquestador.</p>
</div>
</div>
<div id="sota" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Estado del Arte<a href="teorico.html#sota" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Si bien anteriormente las búsquedas de información dentro de un corpus se procesaban determinando la aparición de palabras dentro de un texto, este método ha ido evolucionando, pasando de tener motores de búsqueda (<em>search engines</em> ) a los denominados motores de respuestas ( <em>answering engines</em> ) <span class="citation">(<a href="#ref-balog2018">Balog 2018</a>)</span>, donde el sistema ante una determinada consulta va a retornar una serie de resultados enriquecidos, mostrando la identificación de entidades, hechos y cualquier otro dato estructurado que esté de forma explícita e implícita, mencionado dentro de los textos que conforman el corpus.</p>
<p>Es por esto que, para hablar sobre el estado del arte tanto en los sistemas de recuperación de información, así como en el procesamiento del lenguaje natural y en la medición de similitud entre documentos, es necesario referir la representación de los textos mediante <em>embeddings,</em> los cuales serán expuestos a continuación.</p>
<div id="embed" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Embeddings<a href="teorico.html#embed" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A partir del siguiente ejemplo que contiene dos frases se va a plantear el problema que motiva la necesidad de contar con modelos distintos al de índice invertido para hacer la representación de los textos computacionalmente:</p>
<p>Frase 1: el modelo de banco de tres asientos está en oferta.</p>
<p>Frase 2: voy a depositar dinero al banco.</p>
<p>En este ejemplo claramente se distingue que el uso de la palabra “banco” tiene significados distintos. Es por esto que el lingüista Firth J.R. enfrentó el problema planteado con el siguiente postulado: “entenderás el significado de una palabra analizando aquellas que la acompañan”, donde queda patente que para comprender el concepto de un término hay que revisar el contexto en el que ocurre y no verlo como una unidad independiente del documento que lo contiene.</p>
<p>Continuando con la exposición del problema, si se tuviese un sistema de recuperación de información sustentado en el modelo de índice invertido, con las frases del ejemplo registradas en él, ante el <em>query</em> “banco con asientos”, posiblemente se obtendrían como resultado todos los documentos donde aparezca la palabra “banco”, tanto el que contiene “el modelo de banco de tres asientos está en oferta”, como el otro que indica “voy a depositar dinero al banco”, sin tener en consideración la semántica de la palabra “banco” dentro del texto del <em>query,</em> que en este caso se corresponde a la frase 1, que es “asientos, con respaldo o sin él…”. Con base a lo anterior, sabemos que la situación ideal para un usuario sería que el SRI recuperase exclusivamente el documento que contiene la frase “el modelo de banco de tres asientos está en oferta”, o incluso yendo más lejos, si el <em>query</em> fuese “fabrica de sillas”, también se aspiraría que el sistema arrojase como resultado el documento con el texto “el modelo de banco de tres asientos está en oferta”, ya que asiento, banco y silla pueden ocurrir en contextos semánticos similares. Lo antes expuesto conlleva a ver las limitaciones que muestra el modelo de índice invertido y plantea la necesidad de contar con modelos donde se tengan representaciones de los datos con una estructura en la que pueda quedar plasmado el significado de las palabras y se puedan mejorar los procesos de IR.</p>
<p>De esta forma se tiene que, son los <em>embeddings</em> la representación, mediante vectores de las palabras y frases, que hoy constituye el estado del arte en los procesos de recuperación de información al lograr que sea posible aplicar distintos métodos algebraicos y computacionales para inspeccionar el vocabulario de un determinado corpus y tener nociones más precisas sobre la cercanía de una palabra con otra e igualmente poder hacer mediciones de similitud entre un documento, que se ha transformado en partes o en su totalidad en <em>embeddings</em>, con otro que tenga el mismo tipo de representación. </p>
<p>Al respecto, para comprender qué son los <em>embeddings</em> se debe partir de estudiar la hipótesis distribucional, la cual se enmarca en al área de la lingüística y enuncia que la similaridad en significados resulta en que también se presente una similaridad en la distribución lingüística. Dos palabras que sean próximas en significado, entendido como que sean intercambiables en un texto, es un fenómeno que también se detectará en la distribución que presentan dichas palabras dentro de un corpus. Con base a lo anterior, de esta hipótesis surge la propuesta de crear la “distribución semántica”, donde se representa el significado de una palabra mediante el proceso en que se toma como entrada grandes cantidades de texto y se construye un modelo de distribución, también llamado “espacio semántico”, que logra extraer la representación semántica de todo el vocabulario en un espacio <em>n</em>-dimensional, haciendo que cada palabra se muestre como un vector con una representación numérica densa.</p>
<p>En el siguiente ejemplo <a href="teorico.html#tab:tblembedding">3.1</a> que se obtiene del trabajo <em>Distributional Semantics and Linguistic Theory</em> <span class="citation">(<a href="#ref-boleda2020">Boleda 2020</a>)</span>, se muestra una versión simplificada de un espacio semántico de dos dimensiones donde están los vectores que se corresponden con tres palabras que son <em>postdoc</em> (post doctorado)<em>, estudent</em> (estudiante) y <em>wealth (riqueza)</em>:</p>
<table>
<caption>
<span id="tab:tblembedding">Cuadro 3.1: </span>Embedding bidimensional para representar palabras
</caption>
<thead>
<tr>
<th style="text-align:left;">
Palabra
</th>
<th style="text-align:right;">
Dimensión.1
</th>
<th style="text-align:right;">
Dimensión.2
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
postdoc
</td>
<td style="text-align:right;">
0.71038
</td>
<td style="text-align:right;">
1.76058
</td>
</tr>
<tr>
<td style="text-align:left;">
estudent
</td>
<td style="text-align:right;">
0.43679
</td>
<td style="text-align:right;">
1.93841
</td>
</tr>
<tr>
<td style="text-align:left;">
wealth
</td>
<td style="text-align:right;">
1.77337
</td>
<td style="text-align:right;">
0.00012
</td>
</tr>
</tbody>
</table>
<p>De acuerdo a lo anterior, al representarse cada palabra con un vector de dos componentes, se puede hacer la gráfica en un plano, como se aprecia en la figura <a href="teorico.html#fig:embeddingimg">3.3</a>, donde al aplicar la medición de similitud coseno, revisada en <a href="teorico.html#similitud">3.3.3</a>, “Similitud de Documentos”, se determina que las palabras <em>postdoc</em> y <em>student</em> se encuentran más próximas y presentan una mayor similitud que <em>postdoc</em> y <em>wealth</em>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:embeddingimg"></span>
<img src="images/03-marco-teorico/word_vec.png" alt="Representación de palabras en un plano" width="55%" />
<p class="caption">
Figura 3.3: Representación de palabras en un plano
</p>
</div>
<p>De esta manera se tiene que, al generar la representación completa de un espacio semántico, haciendo la búsqueda de una palabra podemos encontrar también aquellas que son cercanas y no limitar la búsqueda al <em>match</em> que los modelos anteriormente estudiados sí imponían. Más adelante también veremos que el modelo semántico puede ser expandido y representar mediante un <em>embedding</em> oraciones (<em>sentences</em>), siendo esto el sustento que permite que al hacer una pregunta o un <em>query</em> a un sistema de recuperación de información, este sea capaz de encontrar la respuesta dentro del corpus, ya que la pregunta o <em>query</em> se transforma en un <em>embedding</em> y luego se determina en el espacio semántico cuál es la oración que más se aproxima y guarda algún tipo de proximidad o relación de distancia vectorial con la pregunta formulada. </p>
<p>Es importante mencionar que, han sido diversos los algoritmos implementados para crear los <em>embeddings</em>, evolucionando para lograr que las representaciones de los textos resulten de mayor provecho y rendimiento. A manera ilustrativa, para comprender una parte del funcionamiento de los <em>embeddings,</em> se va a usar el modelo “<em>GloVe: Global Vectors for Word Representation</em>” <span class="citation">(<a href="#ref-pennington2014">Pennington, Socher, y Manning 2014</a>)</span> en el cual mediante un vector de 100 componentes, siendo cada uno de estos un número real de ocho o más decimales, se logra hacer la representación de una palabra. A continuación se muestran los componentes del vector obtenido para el vocablo <em>king</em>, simplificado en este caso a 50 componentes principales:</p>
<p>“0.50451, 0.68607, -0.59517, -0.022801, 0.60046, -0.13498, -0.08813, 0.47377, -0.61798, -0.31012, -0.076666, 1.493, -0.034189, -0.98173, 0.68229, 0.81722, -0.51874, -0.31503, -0.55809, 0.66421, 0.1961, -0.13495, -0.11476, -0.30344, 0.41177, -2.223, -1.0756, -1.0783, -0.34354, 0.33505, 1.9927, -0.04234, -0.64319, 0.71125, 0.49159, 0.16754, 0.34344, -0.25663, -0.8523, 0.1661, 0.40102, 1.1685, -1.0137, -0.21585, -0.15155, 0.78321, -0.91241, -1.6106, -0.64426, -0.51042”</p>
<p>No obstante, ya que estos números dificultan la comprensión intuitiva, con la finalidad de facilitar el análisis, el vector revisado se procede a representarlo gráficamente en la figura <a href="teorico.html#fig:embking">3.4</a>, mapeando los componentes a una paleta de colores.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:embking"></span>
<img src="images/03-marco-teorico/embking.png" alt="Representación de palabra king mediante el modelo GloVe" width="95%" />
<p class="caption">
Figura 3.4: Representación de palabra king mediante el modelo GloVe
</p>
</div>
<p>Igualmente, usando el mismo modelo y método de visualización, una representación de las palabras <em>king</em> (rey), <em>man</em> (hombre) y <em>woman</em> (mujer) es la que se observa en la imagen <a href="teorico.html#fig:GloVeEmbedd">3.5</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GloVeEmbedd"></span>
<img src="images/03-marco-teorico/embedding.png" alt="Representación de palabras mediante el modelo GloVe" width="95%" />
<p class="caption">
Figura 3.5: Representación de palabras mediante el modelo GloVe
</p>
</div>
<p>En tal sentido se tiene que, la representación muestra gráficamente que las palabras <em>man</em> y <em>woman</em> son más “parecidas” visualmente que <em>king</em> y <em>man</em>, idea que se puede generalizar para entender cómo las distintas palabras que se llevan aun espacio semántico pueden presentar proximidades, similitudes o diferencias, obteniendo de esta forma un significado semántico según la posición relativa que cada una tenga con respecto a la otra en el espacio. El crédito a la visualización que se muestra en <a href="teorico.html#fig:GloVeEmbedd">3.5</a> corresponde al divulgador Jay Alammar <span class="citation">(<a href="#ref-wordtovec">Alammar 2023</a>)</span>.</p>
<p>No obstante, en los puntos expuestos aún no se ha explicado cómo se generan los <em>embeddings</em>, lo cual es indispensable para entender sus capacidades. Retrotrayéndonos al año 2003, se hizo una investigación en la cual se obtuvo que mediante el entrenamiento de redes neuronales de aprendizaje profundo se logrará modelar la probabilidad y así predecir, las secuencias de palabras dentro de los textos, demostrando la capacidad de estas redes para capturar patrones complejos en datos textuales <span class="citation">(<a href="#ref-Bengio:2003:NPL:944919.944966">Bengio et al. 2003</a>)</span> . Así mismo, también se tiene como otro hito la investigación “<em>Semantic Hashing</em>” <span class="citation">(<a href="#ref-salakhutdinov2009">Salakhutdinov y Hinton 2009</a>)</span> donde usaron redes neuronales para transformar datos de alta dimensionalidad en representaciones binarias de baja dimensionalidad. Esa investigación también añadió como un aporte el que se empezarán a usar técnicas de aprendizaje no supervisado para entrenar las redes neuronales, deshaciéndose de los cuellos de botella que previamente introducían los procesos de etiquetado, necesarios en métodos supervisados.</p>
<p>Otro punto a resaltar fue lo que significó la ampliación de capacidades de computo en sistemas distribuidos compuestos por tarjetas gráficas (<em>Graphics Processing Unit</em> - GPU) a inicios de 2010, ya que este tipo de procesadores facilitan los cálculos de las redes neuronales, con lo cual se empezó a incrementarse el entrenamiento y uso de estas redes para ese momento. También fue para aquel entonces, cuando se publica la investigación <em>“Word2Vec: Efficient Estimation of Word Representations in Vector Space</em>” <span class="citation">(<a href="#ref-mikolov2013">Mikolov, Chen, et al. 2013</a>)</span> que implementa la técnica <em>Skip-gram</em> y <em>Continuous Bag of Words (CBOW)</em> que permitieron a las redes neuronales el aprendizaje de representaciones semánticas de palabras a partir de grandes volúmenes de texto. <em>Word2Vec</em> no solo demostró ser eficiente computacionalmente, sino que también producía <em>embeddings</em> que capturaban relaciones semánticas y sintácticas transformando cómo se abordaban las tareas de NLP y las aplicaciones de recuperación de información.</p>
<p>Como consecuencia, una vez que se empezó a tener un método para capturar la semántica de las palabras, debió seguir el paso de lograr representar el sentido semántico de frases (<em>sentences</em> en inglés) y expresiones más complejas. Lo anterior se logró con la investigación “<em>Distributed Representations of Words and Phrases and their Compositionality</em>” (2013) <span class="citation">(<a href="#ref-mikolov2013a">Mikolov, Sutskever, et al. 2013</a>)</span> donde se hicieron representaciones vectoriales distribuidas para frases, mejorando la capacidad de capturar significados contextuales y relaciones sintácticas en un nivel más alto, lo cual resultó crucial para mejorar los sistemas de recuperación de información y también para la traducción automática.</p>
<p>Por otra parte, la investigación <em>GloVe</em> <span class="citation">(<a href="#ref-pennington2014">Pennington, Socher, y Manning 2014</a>)</span>, usada en un punto previo para hacer la representación gráfica de tres palabras, también motivó grandes avances al superar limitaciones que presentaban investigaciones anteriores al permitir generar analogías del tipo: (vector de <em>embedding</em> para la palabra “rey”) (menos -) (vector de embedding para la palabra “hombre”) (más +) (embedding para la palabra “mujer”), (es igual=) o muy aproximado en el espacio semántico, al (vector de embedding de la palabra “reina”) o simplificado como “rey-hombre+mujer = reina”.</p>
<p>Igualmente, con esa investigación se intensificó el uso de esos modelos en tareas de clasificación de texto, como las revisadas en <a href="teorico.html#nlproc">3.3.1</a>,“Procesamiento del Lenguaje Natural”, ya que las redes neuronales empezaron a entrenarse con representaciones de vectores de gran densidad que contenían las palabras, el POS y el etiquetado de las dependencias, conteniendo cada vector 200 componentes y de esta manera se alcanzaron mejores indicadores de desempeño en el etiquetado <span class="citation">(<a href="#ref-chen2014">Chen y Manning 2014a</a>)</span>. Cabe destacar que el trabajo citado fue el que dio soporte a la librería revisada anteriormente de nombre “coreNLP” de la Universidad de Stanford.</p>
</div>
<div id="trans" class="section level3 hasAnchor" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Arquitectura de Redes Neuronales <em>Transformers</em><a href="teorico.html#trans" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>En el año 2017 se publica “<em>Attention Is All You Need</em>” <span class="citation">(<a href="#ref-vaswani2017">Vaswani et al. 2017</a>)</span> el cual fue una investigación donde se introdujo una nueva arquitectura de redes neuronales que eliminó ciertas limitaciones que venían presentando los modelos de redes neuronales recurrentes y las convolucionales en poder trabajar con largas cadenas de texto. La solución introdujo los llamados “mecanismos de atención” <a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> que abrieron el camino para la creación de nuevos modelos de lenguaje como BERT <span class="citation">(<a href="#ref-devlin2018">Devlin et al. 2018</a>)</span> que capturaban la riqueza de significados y las relaciones complejas del lenguaje mejorando la comprensión de textos, traducción automática y la generación de texto.</p>
<p>Igualmente destacó el trabajo “<em>RoBERTa: A Robustly Optimized BERT Pretraining Approach</em>” <span class="citation">(<a href="#ref-liu2019">Liu et al. 2019</a>)</span> en el que se optimizó el entrenamiento preexistente de <em>BERT</em> al desvincular la tarea de pre-entrenamiento del tamaño de la cantidad de ejemplos de entrenamiento (<em>batch size</em>) y la duración del entrenamiento. Al escalar el tamaño del lote y la cantidad de datos, <em>RoBERTa</em> mejoró la comprensión del modelo sobre el lenguaje, logrando una capacidad de generalización excepcional.</p>
<p>Como consecuencia, estos métodos que venían innovando e incrementando las capacidades, por otra parte también hacían que el tamaño de los conjuntos de datos usados para el entrenamiento fuese creciendo exponencialmente, como se analizará en la sección <a href="teorico.html#LLM">3.5.3</a>, “Largos Modelos de Lenguaje”.</p>
<p>En este mismo orden de ideas se tiene que, otro modelo basado en la arquitectura <em>Transformers</em> que es necesario referir, ya que a un componente del SCSU le da soporte, es el que se publicó bajo el título “<em>Sentence-BERT: Sentence Embedding</em>s” <span class="citation">(<a href="#ref-reimers2019a">Reimers y Gurevych 2019</a>)</span> que a diferencia de los modelos anteriormente expuestos, que trabajaban con la codificación de palabras, en él se logra la codificación de oraciones usando una variante de <em>BERT</em> <span class="citation">(<a href="#ref-devlin2018">Devlin et al. 2018</a>)</span> permitiendo entender la similitud semántica entre pares de estas.</p>
<p>Sin embargo, un punto que fue necesario resolver para masificar la adopción de estos modelos era lograr contar con representaciones de <em>embeddings</em> para distintos idiomas, ya que inicialmente solo estaban entrenados con textos en idioma inglés impidiendo que fueran de utilidad para otras lenguas. En este sentido, una de las investigaciones que permitió avanzar hacia modelos multilingües fue “<em>Making Monolingual Sentence Embeddings Multilingual</em>” <span class="citation">(<a href="#ref-reimers2020">Reimers y Gurevych 2020</a>)</span>, usando la técnica “<em>Knowledge Distillation</em>”, la cual permitió que en lugar de entrenar modelos para cada idioma, solo se utilice un único modelo de referencia monolingüe para guiar el entrenamiento de modelos en múltiples idiomas, basándose en la idea de que “una frase traducida debe situarse en el mismo lugar del espacio vectorial-semántico que la frase original” <span class="citation">(<a href="#ref-reimers2020">Reimers y Gurevych 2020</a>)</span>.</p>
<p>En complemento a estas investigaciones, también se tiene a “<em>BETO: Spanish BERT</em>” <span class="citation">(<a href="#ref-CaneteCFP2020">Cañete et al. 2020</a>)</span>, que teniendo como sustento BERT, fue un modelo entrenado por el Departamento de Ciencias de la Computación Universidad de Chile, disponible en el enlace <a href="https://github.com/dccuchile/beto" class="uri">https://github.com/dccuchile/beto</a>, al cual se considera como perteneciente el estado del arte para el idioma español, alcanzando una precisión del 98,97% en tareas como el POS. También se aprovecha de indicar que este modelo igualmente se insertó en la implementación del SCSU.</p>
<p>Otro aspecto que es necesario señalar es que todas las investigaciones citadas se hicieron de dominio público y en muchos casos también se colocó a disposición de la comunidad científica los propios modelos preentrenados y los conjuntos de datos del entrenamiento, lo cual conllevó a que se lograra la reproducibilidad de los mismos, beneficiando a la comunidad científica y a los desarrolladores.</p>
<p>En caso de resultar de interés, para obtener información sobre los modelos de <em>embeddings</em> que presentan una elevada precisión y son de alta demanda por la comunidad <em>open source</em>, siendo parte del estado del arte, se tienen herramientas como el “<em>MTEB: Massive Text Embedding Benchmark</em>” <span class="citation">(<a href="#ref-muennighoff2022">Muennighoff et al. 2022</a>)</span> al que se puede acceder en el enlace <a href="https://huggingface.co/spaces/mteb/leaderboard" class="uri">https://huggingface.co/spaces/mteb/leaderboard</a> donde muestran métricas de 140 modelos preentrenados en el área del lenguaje disponibles para el uso público.</p>
</div>
<div id="LLM" class="section level3 hasAnchor" number="3.5.3">
<h3><span class="header-section-number">3.5.3</span> Largos Modelos de Lenguaje<a href="teorico.html#LLM" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Con la aparición de la arquitectura <em>Tranformers</em> se abrió el camino para la aparición de los Largos Modelos de Lenguaje <em>(Large Language Models -LLM´s)</em>. En principio pudiese parecer estar fuera del alcance de este trabajo exponer estos modelos, pero el estado del arte de los sistemas de recuperación de información se intersecta con ellos y no resultan ajenos a trabajos futuros que puedan suceder a esta investigación.</p>
<p>Como se pudo ver en la sección <a href="teorico.html#embed">3.5.1</a>, “<em>Embeddings”</em>, la tendencia ha sido ir incrementando la cantidad de datos con que se entrenan estos modelos así como también el número de parámetros que conforman al propio modelo. En la figura <a href="teorico.html#fig:llm">3.6</a> vemos las variaciones increméntales que se han dado desde la publicación del modelo basado en los <em>Transformers</em> en el año 2017.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:llm"></span>
<img src="images/03-marco-teorico/llms.png" alt="Evolución en la cantidad de parámetros en los LLM" width="85%" />
<p class="caption">
Figura 3.6: Evolución en la cantidad de parámetros en los LLM
</p>
</div>
<p>El crédito a la visualización <a href="teorico.html#fig:llm">3.6</a> corresponde a Harishdatala <span class="citation">(<a href="#ref-llmsize">Harishdatala 2023</a>)</span>. En general, los largos modelos de lenguaje son entrenados con enormes corpus de textos recopilados de foros de internet, de páginas web, de libros digitalizados y de un basto cúmulo de textos que también pueden incluir datos sintéticos.</p>
<p>De esta manera, sin entrar en mayores consideraciones sobre este crecimiento y los costos asociados, tanto monetarios, ambientales y de recursos computacionales, que imposibilitan a instituciones educativas, a empresas de mediano tamaño o a investigadores independientes, poder acceder a los sistemas de computadores necesarios para entrenar modelos de estas características, resulta significativo que a finales del año 2022 a uno de los modelos llamado “<em>Generative Pre-trained Transformer 3</em>” de la empresa OpenAI, del cual no se dispone mayor documentación sobre su arquitectura ni precisión sobre el método de entrenamiento, le es realizado un proceso de “<em>fine tunning</em>” que es un ajuste a los parámetros mediante un reentrenamiento y así se crea lo que hoy se conoce comercialmente como ChatGPT 3.5, introduciendo mediante una interfaz de usuario la funcionalidad de que se pueda interactuar con el modelo simulando una conversación, sorprendiendo por la capacidad de lograr emitir respuestas sobre una gran diversidad de temas de una manera fluida.</p>
<p>En complemento de lo anterior se tiene que, representando la interacción humano - LLM mediante una analogía con los SRI, lo que se aprecia es a un usuario haciendo un <em>query</em> ante un enorme corpus que excede y se organiza de una forma distinta a lo que se había revisado en <a href="teorico.html#SRI">3.2.1</a>, “Sistemas de Recuperación de Información”, donde se tenía una base de datos con documentos indexados. Ahora son distintos, tanto el proceso de interacción, como la representación de la información, ya que cada vez que se solicita un <em>query</em>, lo primera diferencia que se tiene es que, puede ser expresado en lenguaje natural y la segunda, es que no debe ser una condición que se de algún <em>match</em> con una palabra que conforma el índice invertido. En realidad el texto de la consulta es transformado en un <em>embedding,</em> que posteriormente activa capas de la red neuronal, conformadas las mismas por los parámetros del modelo y así de esta forma, mediante un proceso estocástico, el LLM va prediciendo palabra a palabra, conformando una respuesta a lo que fue el <em>query</em> inicial. Lo anterior puede cubrir la necesidad de información requerida con respuestas que estén acorde a lo esperado, siendo novedosas, fidedignas, o no tanto, a la realidad. Sin embargo, no se debe olvidar que la calidad del texto de la respuesta que produzca el LLM, dependerá de aspectos que traspasan lo computacional y se asocian un tanto más a las previsiones que se hayan tomado, por parte de quienes entrenan el modelo, para mitigar sesgos contenidos en los textos o entradas de datos incorrectas en la fase de entrenamiento del mismo.</p>
<p>Por otra parte, es indispensable resaltar como un hecho muy importante en el campo de los LLM´s que en el año 2023 algunas compañías e institutos de investigación privados, empezaron a liberar de licencias de uso ciertos modelos, con distintos pesos y versiones para la comunidad <em>open source</em>, como lo es el modelo Falcon <span class="citation">(<a href="#ref-penedo2023">Penedo et al. 2023</a>)</span> o el modelo OpenLlama2 <span class="citation">(<a href="#ref-touvron2023">Touvron et al. 2023</a>)</span> <a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>. Lo anterior, en conjunto con las facilidades para el desarrollo que aportan plataformas como <a href="huggingface.com">huggingface.com</a> para la implementación de aplicaciones de inteligencia artificial, mediante el almacenamiento de modelos preentrenados, conjuntos de datos para entrenamiento o sobre entrenamiento, así como librerías con <em>pipelines</em> de fácil integración mediante API´s unificadas <span class="citation">(<a href="#ref-wolf2019">Wolf et al. 2019</a>)</span>, estos LLM´s dejaron de tener un uso limitado solo para grandes empresas o consorcios tecnológicos y para la fecha es viable que corran en computadoras con capacidades limitadas, mediante la aplicación de procesos como el <em>quantized</em>, que se presenta en la investigación <span class="citation">(<a href="#ref-dettmers2023">Dettmers et al. 2023</a>)</span>, que por ejemplo permite que un modelo de lenguaje que inicialmente necesita unos 16 gb de memoria ram en GPU para ser desplegado, pueda disminuir a una cuarta parte, 4 gb, haciendo viable que se pueda desplegar en un computador con menores recursos disponibles.</p>
<p>Para cerrar el tema, la diversidad de modelos preentrenados, con distintas versiones que les ha sido efectuado un proceso de <em>fine tunning</em> o cuantizaciones, se puede ver en el enlace <span class="citation">(<a href="#ref-openllm">Edward Beeching 2023</a>)</span> <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" class="uri">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</a> donde se encuentra un tablero que muestra los modelos que presentan mayor popularidad, descargas y métricas de evaluación de su comportamiento.</p>
</div>
<div id="int" class="section level3 hasAnchor" number="3.5.4">
<h3><span class="header-section-number">3.5.4</span> Integración<a href="teorico.html#int" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Al contar con versiones <em>open source</em> de estos modelos es posible integrarlos en procesos de recuperación de información, principalmente mediante dos técnicas.</p>
<ol style="list-style-type: decimal">
<li><p><em>Retrieval Augmented Generation RAG</em> <span class="citation">(<a href="#ref-lewis2020">Lewis et al. 2020</a>)</span>: esta técnica permite que un LLM que fue entrenado con un determinado corpus, pueda activar la extracción de información desde fuentes externas, como páginas de internet o una bases de datos, complementando la información que inicialmente dispone el modelo, asociada a los datos de entrenamiento. Cabe destacar que, específicamente al hablar de las bases de datos que almacenan <em>embeddings</em>, a estas se les denomina <em>Vector DataBase.</em> Bien sea que el modelo de lenguaje acuda a una página de internet o a la <em>Vector Database</em>, una vez que la información se recupera, el LLM se encarga de procesarla e integrarla en una respuesta que debe tener una estructura compresible y coherente.</p></li>
<li><p><em>Fine Tunning</em> <span class="citation">(<a href="#ref-lv2023">Lv et al. 2023</a>)</span>: con un conjunto de datos etiquetado, generalmente con información de un dominio específico de un volumen mucho menor al que inicialmente fue entrenado un determinado LLM, se puede lograr que el modelo sea sobreentrenado con métodos como el propuesto en la investigación <em>“Universal Language Model Fine-tuning for Text Classification</em>” <span class="citation">(<a href="#ref-howard2018">Howard y Ruder 2018</a>)</span>, mejorando notablemente su desempeño en emitir respuestas sobre ese particular dominio en el que se realizó el ajuste.</p></li>
</ol>
<p>En resumen, estas técnicas por separado o en paralelo, pueden implementarse para crear sistemas de recuperación de información soportados en un LLM, el cual se pueda adaptar y convertirse en un experto de una precisa área de estudio que indique referencias a los trabajos que conforman un determinado corpus.</p>
</div>
<div id="busquedasemantica" class="section level3 hasAnchor" number="3.5.5">
<h3><span class="header-section-number">3.5.5</span> Búsqueda Semántica<a href="teorico.html#busquedasemantica" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Es una evolución en la forma en que los motores de búsqueda comprenden y responden a las consultas. A diferencia de la búsqueda tradicional basada en la aparición de palabras clave, la cual se centra en encontrar coincidencias literales entre la consulta y el contenido web, la búsqueda semántica tiene como objetivo comprender el significado contextual de las consultas y ofrecer resultados más relevantes y precisos.</p>
<p>Para ahondar en la comprensión de su significado, se tiene que en lugar de simplemente emparejar palabras clave, la búsqueda semántica utiliza <em>embeddings</em> para representar el texto y de una mejor forma se logra comprender la intención que motiva la consulta, lo cual implica que se analiza la relación semántica entre las palabras, interpretando el contexto y comprendiendo el significado subyacente.</p>
<p>En tan sentido, desde un punto de vista relativo a la implementación, se tiene que en las búsquedas semánticas las cadenas de texto de un documento se representan como vectores de tipo <em>embeddings,</em> los cuales son almacenados en una base de datos y el sistema de recuperación de información al recibir un <em>query</em> transforma el texto en otro <em>embedding</em>, lo cual permite determinar el grado de similitud vectorial y así proceder a recuperar los documentos que presenten mayor parecido <span class="citation">(<a href="#ref-muennighoff2022a">Muennighoff 2022</a>)</span>.</p>
<p>Es importante destacar de manera muy particular que, el éxito que se obtenga al usar este tipo de búsqueda dependerá en una gran medida de la correcta selección del modelo de aprendiza automático preentrenado que se use para convertir los textos a <em>embeddings</em>. Por ejemplo, existen modelos que han sido entrenados para un idioma o para un dominio específico, lo cual hará que un sistema de recuperación de información que sea diseñado para un uso particular, se verá beneficiado si se selecciona un modelo acorde a sus necesidades.</p>
</div>
<div id="tendencias-actuales" class="section level3 hasAnchor" number="3.5.6">
<h3><span class="header-section-number">3.5.6</span> Tendencias Actuales<a href="teorico.html#tendencias-actuales" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Finalmente, se mencionan algunos puntos de lo que hoy constituye el estado del arte en los temas que hemos ido revisando a lo largo de este capítulo:</p>
<ol style="list-style-type: decimal">
<li><p>Con modelos como BERT se han hecho implementaciones modificadas para hacer el re ordenamiento de los resultados obtenidos en un proceso de búsqueda, bien sea mediante las técnicas tradicionales o mediante técnicas de búsqueda semántica<span class="citation">(<a href="#ref-nogueira2019">Nogueira y Cho 2019</a>)</span>. Igualmente, con redes neuronales se ha buscado simular el comportamiento humano para jerarquizar los resultados obtenidos en procesos de búsqueda <span class="citation">(<a href="#ref-pang2017">Pang et al. 2017</a>)</span>.</p></li>
<li><p>Se están proponiendo nuevos métodos para evaluar la eficacia en los sistemas de “preguntas-respuestas” basados en similaridad semántica, dadas las limitaciones que presentan las métricas tradicionales que no reflejan el desempeño de estos nuevos modelos <span class="citation">(<a href="#ref-risch2021">Risch et al. 2021</a>)</span>.</p></li>
<li><p>Mediante técnicas de aprendizaje profundo se están creando conjuntos de datos sintéticos de dominio público que permitan evaluar el desempeño de los sistemas de recuperación de información, usando como entrada las publicaciones de Wikipedia y creando con estos modelos los <em>querys</em>, que permitan medir el grado de precisión que alcanza un determinado sistema <span class="citation">(<a href="#ref-frej-etal-2020-wikir">Frej, Schwab, y Chevallet 2020</a>)</span>.</p></li>
</ol>
<p>Expuestos los puntos anteriores se culmina el recorrido del marco teórico-referencial que soporta esta investigación.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-aggarwal2018a" class="csl-entry">
Aggarwal, Charu C. 2018a. <em>Machine Learning for Text</em>. 1st ed. 2018. Cham: Springer International Publishing : Imprint: Springer. <a href="https://doi.org/10.1007/978-3-319-73531-3">https://doi.org/10.1007/978-3-319-73531-3</a>.
</div>
<div id="ref-miningt2012" class="csl-entry">
Aggarwal, Charu C., y ChengXiang Zhai, eds. 2012. <em>Mining text data</em>. New York Heidelberg: Springer. <a href="https://doi.org/10.1007/978-1-4614-3223-4">https://doi.org/10.1007/978-1-4614-3223-4</a>.
</div>
<div id="ref-aho1975" class="csl-entry">
Aho, Alfred V., y Margaret J. Corasick. 1975. <span>«Efficient String Matching»</span>. <em>Communications of the ACM</em> 18 (6): 333-40. <a href="https://doi.org/10.1145/360825.360855">https://doi.org/10.1145/360825.360855</a>.
</div>
<div id="ref-wordtovec" class="csl-entry">
Alammar, J. 2023. <span>«The Illustrated Word2vec»</span>. <a href="https://jalammar.github.io/illustrated-word2vec" class="uri">https://jalammar.github.io/illustrated-word2vec</a>.
</div>
<div id="ref-balog2018" class="csl-entry">
Balog, Krisztian. 2018. <em>Entity-Oriented Search</em>. 1st ed. 2018. The Information Retrieval Series 39. Cham: Springer International Publishing : Imprint: Springer. <a href="https://doi.org/10.1007/978-3-319-93935-3">https://doi.org/10.1007/978-3-319-93935-3</a>.
</div>
<div id="ref-Bengio:2003:NPL:944919.944966" class="csl-entry">
Bengio, Yoshua, Réjean Ducharme, Pascal Vincent, y Christian Janvin. 2003. <span>«A Neural Probabilistic Language Model»</span>. <em>J. Mach. Learn. Res.</em> 3 (marzo): 1137-55. <a href="http://dl.acm.org/citation.cfm?id=944919.944966">http://dl.acm.org/citation.cfm?id=944919.944966</a>.
</div>
<div id="ref-boleda2020" class="csl-entry">
Boleda, Gemma. 2020. <span>«Distributional Semantics and Linguistic Theory»</span>. <em>Annual Review of Linguistics</em> 6 (1): 213-34. <a href="https://doi.org/10.1146/annurev-linguistics-011619-030303">https://doi.org/10.1146/annurev-linguistics-011619-030303</a>.
</div>
<div id="ref-brin1998" class="csl-entry">
Brin, Sergey, y Lawrence Page. 1998. <span>«The Anatomy of a Large-Scale Hypertextual Web Search Engine»</span>. <em>Computer Networks and ISDN Systems</em> 30 (1-7): 107-17. <a href="https://doi.org/10.1016/s0169-7552(98)00110-x">https://doi.org/10.1016/s0169-7552(98)00110-x</a>.
</div>
<div id="ref-büttcher2010" class="csl-entry">
Büttcher, Stefan, Charles L. A. Clarke, y Gordon V. Cormack. 2010b. <em>Information retrieval: implementing and evaluating search engines</em>. Cambridge, Mass: MIT Press.
</div>
<div id="ref-büttcher2010a" class="csl-entry">
———. 2010a. <em>Information retrieval: implementing and evaluating search engines</em>. Cambridge, Mass: MIT Press.
</div>
<div id="ref-CaneteCFP2020" class="csl-entry">
Cañete, José, Gabriel Chaperon, Rodrigo Fuentes, Jou-Hui Ho, Hojin Kang, y Jorge Pérez. 2020. <span>«Spanish Pre-Trained BERT Model and Evaluation Data»</span>. En <em>PML4DC at ICLR 2020</em>.
</div>
<div id="ref-chen2014" class="csl-entry">
Chen, Danqi, y Christopher Manning. 2014a. <span>«A Fast and Accurate Dependency Parser using Neural Networks»</span>. <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>. <a href="https://doi.org/10.3115/v1/d14-1082">https://doi.org/10.3115/v1/d14-1082</a>.
</div>
<div id="ref-chen2014fast" class="csl-entry">
Chen, Danqi, y Christopher D Manning. 2014b. <span>«A fast and accurate dependency parser using neural networks»</span>. En <em>Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</em>, 740-50.
</div>
<div id="ref-cook2017" class="csl-entry">
Cook, Joshua. 2017. <em>Docker for Data Science</em>. Apress. <a href="https://doi.org/10.1007/978-1-4842-3012-1">https://doi.org/10.1007/978-1-4842-3012-1</a>.
</div>
<div id="ref-distribu2012" class="csl-entry">
Coulouris, George F., ed. 2012. <em>Distributed systems: concepts and design</em>. 5th ed. Boston: Addison-Wesley.
</div>
<div id="ref-czaja2018" class="csl-entry">
Czaja, Ludwik. 2018. <em>Introduction to Distributed Computer Systems: Principles and Features</em>. 1st ed. 2018. Lecture Notes en Networks y Systems 27. Cham: Springer International Publishing : Imprint: Springer. <a href="https://doi.org/10.1007/978-3-319-72023-4">https://doi.org/10.1007/978-3-319-72023-4</a>.
</div>
<div id="ref-desagulier2017" class="csl-entry">
Desagulier, Guillaume. 2017. <em>Corpus Linguistics and Statistics with R</em>. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-64572-8">https://doi.org/10.1007/978-3-319-64572-8</a>.
</div>
<div id="ref-dettmers2023" class="csl-entry">
Dettmers, Tim, Artidoro Pagnoni, Ari Holtzman, y Luke Zettlemoyer. 2023. <span>«QLoRA: Efficient Finetuning of Quantized LLMs»</span>. <a href="https://doi.org/10.48550/ARXIV.2305.14314">https://doi.org/10.48550/ARXIV.2305.14314</a>.
</div>
<div id="ref-devlin2018" class="csl-entry">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, y Kristina Toutanova. 2018. <span>«BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding»</span>. <a href="https://doi.org/10.48550/ARXIV.1810.04805">https://doi.org/10.48550/ARXIV.1810.04805</a>.
</div>
<div id="ref-dueñas2011" class="csl-entry">
Dueñas, Marco, Diana Rojas, y María Eugenia Morales. 2011. <span>«Propuesta metodológica para realizar mapas de conocimiento»</span>. <em>Revista Facultad de Ciencias Económicas</em> 20 (1): 77-90. <a href="https://doi.org/10.18359/rfce.2186">https://doi.org/10.18359/rfce.2186</a>.
</div>
<div id="ref-openllm" class="csl-entry">
Edward Beeching, Sheon Han, Nathan Habib. 2023. <span>«Open LLM Leaderboard»</span>. <a href="https://huggingface.co/open-llm-leaderboard" class="uri">https://huggingface.co/open-llm-leaderboard</a>; Hugging Face.
</div>
<div id="ref-eisenstein2019" class="csl-entry">
Eisenstein, Jacob. 2019. <em>Introduction to natural language processing</em>. Adaptive computation y machine learning. Cambridge, Massachusetts: The MIT Press.
</div>
<div id="ref-frej-etal-2020-wikir" class="csl-entry">
Frej, Jibril, Didier Schwab, y Jean-Pierre Chevallet. 2020. <span>«<span>WIKIR</span>: A Python Toolkit for Building a Large-scale <span>W</span>ikipedia-based <span>E</span>nglish Information Retrieval Dataset»</span>. En <em>Proceedings of the Twelfth Language Resources and Evaluation Conference</em>, 1926-33. Marseille, France: European Language Resources Association. <a href="https://aclanthology.org/2020.lrec-1.237">https://aclanthology.org/2020.lrec-1.237</a>.
</div>
<div id="ref-goodrich2013" class="csl-entry">
Goodrich, Michael T., Roberto Tamassia, y Michael H. Goldwasser. 2013. <em>Data structures and algorithms in Python</em>. Hoboken, NJ: Wiley.
</div>
<div id="ref-llmsize" class="csl-entry">
Harishdatala. 2023. <span>«Unveiling the Power of Large Language Models (LLMs)»</span>. <a href="https://medium.com/@harishdatalab/unveiling-the-power-of-large-language-models-llms-e235c4eba8a9" class="uri">https://medium.com/@harishdatalab/unveiling-the-power-of-large-language-models-llms-e235c4eba8a9</a>.
</div>
<div id="ref-harman2011" class="csl-entry">
Harman, Donna. 2011. <em>Information retrieval evaluation</em>. Synthesis lectures on information concepts, retrieval, y services 19. San Rafael, Calif.: Morgan Claypool.
</div>
<div id="ref-howard2018" class="csl-entry">
Howard, Jeremy, y Sebastian Ruder. 2018. <span>«Universal Language Model Fine-tuning for Text Classification»</span>. <a href="https://doi.org/10.48550/ARXIV.1801.06146">https://doi.org/10.48550/ARXIV.1801.06146</a>.
</div>
<div id="ref-knuth1997" class="csl-entry">
Knuth, Donald Ervin. 1997. <em>The art of computer programming</em>. 3rd ed. Reading, Mass: Addison-Wesley.
</div>
<div id="ref-kraft2017" class="csl-entry">
Kraft, Donald H., y Erin Colvin. 2017. <span>«Fuzzy Information Retrieval»</span>. <em>Synthesis Lectures on Information Concepts, Retrieval, and Services</em> 9 (1): i-63. <a href="https://doi.org/10.2200/s00752ed1v01y201701icr055">https://doi.org/10.2200/s00752ed1v01y201701icr055</a>.
</div>
<div id="ref-lewis2020" class="csl-entry">
Lewis, Patrick, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, et al. 2020. <span>«Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks»</span>. <a href="https://doi.org/10.48550/ARXIV.2005.11401">https://doi.org/10.48550/ARXIV.2005.11401</a>.
</div>
<div id="ref-liu2019" class="csl-entry">
Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, y Veselin Stoyanov. 2019. <span>«RoBERTa: A Robustly Optimized BERT Pretraining Approach»</span>. <a href="https://doi.org/10.48550/ARXIV.1907.11692">https://doi.org/10.48550/ARXIV.1907.11692</a>.
</div>
<div id="ref-lv2023" class="csl-entry">
Lv, Kai, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, y Xipeng Qiu. 2023. <span>«Full Parameter Fine-tuning for Large Language Models with Limited Resources»</span>. <a href="https://doi.org/10.48550/ARXIV.2306.09782">https://doi.org/10.48550/ARXIV.2306.09782</a>.
</div>
<div id="ref-Mahapatra2011" class="csl-entry">
Mahapatra, Ajit Kumar, y Sitanath Biswas. 2011. <span>«Inverted indexes: Types and techniques»</span>. <em>International Journal of Computer Science Issues</em> 8 (julio).
</div>
<div id="ref-manning2008" class="csl-entry">
Manning, Christopher D., Prabhakar Raghavan, y Hinrich Schütze. 2008. <em>Introduction to information retrieval</em>. New York: Cambridge University Press.
</div>
<div id="ref-manning-etal-2014-stanford" class="csl-entry">
Manning, Christopher, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, y David McClosky. 2014. <span>«The <span>S</span>tanford <span>C</span>ore<span>NLP</span> Natural Language Processing Toolkit»</span>. En <em>Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</em>, 55-60. Baltimore, Maryland: Association for Computational Linguistics. <a href="https://doi.org/10.3115/v1/P14-5010">https://doi.org/10.3115/v1/P14-5010</a>.
</div>
<div id="ref-demarneffe2021" class="csl-entry">
Marneffe, Marie-Catherine de, Christopher D. Manning, Joakim Nivre, y Daniel Zeman. 2021. <span>«Universal Dependencies»</span>. <em>Computational Linguistics</em>, mayo, 1-54. <a href="https://doi.org/10.1162/coli_a_00402">https://doi.org/10.1162/coli_a_00402</a>.
</div>
<div id="ref-martiaurora" class="csl-entry">
Martín de Santa Olalla Sánchez, Aurora. 1994. <span>«Una propuesta de codificación morfosintáctica para corpus de referencia en lengua española»</span>.
</div>
<div id="ref-mikolov2013" class="csl-entry">
Mikolov, Tomas, Kai Chen, Greg Corrado, y Jeffrey Dean. 2013. <span>«Efficient Estimation of Word Representations in Vector Space»</span>. <a href="https://doi.org/10.48550/ARXIV.1301.3781">https://doi.org/10.48550/ARXIV.1301.3781</a>.
</div>
<div id="ref-mikolov2013a" class="csl-entry">
Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, y Jeffrey Dean. 2013. <span>«Distributed Representations of Words and Phrases and their Compositionality»</span>. <a href="https://doi.org/10.48550/ARXIV.1310.4546">https://doi.org/10.48550/ARXIV.1310.4546</a>.
</div>
<div id="ref-muennighoff2022a" class="csl-entry">
Muennighoff, Niklas. 2022. <span>«SGPT: GPT Sentence Embeddings for Semantic Search»</span>. <a href="https://doi.org/10.48550/ARXIV.2202.08904">https://doi.org/10.48550/ARXIV.2202.08904</a>.
</div>
<div id="ref-muennighoff2022" class="csl-entry">
Muennighoff, Niklas, Nouamane Tazi, Loïc Magne, y Nils Reimers. 2022. <span>«MTEB: Massive Text Embedding Benchmark»</span>. <a href="https://doi.org/10.48550/ARXIV.2210.07316">https://doi.org/10.48550/ARXIV.2210.07316</a>.
</div>
<div id="ref-nogueira2019" class="csl-entry">
Nogueira, Rodrigo, y Kyunghyun Cho. 2019. <span>«Passage Re-ranking with BERT»</span>. <a href="https://doi.org/10.48550/ARXIV.1901.04085">https://doi.org/10.48550/ARXIV.1901.04085</a>.
</div>
<div id="ref-nüst2020" class="csl-entry">
Nüst, Daniel, Vanessa Sochat, Ben Marwick, Stephen J. Eglen, Tim Head, Tony Hirst, y Benjamin D. Evans. 2020. <span>«Ten Simple Rules for Writing Dockerfiles for Reproducible Data Science»</span>. Editado por Scott Markel. <em>PLOS Computational Biology</em> 16 (11): e1008316. <a href="https://doi.org/10.1371/journal.pcbi.1008316">https://doi.org/10.1371/journal.pcbi.1008316</a>.
</div>
<div id="ref-pang2017" class="csl-entry">
Pang, Liang, Yanyan Lan, Jiafeng Guo, Jun Xu, Jingfang Xu, y Xueqi Cheng. 2017. <span>«DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval»</span>. <em>arXiv</em>. <a href="https://doi.org/10.48550/ARXIV.1710.05649">https://doi.org/10.48550/ARXIV.1710.05649</a>.
</div>
<div id="ref-penedo2023" class="csl-entry">
Penedo, Guilherme, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, y Julien Launay. 2023. <span>«The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only»</span>. <a href="https://doi.org/10.48550/ARXIV.2306.01116">https://doi.org/10.48550/ARXIV.2306.01116</a>.
</div>
<div id="ref-pennington2014" class="csl-entry">
Pennington, Jeffrey, Richard Socher, y Christopher Manning. 2014. <span>«Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)»</span>. En, 1532-43. Doha, Qatar: Association for Computational Linguistics. <a href="https://doi.org/10.3115/v1/D14-1162">https://doi.org/10.3115/v1/D14-1162</a>.
</div>
<div id="ref-reimers2019a" class="csl-entry">
Reimers, Nils, y Iryna Gurevych. 2019. <span>«Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks»</span>. <a href="https://doi.org/10.48550/ARXIV.1908.10084">https://doi.org/10.48550/ARXIV.1908.10084</a>.
</div>
<div id="ref-reimers2020" class="csl-entry">
———. 2020. <span>«Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation»</span>. <a href="https://doi.org/10.48550/ARXIV.2004.09813">https://doi.org/10.48550/ARXIV.2004.09813</a>.
</div>
<div id="ref-risch2021" class="csl-entry">
Risch, Julian, Timo Möller, Julian Gutsch, y Malte Pietsch. 2021. <span>«Semantic Answer Similarity for Evaluating Question Answering Models»</span>. <em>arXiv</em>. <a href="https://doi.org/10.48550/ARXIV.2108.06130">https://doi.org/10.48550/ARXIV.2108.06130</a>.
</div>
<div id="ref-robertson2009" class="csl-entry">
Robertson, Stephen, y Hugo Zaragoza. 2009. <span>«The Probabilistic Relevance Framework: BM25 and Beyond»</span>. <em>Foundations and Trends® in Information Retrieval</em> 3 (4): 333-89. <a href="https://doi.org/10.1561/1500000019">https://doi.org/10.1561/1500000019</a>.
</div>
<div id="ref-salakhutdinov2009" class="csl-entry">
Salakhutdinov, Ruslan, y Geoffrey Hinton. 2009. <span>«Semantic Hashing»</span>. <em>International Journal of Approximate Reasoning</em> 50 (7): 969-78. <a href="https://doi.org/10.1016/j.ijar.2008.11.006">https://doi.org/10.1016/j.ijar.2008.11.006</a>.
</div>
<div id="ref-segev2021" class="csl-entry">
Segev, Elad. 2021. <em>Semantic Network Analysis in Social Sciences</em>. Routledge. <a href="https://doi.org/10.4324/9781003120100">https://doi.org/10.4324/9781003120100</a>.
</div>
<div id="ref-smith1981" class="csl-entry">
Smith, T. F., y M. S. Waterman. 1981. <span>«Identification of Common Molecular Subsequences»</span>. <em>Journal of Molecular Biology</em> 147 (1): 195-97. <a href="https://doi.org/10.1016/0022-2836(81)90087-5">https://doi.org/10.1016/0022-2836(81)90087-5</a>.
</div>
<div id="ref-straka2017" class="csl-entry">
Straka, Milan, y Jana Straková. 2017. <span>«Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe»</span>. <em>Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</em>. <a href="https://doi.org/10.18653/v1/k17-3009">https://doi.org/10.18653/v1/k17-3009</a>.
</div>
<div id="ref-touvron2023" class="csl-entry">
Touvron, Hugo, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, et al. 2023. <span>«Llama 2: Open Foundation and Fine-Tuned Chat Models»</span>. <a href="https://doi.org/10.48550/ARXIV.2307.09288">https://doi.org/10.48550/ARXIV.2307.09288</a>.
</div>
<div id="ref-vaswani2017" class="csl-entry">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, y Illia Polosukhin. 2017. <span>«Attention Is All You Need»</span>. <a href="https://doi.org/10.48550/ARXIV.1706.03762">https://doi.org/10.48550/ARXIV.1706.03762</a>.
</div>
<div id="ref-willett2006" class="csl-entry">
Willett, Peter. 2006. <span>«The Porter Stemming Algorithm: Then and Now»</span>. <em>Program</em> 40 (3): 219-23. <a href="https://doi.org/10.1108/00330330610681295">https://doi.org/10.1108/00330330610681295</a>.
</div>
<div id="ref-wolf2019" class="csl-entry">
Wolf, Thomas, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, et al. 2019. <span>«HuggingFace’s Transformers: State-of-the-art Natural Language Processing»</span>. <a href="https://doi.org/10.48550/ARXIV.1910.03771">https://doi.org/10.48550/ARXIV.1910.03771</a>.
</div>
<div id="ref-worldde2016" class="csl-entry">
<em>World Development Report 2016: Digital Dividends</em>. 2016. Washington, DC: World Bank.
</div>
<div id="ref-zhai2016" class="csl-entry">
Zhai, ChengXiang, y Sean Massung. 2016. <em>Text data management and analysis: a practical introduction to information retrieval and text mining</em>. First Edition. ACM Books <span>#</span>12. New York: ACM Books.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>En el vínculo <a href="https://github.com/postgrespro/rum" class="uri">https://github.com/postgrespro/rum</a> se tiene acceso a la explicación e implementación de este índice para PostgreSQL.<a href="teorico.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>este índice fue presentado en la Postgres Conference en el año 2014 <a href="https://www.pgcon.org/2014/schedule/attachments/318_pgcon-2014-vodka.pdf" class="uri">https://www.pgcon.org/2014/schedule/attachments/318_pgcon-2014-vodka.pdf</a><a href="teorico.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p><a href="https://nlp.lsi.upc.edu/freeling/node/1" class="uri">https://nlp.lsi.upc.edu/freeling/node/1</a><a href="teorico.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p><strong>AnCora</strong> es un corpus del <strong>catalán (AnCora-CA)</strong> y del <strong>español (AnCora-ES)</strong> con diferentes niveles de anotación como lema y categoría morfológica, constituyentes y funciones sintácticas, estructura argumental y papeles temáticos, clase semántica verbal, tipo denotativo de los nombres deverbales, sentidos de WordNet nominales, entidades nombradas (NER), relaciones de correferencia (<a href="http://clic.ub.edu/corpus/es/ancora" class="uri">http://clic.ub.edu/corpus/es/ancora</a>)<a href="teorico.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Los mecanismos de atención permiten al modelo asignar ponderaciones dinámicas a diferentes partes de la entrada, lo que resulta en una comprensión más profunda y contextualizada del texto.<a href="teorico.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>la compañía que entrenó el modelo y lo liberó que es Meta indicó que es OpenSource, pero revisiones técnicas hechas a la licencia cuestionan que se pueda considerar que realmente cumpla las especificaciones para que sea considerado plenamente “open source”. En el enlace <a href="https://opensourceconnections.com/blog/2023/07/19/is-llama-2-open-source-no-and-perhaps-we-need-a-new-definition-of-open/">Is Llama 2 open source?</a> se encuentra un análisis sobre el tema.<a href="teorico.html#fnref6" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="capproblema.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["twitter", "linkedin", "whatsapp"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"lib_dir": "book_assets",
"split_by": "section",
"config": {
"toc": {
"collapse": "subsection",
"scroll_highlight": true
}
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
